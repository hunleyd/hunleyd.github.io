<!DOCTYPE html>
<html>
  <head>
    <title>EXPLAINing intermittent perf problems – Douglas J Hunley – It's not that I'm anti-social; I'm just not user-friendly</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="We&#39;ve all gotten the dreaded email/call from a user stating that a query is &quot;slow sometimes&quot;. If you&#39;re lucky, the &quot;sometimes&quot; actually ends up being fairly consistent and you can fairly easily determine what&#39;s happening (an errant cron job, for example). All too often though, the issue really is sporadic, fleeting, and indeterministic. So how do you track these down? And more importantly what do you do about them once found?
" />
    <meta property="og:description" content="We&#39;ve all gotten the dreaded email/call from a user stating that a query is &quot;slow sometimes&quot;. If you&#39;re lucky, the &quot;sometimes&quot; actually ends up being fairly consistent and you can fairly easily determine what&#39;s happening (an errant cron job, for example). All too often though, the issue really is sporadic, fleeting, and indeterministic. So how do you track these down? And more importantly what do you do about them once found?
" />
    
    <meta name="author" content="Douglas J Hunley" />

    
    <meta property="og:title" content="EXPLAINing intermittent perf problems" />
    <meta property="twitter:title" content="EXPLAINing intermittent perf problems" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Douglas J Hunley - It's not that I'm anti-social; I'm just not user-friendly" href="/feed.xml" />
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://raw.githubusercontent.com/hunleyd/hunleyd.github.io/master/images/jekyll-logo.png" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Douglas J Hunley</a></h1>
            <p class="site-description">It's not that I'm anti-social; I'm just not user-friendly</p>
          </div>

          <nav>
            
            <font size=-1><a href="/about/">About</a></font>
            
            <font size=-1><a href="/archive/">Archive</a></font>
            
            <font size=-1><a href="/feed/">Feeds</a></font>
            
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
        <article class="post">
  <h1>EXPLAINing intermittent perf problems</h1>
  <div class="meta_wrapper"><time datetime="2016-11-28">November 28, 2016</time><ul class="tag_list_in_post"><li class="tag_list_item"><a class="tag_list_link" href="/tag/postgresql">postgresql</a></li></ul></div>

  <div class="entry">
    <p><img src="http://i0.kym-cdn.com/entries/icons/original/000/010/997/35s7cv.jpg" border=0 align=left style="PADDING-RIGHT: 5px">We&#39;ve all gotten the dreaded email/call from a user stating that a query is &quot;slow sometimes&quot;. If you&#39;re lucky, the &quot;sometimes&quot; actually ends up being fairly consistent and you can fairly easily determine what&#39;s happening (an errant cron job, for example). All too often though, the issue really is sporadic, fleeting, and indeterministic. So how do you track these down? And more importantly what do you do about them once found?</p>

<p>For starters, you as the DBA should have your PostgreSQL logging configured to log these slow performing queries. After all, you and the devs and the users can agree that all queries should complete in some measure of time (1 sec, 1 minute, etc). So, once you know what this acceptable elapsed time is, you can easily log any query that runs longer by just setting this in your <code>postgresql.conf</code>:</p>
<figure class="highlight"><pre><code class="language-text" data-lang="text">log_min_duration_statement = 1000   # log anything running longer than 1s
</code></pre></figure>
<p>And now, you have all queries with long run times logged automatically. And these show up nicely in your pgBadger reports too!</p>

<p>If you&#39;re lucky, you&#39;ll be able to use <code>EXPLAIN</code> to see why the query is behaving poorly. However, if your life is like mine, the explain plan will be reasonable and won&#39;t have any smoking guns to speak of. Which means the performance is either load dependent or being influenced by other processes (something is blowing out your caches, for example). In these cases, what you really need is the <code>EXPLAIN</code> output from the very instant that it performed poorly. However, you can&#39;t go back in time to get it. But what you can do is make use of the <code>auto_explain</code> module that ships with PostgreSQL.</p>

<p>In case the name wasn&#39;t obvious enough, the <code>auto_explain</code> module causes PostgreSQL to automatically run <code>EXPLAIN</code> on queries according to thresholds that you configure. These automatically generated plans are then logged into the normal PostgreSQL logs. Let&#39;s walk through setting it up and see how it works.</p>

<p>First, in your <code>postgresql.conf</code> we want to enable the module:</p>
<figure class="highlight"><pre><code class="language-text" data-lang="text">shared_preload_libraries = &#39;auto_explain&#39;  # change requires restart
</code></pre></figure>
<p>As stated, you will have to restart the postmaster to get the module to load. However, let&#39;s configure it in <code>postgresql.conf</code> first:</p>
<figure class="highlight"><pre><code class="language-text" data-lang="text"># Add settings for extensions here
#
# auto_explain
# http://www.postgresql.org/docs/current/static/auto-explain.html
auto_explain.log_analyze = true
auto_explain.log_timing = true
auto_explain.log_verbose = true
auto_explain.log_min_duration = &#39;1000ms&#39;
auto_explain.log_nested_statements = true
auto_explain.log_buffers = true
# auto_explain
</code></pre></figure>
<p>What we&#39;ve done here is configure <code>auto_explain</code> to</p>

<ul>
<li>use <code>EXPLAIN ANALYZE</code><sup>1</sup></li>
<li>to use the <code>TIMING</code> option of <code>EXPLAIN</code></li>
<li>to use the <code>VERBOSE</code> option of <code>EXPLAIN</code></li>
<li>to log the plan for anything running longer than 1 second (matches <code>log_min_duration_statement</code>, above)</li>
<li>to include statements inside a function to also be logged</li>
<li>to use the <code>BUFFERS</code> option of <code>EXPLAIN</code></li>
</ul>

<p>As with most <code>GUC</code> in PostgreSQL, these can all be changed using <code>SET</code> in a given session, but we&#39;re setting the defaults here. Now that we have them setup, let&#39;s see what it looks like in practice.</p>
<figure class="highlight"><pre><code class="language-sql" data-lang="sql">(doug@[local]:5432/doug[28838]) # CREATE TABLE x(t text);
CREATE TABLE
Time: 6.022 ms
(doug@[local]:5432/doug[28838]) # INSERT INTO x(t) SELECT generate_series(1,10000);
INSERT 0 10000
Time: 23.565 ms
(doug@[local]:5432/doug[28838]) #
</code></pre></figure>
<p>We connected to PostgreSQL, created a test table, and then used <code>generate_series</code> to insert 10k rows. In our logs, the following were added:</p>
<figure class="highlight"><pre><code class="language-text" data-lang="text">2016-11-28 13:20:30 EST [28838]: [18-1] user=doug,db=doug,app=psql,client=[local] LOG:  duration: 33.987 ms  statement: CREATE TABLE x(t text);
2016-11-28 13:20:59 EST [28838]: [19-1] user=doug,db=doug,app=psql,client=[local] LOG:  duration: 16.461 ms  plan:
  Query Text: INSERT INTO x(t) SELECT generate_series(1,10000);
  Insert on public.x  (cost=0.00..50.02 rows=1000 width=32) (actual time=16.459..16.459 rows=0 loops=1)
    Buffers: shared hit=10085 read=47 dirtied=45
    I/O Timings: read=0.012
    -&gt;  Subquery Scan on &quot;*SELECT*&quot;  (cost=0.00..50.02 rows=1000 width=32) (actual time=0.010..4.755 rows=10000 loops=1)
          Output: &quot;*SELECT*&quot;.generate_series
          -&gt;  Result  (cost=0.00..15.02 rows=1000 width=4) (actual time=0.007..1.364 rows=10000 loops=1)
                Output: generate_series(1, 10000)
2016-11-28 13:20:59 EST [28838]: [20-1] user=doug,db=doug,app=psql,client=[local] LOG:  duration: 23.374 ms  statement: INSERT INTO x(t) SELECT generate_series(1,10000);
2016-11-28 13:21:00 EST [30079]: [1-1] user=,db=,app=,client= LOG:  automatic analyze of table &quot;doug.public.x&quot; system usage: CPU 0.00s/0.11u sec elapsed 0.14 sec
</code></pre></figure>
<p>(Note that for illustrative purposes, I issued <code>SET auto_explain.log_min_duration = &#39;0ms&#39;</code>)</p>

<p>So, you can see that the <code>CREATE TABLE</code> didn&#39;t log anything through the <code>auto_explain</code> module, but the <code>INSERT INTO</code> did. This is a boring example, so let&#39;s try a <code>SELECT</code> against our table:</p>
<figure class="highlight"><pre><code class="language-sql" data-lang="sql">(doug@[local]:5432/doug[28838]) # SELECT * FROM x ORDER BY t LIMIT 10;
┌───────┐
│   t   │
├───────┤
│ 1     │
│ 10    │
│ 100   │
│ 1000  │
│ 10000 │
│ 1001  │
│ 1002  │
│ 1003  │
│ 1004  │
│ 1005  │
└───────┘
(10 rows)

Time: 11.982 ms
</code></pre></figure>
<p>and the logs look like:</p>
<figure class="highlight"><pre><code class="language-text" data-lang="text">2016-11-28 13:27:38 EST [322]: [7-1] user=,db=,app=,client= LOG:  checkpoint starting: time
2016-11-28 13:27:46 EST [322]: [8-1] user=,db=,app=,client= LOG:  checkpoint complete: wrote 75 buffers (0.0%); 0 transaction log file(s) added, 0 removed, 0 recycled; write=7.569 s, sync=0.092 s, total=7.920 s; sync files=23, longest=0.092 s, average=0.004 s; distance=685 kB, estimate=685 kB
2016-11-28 13:28:48 EST [28838]: [21-1] user=doug,db=doug,app=psql,client=[local] LOG:  duration: 11.120 ms  plan:
  Query Text: SELECT * FROM x ORDER BY t LIMIT 10;
  Limit  (cost=561.10..561.12 rows=10 width=4) (actual time=11.073..11.073 rows=10 loops=1)
    Output: t
    Buffers: shared hit=45
    -&gt;  Sort  (cost=561.10..586.10 rows=10000 width=4) (actual time=11.072..11.072 rows=10 loops=1)
          Output: t
          Sort Key: x.t
          Sort Method: top-N heapsort  Memory: 25kB
          Buffers: shared hit=45
          -&gt;  Seq Scan on public.x  (cost=0.00..345.00 rows=10000 width=4) (actual time=0.018..1.224 rows=10000 loops=1)
                Output: t
                Buffers: shared hit=45
2016-11-28 13:28:48 EST [28838]: [22-1] user=doug,db=doug,app=psql,client=[local] LOG:  duration: 11.813 ms  statement: SELECT * FROM x ORDER BY t LIMIT 10;
</code></pre></figure>
<p>(You can safely ignore the checkpoint lines at the top there)</p>

<p>There you have both the statement we ran, and the full <code>EXPLAIN</code> plan. You can see we did a sequential scan on the table (looks like it was all in the <code>shared_buffers</code> too) and then we passed that up to a <code>sort</code> node for an in-memory sort, and then passed that result set up to the <code>limit</code> node.</p>

<p>While this is a stupid simple example, I hope you can see that having this in production for large, complicated queries will allow you to better diagnose issues. For example, simply doing a manual <code>EXPLAIN ANALYZE</code> on the same query and seeing that you get a different plan is potentially enough to rule out (or in) certain culprits for the intermittent performance issue.
<br>
<br>
<br>
<br>
<sup>1</sup> - This option causes the postmaster to collect info on <em>every</em> statement executed, even if <code>auto_explain</code> isn&#39;t going to log it. It has a measurable impact on overall performance. Please test on your workload and decide for yourself if the overhead is worth the trade-off</p>

  </div>

  <font size=-1><a href="/">Home</a> / <a href="">EXPLAINing intermittent perf problems</a></font>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'douglashunley';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          <ul class="tag_list_in_post">
    
        <li>
            <a href="https://twitter.com/hunleyd">
                <i class="fa fa-twitter"></i>
            <a>
        </li>
    
</ul>

          <p>
            &copy; 2016. All rights reserved.
          </p>
        </footer>
      </div>
    </div>

  </body>
</html>
