[{"categories":["work","cpa","ansible"],"contents":"We\u0026rsquo;re back today with more \u0026lsquo;insider info\u0026rsquo; on upcoming changes to Crunchy Postgres via Automation (CPA). If you missed any of the prior posts, you can catch up here. We recently committed support for HAProxy 3.0.x into development and changed our default HAProxy configuration around to accommodate the version bump.\nOne of the first things we changed is how we build our configuration. Recent versions of HAProxy have the ability to use a configuration directory instead of (or in addition to) a configuration file, and now that we build and ship our own binaries across all our supported OSes, I\u0026rsquo;ve been eager to switch to a configuration directory. With the upcoming release of CPA 2.3.0, we will be dropping multiple configuration file \u0026lsquo;stubs\u0026rsquo; into /etc/haproxy/cfg.d. We\u0026rsquo;ll be creating a file for our global directives, a file for our defaults, a file for our stats configuration (unless disabled the user), and one file for each CPA cluster that this HAProxy is a part of. This makes it significantly easier for us to configure and reconfigure HAProxy as we stand up/tear down CPA clusters and I\u0026rsquo;m personally very happy to be able to simplify the code instead of piecing a full configuration together by hand.\nNext up, we decided to finally fix our logging configuration. For the longest time, we\u0026rsquo;ve simply used the default log and called it \u0026lsquo;good enough\u0026rsquo;. As part of moving to HAProxy 3.0.x, we took the time to reevaluate our logging and we\u0026rsquo;re now using tcplog for our front and back ends while using httplog for the statistics listener (if enabled). We\u0026rsquo;ve additionally configured the log to be stderr format iso local7 to give us a more structured log format that is emitted for journalctl to capture. Finally, we set dontlognull and logasap to ensure that log entries appear as quickly as possible and are actually useful.\nWhile we were tweaking things, we made some small security tweaks: we enabled rejection of privileged ports, and we configured the default SSL ciphers to use PROFILE=SYSTEM which means any hardening you\u0026rsquo;ve done to the system SSL (disallowing ciphers, etc) will be automatically inherited by HAProxy.\nGiven the multi-core nature of today\u0026rsquo;s servers, we have decided to configure HAProxys nbthread and cpu-map automatically based on the number of cores in the machine. We\u0026rsquo;re a little conservative with our settings since we can\u0026rsquo;t easily know of the HAProxy node is dedicated, but users should still see noticeable improvements in HAProxy performance.\nAnd finally, we enabled \u0026lsquo;stick tables\u0026rsquo; based on destination for our backends. This should help route connections faster under load.\nIn all, a bunch of minor QoL improvements that should add up to a decent impact on our HAProxy experience. There\u0026rsquo;s certainly a lot more that could be done, especially around TLS/SSL, but I\u0026rsquo;m hopeful this will give us a good enough base until we can replace HAProxy in the mid-term.\n:wq\n","permalink":"https://hunleyd.github.io/posts/coming-soon-to-cpa---haproxy-3.0.x/","tags":["work","cpa","ansible"],"title":"Coming Soon to CPA - HAProxy 3.0.x"},{"categories":["work","cpa","ansible"],"contents":"Continuing the Crunchy Postgres via Automation (CPA) blog series, we\u0026rsquo;re back today with another \u0026lsquo;Coming Soon\u0026rsquo; entry, where we discuss items that have been merged into development and will appear in a future release. This time around, we\u0026rsquo;re adding a new supported OS platform, dropping an existing OS platform, and raising the minimal version of ansible-core.\nFirst up, we\u0026rsquo;re happy to announce that support for Ubuntu 24.04 (Noble Numbat) has been added. Noble is an LTS release, and like both Focal and Jammy before it, we will maintain support for it in CPA for the life of the release. We recommend that Noble users use the Ansible PPA to install ansible-core 2.17.7 if it will be used as your Ansible controller. Many thanks to our Build team for getting us package parity with our other supported OSes and allowing us to fully support Noble.\nNext up, we\u0026rsquo;re officially dropping support in CPA for Ubuntu 20.04 (Focal Fossa); this LTS release will become end up life upstream in April, so it\u0026rsquo;s time to retire it as a supported CPA platform. Existing releases of CPA will continue to function as normal against Focal, but upcoming CPA releases will refuse to work with it. We advise you to plan on upgrading to a supported LTS release as quickly as you can.\nThe eagle-eyed among you might wonder why it took us almost a year to add support for Noble. The answer is that Focal was still supported and we didn\u0026rsquo;t wish to bloat our supported OS matrix. We currently support Ubuntu Focal and Jammy as well as RedHat 8 and 9 and have our hands full managing the packaging and dependencies with just those four releases. Therefore, we decided that we would add Noble when we drop Focal. Likewise, we\u0026rsquo;ll add RedHat 10 when we drop RedHat 8. Or at least, that\u0026rsquo;s how we\u0026rsquo;ll handle things at our current staffing levels.\nFinally, by dropping support for Ubuntu Focal, we can raise the floor on our supported Ansible versions. Currently we support ansible-core 2.13.3 - 2.18.x, but the upcoming release will raise that to ansible-core 2.14.17 - 2.18.x. (oddly, RedHat 9 has 2.14.17 in the default system repos while RedHat 8 has 2.16.3). For the protection of our clients, we also recently made the Ansible version check an actual failure and not just a warning. So this new floor and ceiling will be enforced in the code.\nOK, that covers the newest upcoming hotness for CPA (for now). Until next time, keep hacking!\n:wq\n","permalink":"https://hunleyd.github.io/posts/coming-soon-to-cpa---updated-oses-and-ansible/","tags":["work","cpa","ansible"],"title":"Coming Soon to CPA - Updated OSes and Ansible"},{"categories":["work","cpa","ansible"],"contents":"Welcome back to this blog series on Crunchy Postgres via Automation (CPA). We\u0026rsquo;re doing something a little different today; we\u0026rsquo;re going to discuss something that is merged in development but isn\u0026rsquo;t released yet: package pinning.\nOn Dec 13, I merged a PR to automatically pin and unpin packages that CPA installs so that we no longer have to worry about accidental upgrades when the system is updated:\nhunleyd merged commit 914213e into CrunchyData:development Dec 13, 2024 15:20 EST This PR teaches the pkgmgr role to versionlock/pin packages as we install them for the packages we care about. Said locks/pins are removed as needed to allow upgrades/removes as well. And indeed, standing up a CPA cluster with this patch results in a situation like the following on the target nodes:\n[keith@cd-dc1-pg1 ~]$ sudo dnf versionlock list postgresql17-libs-0:17.2-0Crunchy.el9.* crunchy-backrest-0:2.53.1-0Crunchy.el9.* policycoreutils-python-utils-0:3.6-2.1.el9.* postgresql17-contrib-0:17.2-0Crunchy.el9.* postgresql17-server-0:17.2-0Crunchy.el9.* python3-psycopg2-0:2.9.9-0Crunchy.el9.* pgmonitor-pg17-extension-0:2.0.0-1Crunchy.el9.* python3-patroni-etcd-0:2.1.7-0Crunchy.el9.* node-exporter-0:1.7.0-6Crunchy.el9.* pgmonitor-node-exporter-extras-0:5.1.1-1Crunchy.el9.* pgmonitor-sql-exporter-extras-0:5.1.1-1Crunchy.el9.* sql-exporter-0:0.15.0-1Crunchy.el9.* pgbadger-0:12.4-0Crunchy.el9.* vagrant@ha-192-168-122-16:~$ sudo apt-mark showhold acl libpq5 node-exporter pgbackrest pgbadger pgmonitor-node-exporter-extras pgmonitor-pg16-extension pgmonitor-sql-exporter-extras postgresql-16 postgresql-common python3-patroni-etcd python3-psycopg2 reprepro sql-exporter As you can see, for both RedHat-based and Debian-based systems, we now lock the specific versions of critical packages. This allows the normal system patching to occur without fear of accidentally altering the components we rely on. This solves a long-standing issue with deployed clusters in the field where the Ops team isn\u0026rsquo;t the same team deploying CPA and isn\u0026rsquo;t made aware of the necessary package exclusions.\nTo accomplish this, we\u0026rsquo;ve added the ansible.builtin.dpkg_selections and community.general.dnf_versionlock modules to our code and we\u0026rsquo;ve had to enable force_apt_get on our Debian package install task. Finally, we expanded the package install task file to first unpin the package in question, then install/update/remove said package, update the package facts if we made a change, and finally pin the package if we did an install or an upgrade. It\u0026rsquo;s a little bit slower in execution, both because of the extra steps and because we have to loop over the list of packages, but it\u0026rsquo;s a performance hit we have to take in the name of correctness and resiliency.\nThis new feature will be included in CPA 2.3, which will be available in the first half of 2025.\n:wq\n","permalink":"https://hunleyd.github.io/posts/coming-soon-to-cpa---package-pinning/","tags":["work","cpa","ansible"],"title":"Coming Soon to CPA - Package Pinning"},{"categories":["work","ansible","cpa"],"contents":"Prior posts in this series have discussed the Crunchy Postgres via Automation product, the team behind it, and some of our processes and tooling. This week, we\u0026rsquo;re going to dive into the collection itself.\nCollection? Yes, the CPA product is really an Ansible collection. To quote the docs:\nCollections are a distribution format for Ansible content. You can package and distribute playbooks, roles, modules, and plugins using collections. A typical collection addresses a set of related use cases. For example, the cisco.ios collection automates management of Cisco IOS devices.\nThe CPA product, or crunchydata.pg collection, is a set of roles, modules, and sample playbooks and inventories created to automate the deployment of Postgres clusters. It can stand up a single-node, non-highly-available Postgres instance, a multi-node replicated Postgres instance, and a multi-node highly-available Postgres instance. And because we believe in complete solutions at Crunchy Data, the collection also includes several auxiliary components as well.\nLet\u0026rsquo;s take a look at the whole collection first: The first three entries are typical Ansible collection stuff so we won\u0026rsquo;t go over those. Next up, you can see that we ship several example playbooks as part of our collection. These playbooks are broken up into four sections: one each for playbooks specific to highly-available clusters, one for non-highly-available clusters, one for playbooks that are common across both highly-available and non-highly-available clusters, and finally playbooks specific to deploying pgMonitor separately (we deploy pgMonitor by default as part of our highly-available clusters).\nNot shown, our playbooks directories also include example Ansible inventories as well as group_vars. While most customers will alter these example playbooks and inventories, or write their own from scratch, we believe in shipping fully functional examples as part of the collection for reference. Additionally, we use these examples for our own internal testing, guaranteeing they are fully functional and reflect our recommended best practices with the current collection.\nNext up, we include a sole Python module as part of the collection currently. It\u0026rsquo;s not intended for use by customers directly, but our roles make use of it directly. It\u0026rsquo;s a fairly simple module and works well enough, but if my planned refactoring (which I\u0026rsquo;ll discuss in a later post) happens then it\u0026rsquo;ll be removed. So let\u0026rsquo;s not dwell on it now.\nFinally, we come to the collection\u0026rsquo;s roles, which are the heart of the product. As you can see, we currently have eighteen roles in the collection! Which honestly seems like a lot, but I already have plans in the near term to break some of the existing roles up into more targeted roles, and want to add a new component entirely which will get it\u0026rsquo;s own role. We\u0026rsquo;re working really hard at applying the Unix philosophy of \u0026lsquo;do one thing and do it well\u0026rsquo; to our roles, so while juggling all these can be a bit much at times, we feel the trade-off is worth it.\nMost of these will be self-evident, but let\u0026rsquo;s dive into these roles, shall we?\nalertmanager This role is responsible for (optionally) installing Alertmanager, setting up notification channels, and configuring alerting rules so that you know what is going on with your Crunchy Postgres cluster at all times. This role even supports running multiple Alertmanager nodes in their own highly-available configuration. backrest This role is responsible for installing pgBackRest, configuring the backup repository, and executing any requested backups/restores. blackbox_exporter A sub-component of pgMonitor, this role is responsible for optionally deploying the Prometheus blackbox_exporter onto monitored nodes and configuring Prometheus to report on its metrics. common An 'internal' role that the other roles depend on. Due to the *intricacies* of the twenty-two level Ansible variable hierarchy, this role was born out of a need to make cross-role access to certain variables easier. Its use has expanded since then, and it now handles those things that \"should be set up for all other roles to find/use later\". This role is not designed for users to make use of it. etcd This role is responsible for installing etcd, configuring the etcd nodes to talk to each other, and then setting up the DCS for Patroni. You could say this role and the Patroni role are the 'heart' of our highly-available clusters. firewalld One of those 'auxiliary' pieces mentioned above, the firewalld role will optionally install and configure firewalld on nodes of the cluster, adding the appropriate allow rules as needed for components to talk to each other. Not strictly needed by the CPA product, but as Crunchy Data believes strongly in best practices and system security, this role was added to reduce the friction for our users in deploying security-conscious configurations. grafana Responsible for (optionally) installing Grafana and setting up the default dashboards shipped with pgMonitor, this role works both with an existing Grafana install or a new CPA-installed Grafana. haproxy HAproxy is used in CPA to route incoming connections to the current Patroni leader (or a follower, depending on what connection type you asked for) without the client/application needing to know the status of the Patroni cluster. This role will install the proxy and configure the appropriate front-end and back-end stanzas based on your deployment choices. keepalived Another 'auxiliary' role not strictly needed by CPA itself, the keepalived role exists to make deploying services that aren't highly-available by themselves fault tolerant. For example, once can deploy multiple HAProxy nodes and then have the keepalived role set up and maintain a virtual IP (VIP) to 'float' between them. This role was created based on feedback from customers and has since found fairly widespread use 'in the field'. patroni Obviously *the* linchpin in our highly-available deployments, this role installs and configures Patroni such that it can connect to and monitor the health of the Crunchy Postgres nodes, record said state in the etcd DCS, perform switchover/failover operations, and rebuild/resurrect failed nodes. pgbackup This role was a mistake and is something I'm going to kill at the first opportunity. This role's original intent was to be a singe point of entry for handling any kind of physical or logical backup a user might want. The role works well enough, but having a whole role that basically acts as a redirection into the backrest and postgresql roles tasks just seems ridiculous when our playbooks can already abstract that away. pgbouncer Responsible for installing, configuring, and starting PgBouncer to manage our inbound connection pool, this role is fairly clever in that it configures PgBouncer with two entries for each database in the CPA cluster; one of the entries is for 'rw' traffic and the other for 'ro' traffic. These separate pools point at the respective HAProxy front-ends and route you transparently to the current Patroni leader or to one of the followers as requested. If I can get the time to make it happen, we will be adding support for PgCat in 2025, and then dropping both this role and HAProxy in favor of PgCat in 2026. pgmonitor Probably my least favorite of all the roles (sorry Keith), not because there's anything wrong with the role, but because it does too much IMHO. This role is responsible for every pgMonitor component that doesn't currently have it's own role as well as (re)configuring all the pgMonitor bits to work together. I fully intend for this role to die soon, replaced by individual roles for each and every component of pgMonitor. pkgmgr The star of CPA 2.x, this role is responsible for working with our BoMs, configuring hosts with local software repositories and handling any package operations (installs, upgrades, removals). It's our newest role, and probably one of the better written ones (even if I do say so myself). postgresql The reason for the entire collection, this is the role that installs, configures, and manages Crunchy Postgres. As mentioned above, it can stand up a single Crunchy Postgres node (as a leader or a follower), it can stand up a cluster of Crunchy Postgres nodes that follow each other (in a non-highly-available manner) and it can stand up a highly-available Crunchy Postgres cluster. preflight A role designed to be run once at the beginning of a CPA deployment that will inspect your configuration, examine the targets nodes, and theoretically alert you to any issues that might exist. Ideally, this would happen up front, and allow you to rectify things without getting 47% through a deployment and having it blow up on you. This role has been through a few iterations now, mostly because of the extremely poor way that 'meta dependencies' are handled in Ansible. I have plans for another complete refactor of this role that will also kill the Python module we ship (mentioned above). prometheus Responsible for (optionally) installing and configuring Prometheus as part of our pgMonitor deployment. scheduler The last of our 'auxiliary' roles, this role was born of a need to standardize scheduling tasks in a CPA clusters (backups, bloat monitoring, etc). With this role, you simply specify if you want to use `cron` or a Systemd timer, give the schedule, and specify the command to run, and we'll add/update/remove the schedule entry. This role could arguably be a set of tasks in the common role, but it see enough use/re-use that it lives as its own entity. There you have it; the collection as it exists today. Certainly not the largest Ansible collection I\u0026rsquo;ve ever seen, but also not a trivial collection. The collection today supports RedHat 8 and 9 (and it\u0026rsquo;s many many clones) as well as Ubuntu Focal and Jammy. Additionally, we always support all five of the current Crunchy Postgres releases. I know there are plenty of things the collection doesn\u0026rsquo;t do, or \u0026ldquo;should\u0026rdquo; do differently, but I\u0026rsquo;m pretty pleased with what it does support and how adaptable it is.\nNext time, we\u0026rsquo;ll go over some of the interesting things that we do in our roles. Until then, feel free to hit me up at @hunleyd@fosstodon.org if there\u0026rsquo;s something specific you\u0026rsquo;d like me to write about in this series.\n:wq\n","permalink":"https://hunleyd.github.io/posts/anatomy-of-a-collection-dissecting-cpa/","tags":["work","ansible","cpa"],"title":"Anatomy of a Collection; Dissecting CPA"},{"categories":["work","ansible","postgresql","cpa"],"contents":"As previously mentioned in this Crunchy Postgres via Automation series, we now use a Bill of Materials (BoM) to ship tested-together versions of the components that comprise the CPA product. Accordingly, when the PostgreSQL Global Development Group (PGDG) releases new versions of PostgreSQL as they did yesterday, we have to iterate our BoMs to include these releases.\nThis responsibility falls onto the Crunchy Data internal Build team, who not only compiles the newly released PostgreSQL minor releases on all our supported platforms, but also version bumps or recompilations all the additional packages that depend on PostgreSQL (this includes extensions like PostGIS, pgvector, and pgaudit among many others). Once all these compilations are complete and have passed the regression tests, they then assemble twelve new BoMs for my team to review and incorporate.\nWhy twelve? There is one BoM each for both of our supported Ubuntu releases, and then one BoM for each of the five PostgreSQL supported major versions for RedHat 8 (and compatible) and another five of the same for RedHat 9 (and compatible). These new BoMs are then put up for review, and finally incorporated into CPA itself. This BoM generation and review process is repeated for each of the currently released CPA lines as well as the current development line (making *thirty-six new BoMs as of today). These new BoMs are deliberately limited in scope to only the PostgreSQL minor release and the changes needed to support them. It ends up looking something like this diff: Having merged these new BoMs into the BoM repository, my team then switches to the CPA repository, updates the BoM submodule for the given branch, and then begins the release process for a minor release of CPA. As PGDG released new PostgreSQL minor releases yesterday, you can expect Crunchy Data to announce minor releases of CPA (2.2.1, and 2.1.6) sometime next week. And since PGDG generally follows a quarterly release schedule, that means that CPA does as well. Of course, we might issue a CPA release outside of this quarterly schedule too if any of our other components have a CVE or other significant issue.\nNow about those CVEs. Yesterday\u0026rsquo;s PGDG releases resolve four different CVEs:\nCVE-2024-10976 CVE-2024-10977 CVE-2024-10978 CVE-2024-10979 Please upgrade your PostgreSQL clusters as soon as your chosen PostgreSQL vendor makes these releases available!\n:wq\n","permalink":"https://hunleyd.github.io/posts/on-cves--boms--and-releases/","tags":["work","ansible","postgresql","cpa"],"title":"On CVEs, BoMs, and Releases"},{"categories":["work","cpa","ansible","postgresql"],"contents":"Continuing our series on Crunchy Postgres via Automation, we\u0026rsquo;re here this week to discuss the highlights of our latest release line, v2.2. If you haven\u0026rsquo;t already, you can catch up on the differences between v1 and v2 and then come back, we\u0026rsquo;ll wait for you.\nI should probably expand upon that whole \u0026lsquo;release line\u0026rsquo; thing, shouldn\u0026rsquo;t I? If you read my post about our team structure, you\u0026rsquo;ll recall that our Sustaining team is responsible for backports and maintenance, and \u0026lsquo;release lines\u0026rsquo; directly play into that. With the release of v2.2 yesterday, the Sustaining team will now be responsible for carrying both v2.2 and v2.1 forward with bugfix and maintenance releases, as well as our 1.15 legacy release line. The Development team has now turned our focus on what will be the eventual v2.3 line. Our ultimate goal is to have five concurrent release lines which will allow us to iterate quickly while still allowing for our customers to have plenty of time to vet new releases and go through their compliance processes.\nSo, what\u0026rsquo;s new in the v2.2 line? Let\u0026rsquo;s dive in!\nPostgreSQL 17 support As one would expect from a PostgreSQL company like Crunchy Data, we quickly integrated this years major PostgreSQL release into our product and ensured that it is fully supported by all our components. With this addition, CPA v2.2 now supports PostgreSQL 13 - 17. PostgreSQL 12 de-supported Going hand-in-hand with adding support for PostgreSQL 17, we have removed support for PostgreSQL 12. We're slightly ahead of the PostgreSQL Community on this, but we will continue to support PostgreSQL 12 over the life of CPA v2.1. pgMonitor 5.x support We've rolled the latest and greatest release of pgMonitor into CPA v2.2. This brings in Grafana 10.x and also introduces a new exporter, sql-exporter. You can read more about what's new in pgMonitor in its Changelog. While we continue to support the use of, and indeed still default to, postgres-exporter it should be considered deprecated. CPA v2.2 can optionally converting existing installs from postgres-exporter to sql-exporter. Support for directly monitoring PgBouncer was added In this release, it is now possible to configure the collection of metrics from PgBouncer *without* needing to use the pgbouncer_fdw. We continue to support the use of the FDW for now, but it should be considered deprecated. Ansible-core 2.17.x support The CPA v2.2 release is coded and tested against Ansible-core 2.13.13 - 2.17.x. Since Ansible-core 2.17.x requires Python 3.7+, the CPA v2.2 code will check for and disallow running on incompatible Python versions. Ansible-core 2.12.x de-supported CPA v2.2 raises our supported Ansible-core floor to 2.13.13 and will refuse to work with Ansible-core 2.12.x and lower. Ansible-lint compliance The entire CPA v2.2 codebase, including our sample playbooks, now passes the 'production' profile of the latest ansible-lint (with yaml[line-length] as the sole skip_list rule). Oracle Linux support While Crunchy Data doesn't officially endorse or support the use of Oracle Linux, the CPA v2.2 release now properly recognizes it and treats it like any other RedHat clone. Automatic CPU Affinity suppport When deloying CPA v2.2 against target nodes multiple CPU cores, we will now automatically define CPU Affinity for both etcd and HAProxy for improved performance of both. Offline Bill of Materials support For those customers whose Ansible host is unable to connect to the Crunchy Data Access Portal, it is now possible to obtain an offline copy of the Bill of Materials and run the deployment fully offline. 28% less handler invocations A sweeping refactor of the code has significantly reduced the amount of role handler invocations (28% less on average). This reduction dramatically speeds up the execution of CPA v2.2 roles. Many smaller changes are also included in the CPA v2.2 release, the full list of which is available to customers in our Release Notes. These changes include several component version bumps for bugfixes and CVEs as well as various bugfixes to the CPA collection itself.\nThe team and I hope everyone enjoys this release and we look forward to hearing what our clients think of this release. We have a lot more planned for future CPA releases which I\u0026rsquo;ll share in later posts.\n:wq\n","permalink":"https://hunleyd.github.io/posts/crunchy-postgres-via-automation-v2.2/","tags":["work","cpa","ansible","postgresql"],"title":"Crunchy Postgres via Automation V2.2"},{"categories":["work","cpa","developer","agile"],"contents":"Welcome back to our blog series discussing Crunchy Postgres via Automation (CPA) and the team behind it. This week, we\u0026rsquo;re going to talk about the tooling used by the team and discuss some of the agile-lite processes we\u0026rsquo;ve adopted.\nCode GitHub Like a lot of other teams, our code is hosted on GitHub under the company\u0026rsquo;s organization. We currently make use of three different repositories: our main repository hold the actual Ansible collection, while our Bill of Materials (BoM) is hosted in a second, and finally our documentation\u0026rsquo;s Hugo theme is in a third repository since it\u0026rsquo;s the official corporate theme shared among many teams.\nGit Obviously, we use Git for our source code. Our main source repository has the BoM and Hugo theme as submodules which allows us to tie version branches together across the different repositories. A majority of the team uses worktrees to manage their development. If you\u0026rsquo;re not familiar with them, Graphite has a nice write-up on worktrees. IMNSHO, worktrees are the best way to multi-task within a given repository. I even wrote a tool to automate my worktree workflow that others on the team have started using. Finally, the team uses SSH keys for commit signing.\npre-commit To enforce good Git behavior, we make heavy use of pre-commit in our repositories. We use it to enforce conventional commits, perform linting and enforce several \u0026lsquo;repo standards\u0026rsquo;.\nVagrant / VirtualBox The entire team uses Vagrant to manage our virtual machines for testing using a custom Vagrantfile managed by yours truly. At the moment, ¬æ the team uses VirtualBox virtual machines with one holdout using libvirt (though they plan to convert to VirtualBox soon for consistency and ease of troubleshooting). Our Vagrant boxes are actually built by me using Packer and the templates maintained by the Chef team (the irony is not lost on me).\nDocs Hugo As mentioned above, we currently use Hugo to generate our product documentation for our customer portal though we\u0026rsquo;ll be moving to something else before the new year.\nantsibull-changelog One of the first things I implemented after taking over as Lead Architect is using antsibull-changelog to auto-generate our Changelog at release time. Using this tool pairs nicely with the conventional commits mentioned above and makes it so much easier to ensure nothing gets forgotten. We even have a pre-commit check that ensures you\u0026rsquo;ve added a Changelog fragment.\nNotion Crunchy brought Notion into the mix a while back, and while I don\u0026rsquo;t exactly love it we do use it fairly regularly. We house our team contract, our decision records, and our sprint retrospectives here. We use Notion to record various processes (like our release process), our patch policy, and miscellaneous other documents as well.\nBots Geekbot We use Geekbot to handle our daily standup, our sprint retrospectives, and to ask the team if they have anything to demo. I absolutely love Geekbot and the team behind it and even managed to get the CPK team to use it. Asynchronous, Slack-based meetings that are coordinated, rolled up, and reported on by a bot just make sense IMHO.\npatchback-bot After running into patchback-bot while contributing to the Ansible community.postgresql collection, I quickly adopted it in our main repo. We use this bot to make the sustaining team\u0026rsquo;s job easier. Our bugfix workflow is such that all fixes land in development first (assuming, of course, that said bug exists there). Once the PR is reviewed and approved, we let this bot attempt to port the fix to our older release lines. It works probably 80% of the time, and when it doesn\u0026rsquo;t it updates the PR in question with directions on how to handle the port manually. While it\u0026rsquo;s not perfect, the time that it does save quickly adds up.\nReclaim While most of the team does not, I make heavy use of Reclaim. I have it fully integrated with my Google Calendar and our issue tracker and it manages my workload as well as my 1:1 mtgs, sprint meetings, blocking time for code review, and several other \u0026lsquo;habits\u0026rsquo; (in their terminology). I love this tool so much I pay for it out of my own pocket. I absolutely struggled to stay on top of all my tasks and meetings before, trying to keep my focus on high priority tasks, trying to make sure that tasks I\u0026rsquo;d \u0026lsquo;completed\u0026rsquo; (put up a PR) continued to get attention until they were merged, ensuring I was available to the team for mentorship and co-debugging among any ad hoc items that sprung up killed my productivity and basically ensured that something got lost in the shuffle. With Reclaim, I simply let it schedule my meeting and tasks, and if I need to bump something it\u0026rsquo;s a simple click and Reclaim will reschedule it. Nothing gets missed, and everything gets its share of attention. I truly don\u0026rsquo;t think I can go back to working without Reclaim at this point.\nIssues Linear You might expect us to use GitHub issues, but Crunchy brought Linear in-house for all teams to use, and I jumped on it. I love the integration with Reclaim and Slack and GitHub. Configuring workflows that auto-update Linear issue\u0026rsquo;s status when someone puts up a non-draft PR on GitHub and again when said PR is merged; being able to link Slack threads to Linear issues and have a conversation between other developers, the Build team, and our internal customers that stays in sync between Slack and Linear; and having Linear issue\u0026rsquo;s status auto-update based on Reclaim\u0026rsquo;s knowledge of what I\u0026rsquo;m working (and having \u0026rsquo;time spent\u0026rsquo; auto-recorded in Linear by Reclaim) are just some of the many reasons I love Linear. I absolutely love that we can block our development issues against Build issues, while our Solutions Architect team blocks their delivery issues against ours making the interdependence obvious and auto-notifying everyone as issues resolve. I am so much happier with Linear than I was with GitHub issues.\nWrap-up So there you have it, all the \u0026lsquo;major\u0026rsquo; tooling we use currently. And several references to our agile-ish approach to software development: standups, sprints, kickoffs, and retrospectives. I\u0026rsquo;m not a huge fan of Agile (big \u0026lsquo;A\u0026rsquo;) or Scrum, but after reading The Phoenix Project (and buying a copy for everyone on the team), I do think there\u0026rsquo;s merit in being agile (little \u0026lsquo;A\u0026rsquo;). I\u0026rsquo;m sure things will change over time, but for now we\u0026rsquo;ve \u0026lsquo;borrowed\u0026rsquo; and adapted a few practices and the team seems to run rather well. For those that care about these things, we\u0026rsquo;ve adopted a two-week sprint duration, we plan for six sprints per release, and we use a Fibonacci point system that allows 0 points (which we reserve for epics).\nI think that about covers things for this topic. Next time, we\u0026rsquo;ll discuss the brand new CPA 2.2.0 release!\n:wq\n","permalink":"https://hunleyd.github.io/posts/sometimes-you-should-blame-your-tools/","tags":["work","cpa","developer","agile"],"title":"Sometimes You Should Blame Your Tools"},{"categories":["work","cpa","developer"],"contents":"Welcome to our third installment in this blog series discussing Crunchy Postgres via Automation(CPA). This week, we\u0026rsquo;re going to talk about the team that I work with, it\u0026rsquo;s membership, its structure, the responsibilities, etc.\nLet\u0026rsquo;s start with the team membership:\nGreg Our Product Manager that is shared with the Kubernetes team Doug (me) The Lead Architect for CPA Keith The Lead Architect on pgMonitor and pg_partman as well as the senior most developer on CPA Jim In charge of maintenance on all released CPA lines Yorvi Our newest member, who splits his time between us and Support The eagle-eyed among you might have noticed I didn\u0026rsquo;t list a team lead or team manager. We\u0026rsquo;re currently self-governing/self-directing with Greg and myself acting as co-manager as needed and escalating as needed to Craig K. It\u0026rsquo;s probably not ideal, but it works for now, so why mess with it? üòâ\nYou might be thinking, \u0026ldquo;that\u0026rsquo;s not bad.. a tightly knit team of four developers can get a lot done\u0026rdquo;. And you\u0026rsquo;re not wrong, we do get a lot done, IMHO; especially when you look closer and realize that only two of those four are allocated 100% of the time to CPA. I\u0026rsquo;m actually quite proud of what this team accomplishes with the constraints we have.\nIn addition to developing CPA, this team is also responsible for providing internal support to our Solutions Architect (SA) team as they craft and deploy highly-available solutions for/with our customers. At any point, any member of the SA team can hop on Slack and ask for assistance in deploying or troubleshooting CPA at a customer\u0026rsquo;s site and get help from my team. As any developer will tell you, interrupting flow is a bad thing, but we can\u0026rsquo;t leave our guys and gals in the field hanging, right? And so, one of the first things I did after accepting the Lead Architect role was to split the team into two sub-teams. üò±\nSub-teams?! Yes, that\u0026rsquo;s right. It became pretty apparent that forward momentum was being halted constantly due to interruptions. An SA would show up in our help channel asking for assistance and one or several of us would stop our work and attempt to help them. If several of us were trying to help, it would sometimes be confusing for the SA to know who to answer first, which suggestions to try first, etc. And my team would often spawn a side-conversion about who was going to help and who could go back to coding, or ask our own questions about why a suggestion was made, or why not suggest $x instead. Additionally, it was kind of a \u0026lsquo;free for all\u0026rsquo; when it came to who was working on bugfixes and who was working on $cool_new_feature, which led to a lot of feature work and begrudging bug fixing. ü™≥\nSo I quickly borrowed a page from my time at VA Software(RIP) and split the team into two groups. I called these groups our \u0026lsquo;sustaining engineering\u0026rsquo; and our \u0026lsquo;development engineering\u0026rsquo;.\nThe sustaining team would be \u0026lsquo;interrupt driven\u0026rsquo; and deal with our internal help duties. They would be responsible for responding to the SA inquiries, joining calls, debugging logs, and whatever else to get the SA over their hurdle. They were empowered to @ people for help as needed, with the understanding that they\u0026rsquo;d done all the needful to bring people up to speed on what the issue was thought to be, what had been tried, etc. When not delivering help to the SA team, they would be responsible for maintaining currently released product lines of CPA. It was their job to review PRs against the development branch, examine and back-port bugfixes as needed, and coordinating point releases (2.1.1, 2.1.2, etc).\nThe development team would, obviously, work on development to drive the product forward. They worked primarily on features, large refactors, and paying down debt. New releases of CPA (2.1.0, 2.2.0, etc) are born of the fruits of this team\u0026rsquo;s efforts. They were not expected to respond to the internal help channel but would be available for \u0026rsquo;escalations\u0026rsquo; from the sustaining team. Finally, they were expected to help mentor and train members of the sustaining team for eventual promotion onto the development team.\nI drafted up a team contract describing this division of labor and setting some loose SLAs for internal help issues and shopped the contract around a bit. With some tweaking, the team adopted the contract and we shared it with our SA and Support teams, and we\u0026rsquo;ve been following this model ever since.\nWe still do issue triage and sprint planning together as a group (for lots of reasons), but generally Keith and I focus on driving CPA forward and cutting new releases about every 12 weeks while Jim and Yorvi focus on delivering back-ports and bugfixes in point releases. It\u0026rsquo;s worked out really well for us so far and everyone on the team is much happier. Jim has stepped up to be the unofficial lead for the sustaining team, and is the Release Manager for point releases, which freed up a significant amount of my time.\nAlong the way, we decided to have the Sustaining team \u0026rsquo;take the lead\u0026rsquo; on bugfixes against the development branch as well, both as a mentored learning experience and because they usually end up having to back-port the fix to release branches anyway. This has also helped us to increase development velocity. üèÅ\nIs there more tweaking to be done? Probably. Could we use another body or two? Of course. Does this structure work for us, make everyone happier, and increase our productivity? Absolutely.\nIn my next post, I\u0026rsquo;ll go more into how we\u0026rsquo;ve adopted agile (little \u0026lsquo;a\u0026rsquo;), and some of the tooling we use. See you then!\n:wq\n","permalink":"https://hunleyd.github.io/posts/reconstructing-a-development-team/","tags":["work","cpa","developer"],"title":"Reconstructing a Development Team"},{"categories":["work","cpa"],"contents":"As discussed last week, I\u0026rsquo;m back today to highlight the major differences between the 1.x and 2.x lines of Crunchy Postgres via Automation(CPA).\nCPA 1.x When we initially designed and launched the 1.x line of CPA, we attempted to leverage the OS components as much as possible to reduce our internal support burden and ship things faster. This means that CPA would deploy the Crunchy Certified Postgres repository, but install packages from both it and the normally configured OS repositories. All the \u0026lsquo;core\u0026rsquo; Crunchy Postgres components would come from us, but most other things like etcd or HAProxy would come from the system itself.\nAs you can imagine, this quickly led to issues:\nEach of our supported OS platforms shipped with different versions of these components Each of our supported OS platforms shipped new releases at different schedules All of our customers have different schedules for updating their OS Additionally, we ran into issues where:\nSome customers don\u0026rsquo;t allow their nodes to access the Internet Some customers use internal mirrors of repositories and their refresh schedule is unknown to us I won\u0026rsquo;t bore you with all the details of the pain these issues caused, but it was bad enough that we worked with our internal Build team to create \u0026lsquo;dummy\u0026rsquo; packages (.rpm and .deb) that we called \u0026lsquo;crunchylocks\u0026rsquo;. These packages were empty but had very tight dependencies. For example, the \u0026lsquo;crunchylock\u0026rsquo; package for etcd might have had a dependency of 3.4.3 \u0026lt;= etcd \u0026lt;= 3.4.8, which would, in theory, limit etcd to known working versions. These lock packages worked, mostly, but became a maintenance burden in and of themselves. And when they didn\u0026rsquo;t work, for example when using the best option for dnf when upgrading CPA, they blew up spectacularly.\nSpeaking of CPA upgrades, it now involved selectively removing/upgrading specific \u0026lsquo;crunchylock\u0026rsquo; packages, in the correct order, in lock-step with upgrading the actual components and re-installing existing \u0026lsquo;crunchylock\u0026rsquo; packages if the upgrade failed. The upgrade code became pretty gnarly pretty quickly. Upgrading was, quite frankly, the single biggest \u0026lsquo;issue\u0026rsquo; with CPA 1.x as a whole.\nAdditionally, the CPA 1.x line was limited to Ansible 2.9.6 compatibility for most of it\u0026rsquo;s life. This become a large issue with customers running Ansible Automation Platform (AAP) and a growing issue for development efforts.\nSomething needed to change.\nCPA 2.x As I sat down to plan out the 2.x line in my new role as Architect1, it quickly became apparent that no one was happy with the above; customers, Support, our Solution Architects, my team, and the Build team all wanted, well, something else.\nSo I spent time brainstorming with John, our Build team lead, and we devised a strategy to build all the necessary components in-house and ship everything we directly relied upon. We further decided, based on a suggestion from John, to utilize a Bill of Materials (BoM) to list exactly what we built and shipped. Each version of the BoM would have a 1:1 relationship with CPA 2.x release, e.g. CPA 2.1.0 would be BoM version x, CPA 2.1.1 would be BoM version y, and so on. This would simplify debugging for both the Support and Solution Architects teams as simply knowing the version of CPA would additionally provide the exact versions of all components.\nWorking with Heath, one of our Build Engineers, I crafted a BoM that contained all the information needed for CPA to download all the components directly from the Crunchy Data software portal, verify their size and checksum, and cache these files on the Ansible controller. We would then push the files out to the target nodes, removing the need for these nodes to download anything.\nThe BoM itself is essentially a giant YAML file containing a massive dictionary that the CPA roles load as ai variable. It currently looks something like this:\ncrunchy_automation_bom: target_role_version: x.y.z bom_version: nnnn epcp_path: \u0026#39;/epcp/redhat/EL-8/x86_64/base/\u0026#39; ha_path: \u0026#39;/cpa/redhat/EL-8/x86_64/base/\u0026#39; pg_path: \u0026#39;/postgresql13/redhat/EL-8/x86_64/base/\u0026#39; packages: - { src_repo: \u0026#34;pg\u0026#34;, src: \u0026#34;postgresql13-13.16-1Crunchy.el8.x86_64.rpm\u0026#34;, pkg: \u0026#34;postgresql13\u0026#34;, version: \u0026#34;13.16-1Crunchy\u0026#34;, sha256: \u0026#34;0ecbacc38c691b6cacb95d112e1e3d770ac7ca2f3f822cd6d7ad72a0b1a088c4\u0026#34;, size: \u0026#34;1531568\u0026#34; } As you can see, we have all the information needed to construct an URL to retrieve packages from our software portal. Well, all the information except the customer\u0026rsquo;s portal credentials, which we gather elsewhere in the code ü§ê. Obviously, there are many more lines in the packages sub-dictionary (almost 90).\nWe download these packages on the Ansible controller, verify their checksum to ensure they haven\u0026rsquo;t been tampered with or that bits were munged in transit, and we store them into a cache directory on the Ansible controller. Our code is smart enough to check for packages in the cache directory first and will only download those files that are missing or do not match their checksum; which means you can even use an Ansible controller that doesn\u0026rsquo;t have Internet access since you can simply pre-populate the cache directory by obtaining a tarball from us (an oft-requested feature).\nOnce everything has been verified to be in the cache directory correctly, our code will synchronize the cache directory to all target nodes in the inventory, and then create a software repository (a .repo or .list) that points to the cache on those nodes. At that point, we can freely install our shipped packages without an Internet connection, and our repo is 100% static so customers can freely update the OS (dnf update/apt upgrade) on these nodes w/o fear of the CPA components changing. It\u0026rsquo;s a total win.\nWe also spent a non-trivial amount of time refactoring the code to leave Ansible 2.9.6 behind like the relic it is. The code now works and is tested with Ansible 2.12.10 - 2.16.x (with the upcoming CPA 2.2.0 adding support for Ansible 2.17.x!). Along with the Ansible upgrade, we now work with recent releases of community.postgresql which has allowed us to refactor even more of our code and optimize things. All in all, things are fairly good on the Ansible front now.\nOutro There are a ton of other changes between the CPA 1.x and 2.x, and we\u0026rsquo;ll cover some of them in future posts, but for now, those are the most significant changes. We have a lot planned for the 2.x line, and have even starting queuing up ideas for an eventual 3.x line.\nNext week, we\u0026rsquo;ll talk about team structure and assigned duties. I think you\u0026rsquo;ll be surprised what we\u0026rsquo;re doing with the resources we have.\n:wq\n\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://hunleyd.github.io/posts/crunchy-postgres-via-automation-what-s-new-in-v2/","tags":["work","cpa"],"title":"Crunchy Postgres via Automation: What's new in V2"},{"categories":["ansible","postgresql","work","cpa"],"contents":"In March of \u0026lsquo;23, I took over as the Lead Architect of my employer\u0026rsquo;s Ansible-based automation for creating highly-available PostgreSQL clusters.1 Since then, I\u0026rsquo;ve been responsible for advancing the product: refactoring the code, adding functionality, rethinking some of its core attributes, etc. I\u0026rsquo;ve also taken some steps to informally restructure the team that works on things to better divide up responsibilities and make everyone more productive.\nIn the past ~18 months, we\u0026rsquo;ve launched the 2.x line of the product, and are currently in the midst of testing the upcoming 2.2.0 release. I thought it might be a good time to start blogging about what the team and I are doing, so I\u0026rsquo;m going to start posting semi-regularly about our goings on, our tooling, our process, and maybe even the product itself. As such, the posts will vary in topic; some posts will be about Ansible, others about PostgreSQL, some about the rest of our stack, and a few here and there about DevOps-y type things as they relate to our tooling and processes. I will work to tag each post appropriately so that you can follow only those that interest you, but they will all be tagged with \u0026lsquo;cpa\u0026rsquo; if you want to follow everything.\nAs things currently stand, we build highly-available PostgreSQL clusters with the following components:\nPostgreSQL2 (duh) Patroni3 (for failover/switchover and \u0026lsquo;service uptime\u0026rsquo;) etcd4 (the DCS that maintains Patroni\u0026rsquo;s state) HAProxy5 (for transparently routing connections based on read vs write) PgBouncer6 (for connection pooling) pgBackRest7 (for recovery, continuous archiving, self-healing) pgMonitor8 (for monitoring the installed stack) Keepalived9 (for solving SPoF with the components that don\u0026rsquo;t natively support it)(optional) And a basic production deployment looks something like this: That about covers the overview. Next week, we\u0026rsquo;ll describe the significant differences between the 1.x product line and the new 2.x line. We might even talk a bit of future releases and what is planned for them.\n:wq\nCrunchy Postgres via Automation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPostgreSQL\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPatroni\u0026#160;\u0026#x21a9;\u0026#xfe0e;\netcd\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHAProxy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPgBouncer\u0026#160;\u0026#x21a9;\u0026#xfe0e;\npgBackRest\u0026#160;\u0026#x21a9;\u0026#xfe0e;\npgMonitor\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKeepalived\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://hunleyd.github.io/posts/automating-highly-available-postgresql-clusters/","tags":["ansible","postgresql","work","cpa"],"title":"Automating Highly Available PostgreSQL Clusters"},{"categories":["osx","productivity","apps"],"contents":"We recently moved all our issue tracking at work from GitHub Issues to Linear for one of the products that I work on. While playing around with Linear, I stumbled upon their Reclaim.AI integration and immediately fell in love. I quickly set about connecting the two tools, tweaking my settings, and even opened some issues w/ the Support teams of both products. After about a week, I finally had everything up and running exactly how I wanted it. During our sprint planning meeting, I simply pulled whatever Linear issues I wanted to work on into the cycle, and then let Reclaim schedule my work based on priority and points. While I could probably write an entire blog post about these two apps, these tools are really just the \u0026lsquo;setting\u0026rsquo; for this post. You see, now that I had all my work being scheduled for me by a AI, I found that I needed to have my calendar open at all times to see what I was supposed to be working on. I hate having apps open (or extra tabs in a browser) when I can have the info presented to me in some other fashion. Thankfully, I had previously discovered MeetingBar and was already a huge fan. So now I have MeetingBar running in my OSX menu bar showing me (via Reclaim) exactly what I\u0026rsquo;m supposed to be working on (or what I\u0026rsquo;m about to start working on). I was living the dream.\nAnd then the latest release of MeetingBar dropped with AppleScript support. I had been looking for way to automatically enable \u0026lsquo;Do Not Disturb\u0026rsquo; mode for the duration of a meeting for a while, but I had a couple of issues. Now that my entire day is a set of Reclaim \u0026ldquo;meetings\u0026rdquo; I obviously didn\u0026rsquo;t want DnD on the entire day (annoying as they may be, I do need to see my Slack notifications, lol). I really only wanted DnD mode active if the meeting in question had a URL defined. As a remote worker, my meetings are always an URL to the online meeting, so I dug into the AppleScript support and quickly realized that what I wanted was possible but more complicated than I really wanted to deal with. I scrolled around a bit more on the web looking at suggested solutions for automating DnD and eventually stumlbed upon macos-focus-mode. A quick brew install npm, followed by npm install macos-focus-mode and I was in business!\nI added the macos-focus-mode shortcut as per the directions on their website, tested it out few times manually to ensure it did what I wanted, and finally I went into the MeetingBar settings screen and rigged everything up:\nIn the Preferences, go to the Advanced tab Check the \u0026lsquo;Run AppleScript Automatically\u0026rsquo; checkbox Pick \u0026lsquo;when the event starts\u0026rsquo; from the drop-down click on \u0026lsquo;Edit script\u0026rsquo; Your script should look like this:\non meetingStart(eventId, title, allday, startDate, endDate, eventLocation, repeatingEvent, attendeeCount, meetingUrl, meetingService, meetingNotes) if meetingUrl is not \u0026#34;EMPTY\u0026#34; then set _duration to -(startDate - endDate) / 60 tell application \u0026#34;Shortcuts Events\u0026#34; run the shortcut named \u0026#34;macos-focus-mode\u0026#34; with input _duration end tell end if end meetingStart Click the \u0026lsquo;Save\u0026rsquo; button, then \u0026lsquo;Save script\u0026rsquo; button and you\u0026rsquo;re all set! Now when a meeting starts that has an URL associated with it, MeetingBar will call your saved script, which calls the macos-focus-mode shortcode which enables DnD for the duration of the meeting. It sounds like a bunch of indirection but it happens almost instantly. I couldn\u0026rsquo;t be happier with the setup.\n","permalink":"https://hunleyd.github.io/posts/meetings--menu-bars--and-focus-modes/","tags":["osx","productivity","apps"],"title":"Meetings, menu bars, and focus modes"},{"categories":["computers"],"contents":"After working on it for a bit, and convincing a coworker to use it and provide feedback, I\u0026rsquo;ve finally made gwt an actual thing. I expect no one will actually use it, but whatever, it\u0026rsquo;s up for the world to critique.\nI spend a lot of time in git for $dayjob. And I\u0026rsquo;m always working on more than one thing at a time, so I make heavy use of git worktree. If you\u0026rsquo;re not familiar, git worktree allows you to have multiple branches of the same repo checked out at the same time and you switch between them by simply using cd. Without worktrees, if you try to switch branches in git while having \u0026lsquo;in progress\u0026rsquo; changes it will tell you to commit or stash your changes first. That\u0026rsquo;s super annoying to me and is the main driver for my use of worktrees. The downside to having a ton of worktrees is having to manage them. I hate doing repetitive tasks by hand, so gwt was born to make my life simpler.\nWith gwt, you can list all your worktrees, list all worktrees in a given repo, switch to a given worktree, create a worktree, and nuke a worktree. Mentally, I wanted to mimic the flow of virtualenvwrapper with its workon command. As such, we set GWT_REPO_HOME to the directory where all your Git repos are checked out, and then we source the gwt script and we\u0026rsquo;re all set. To \u0026lsquo;work on\u0026rsquo; a given branch, you simply gwt mybranch and it will cd into the worktree, automatically pull changes from your origin if there are any, and then pull any changes from your upstream branch if there are any. It\u0026rsquo;ll also push any locally committed changes to your origin. All of these steps also output exactly what they\u0026rsquo;re doing so you know what\u0026rsquo;s going on.\nI\u0026rsquo;m sure there are tools out there that do this already. And I\u0026rsquo;m sure there are better ways to do the git things that it does. But I and a coworker find it useful, so..\n","permalink":"https://hunleyd.github.io/posts/introducing-gwt/","tags":["computers"],"title":"Introducing gwt"},{"categories":["smarthome","howto"],"contents":"I don\u0026rsquo;t like the heat. I hate being hot. Always have. I\u0026rsquo;m the guy who will leave the windows open when others have already turned on the furnace. And I used to be guy who ran the AC all the time and ran it at a very low temp. Recently though, I\u0026rsquo;ve come to realize that as long as it\u0026rsquo;s colder in here than it is out there, it\u0026rsquo;s all good. However, the delta has be more than just a few degrees. And I need the fan on.\nSo, with this realization, I set about configuring my Home Assistant to constantly tweak the AC so that it\u0026rsquo;s 10 degrees cooler inside than outside (if I\u0026rsquo;m home). To accomplish this, I use the temperature as reported by Weatherbit.io which I integrated via HACS\u0026rsquo;s integration. I already had my Nest integrated, and could have used its outdoor temperature sensor, but I found it slower to update than Weatherbit.\nThe automation is actually really simple. In fact, it\u0026rsquo;s a little too simple in that I completely forgot to account for when I\u0026rsquo;m sleeping. You see, as much as I don\u0026rsquo;t like the heat, I literally cannot sleep if I\u0026rsquo;m hot. And for whatever reason, I put off heat while I sleep, so the room has to be straight up cold for me to even have a chance of resting well. So I quickly realized that I need to check what time it is and if its late set the temp low and stop adjusting it until I get up in the morning. Which brings me to my binary sensor:\n- platform: tod name: Daytime after: \u0026#34;08:00\u0026#34; before: \u0026#34;23:00\u0026#34; As you can see, I\u0026rsquo;ve defined \u0026lsquo;daytime\u0026rsquo; as between 0800 and 2300.\nSo my automation looks like:\n- alias: Adjust the AC id: \u0026#39;1625265017\u0026#39; description: \u0026#39;Keep it 10 degrees cooler inside (but at least 72); keep it 68 at night\u0026#39; trigger: - platform: state entity_id: sensor.weatherbit_temperature condition: - condition: state entity_id: climate.living_room state: \u0026#39;cool\u0026#39; action: - service: climate.set_temperature target: entity_id: climate.living_room data_template: temperature: \u0026gt;- {%- if states(\u0026#39;binary_sensor.daytime\u0026#39;) == \u0026#39;on\u0026#39; -%} {%- if states(\u0026#39;sensor.weatherbit_temperature\u0026#39;)|int - 10 \u0026gt; 71 -%} {{ states(\u0026#39;sensor.weatherbit_temperature\u0026#39;)|int - 10 }} {%- else -%} 72 {%- endif -%} {%- else -%} 68 {%- endif -%} mode: single Essentially, every time the temperature (as reported by Weatherbit) changes, this automation kicks off. It checks if the Nest is set to \u0026lsquo;cool\u0026rsquo; mode (which means the AC is on and also means I\u0026rsquo;m home as another automation sets the Nest to \u0026lsquo;off\u0026rsquo; when I leave) and then it checks if its \u0026lsquo;daytime\u0026rsquo; or not. If it is, it subtracts 10 from the reported outside temp, checks if that value is greater than 71 degrees and if so, sets the AC to that target temp. If the result is less than or equal to 72, it sets the target temp to 72. Unless it\u0026rsquo;s not \u0026lsquo;daytime\u0026rsquo; in which case it sets it to 68 (my preferred sleeping temp).\nIt\u0026rsquo;s trivial, but it\u0026rsquo;s very powerful. And its saved me some money already, which is always nice.\n","permalink":"https://hunleyd.github.io/posts/i-said-brr-it-s-cold-in-here/","tags":["smarthome","howto"],"title":"I said brr it's cold in here"},{"categories":["smarthome","howto"],"contents":"For a while now, I\u0026rsquo;ve been automating my apartment using the automations component of Home Assistant. In fact, I had amassed quite the collection of automations and was starting to have a problem keeping track of them all and their various interactions (intentional or otherwise). So earlier this year when Home Assistant introduced automation debugging I decided to sit down and refactor the whole thing.\nAt first, I considered deploying and using MQTT which, it seems, a lot of Home Assistant users do. However, I wasn\u0026rsquo;t overly excited at running another container or at learning Yet Another Thing. Likewise, I considered Node-RED and discarded it too. I\u0026rsquo;m probably making things harder for myself by sticking to editing YAML in Vim but I knew my needs weren\u0026rsquo;t overly complicated and by sticking to \u0026rsquo;the basics\u0026rsquo; I knew it would force me to stop being overly-clever and make something that Just Works‚Ñ¢. So I sat down with the new debugging tool, wrote out all of my \u0026lsquo;what do i want automated\u0026rsquo; needs, and pored back over the automation documentation.\nAnd that\u0026rsquo;s when I realized that a timer was the component I\u0026rsquo;d been overlooking all this time. Timers, it turns out, can be used very much in an MQTT-lite fashion. You see, you can have one or more routines triggered by the same timer; in fact, some set of routines can listen for timer (re)start and another set can listen for timer finish/idle. Mix-n-match these up and you can make some very interesting actions happen. And it means my automations each generally do one thing and only one thing (KISS) which, in turn, means they are less likely to fail and easier to debug when they do. I was stoked! I was also annoyed at not having paid any attention to timers from the get-go, but what are you gonna do?\nMy first task was thinking about what timers I\u0026rsquo;d need. I knew right away I\u0026rsquo;d want a timer.away for when I leave home:\ntimer: away: duration: 00:05:00 But what other timers do I need? I quickly realized that I need a timer for almost every switch I have defined in Home Assistant. I could get away with fewer, but I allowed myself a little bit of cleverness and created one for each switch with the name of the timer being the name of the switch it controls. So, for example, I have a TP Link Kasa smart switch controlling my hallway light called switch.hallway_light. So I created a matching timer called timer.switch_hallway_light. This allows me to to write a simple automation like this:\n- alias: Trigger - Start light switch trigger id: \u0026#39;1625266493\u0026#39; description: \u0026#39;When a light turns on, start its corresponding power off trigger\u0026#39; trigger: - platform: state entity_id: switch.bar_light to: \u0026#39;on\u0026#39; - platform: state entity_id: switch.closet_light to: \u0026#39;on\u0026#39; - platform: state entity_id: switch.dining_room_light to: \u0026#39;on\u0026#39; - platform: state entity_id: switch.entryway_light to: \u0026#39;on\u0026#39; - platform: state entity_id: switch.hallway_light to: \u0026#39;on\u0026#39; condition: [] action: - service: timer.start data: duration: \u0026#39;0:10:00\u0026#39; target: entity_id: \u0026#34;timer.{{ trigger.entity_id.split(\u0026#39;.\u0026#39;)|join(\u0026#39;_\u0026#39;) }}\u0026#34; What this does is exactly what it says in the description. Whenever one of the lights in the trigger section turns on, it starts the corresponding timer. The magic is in the very last line of the automation:\nentity_id: \u0026#34;timer.\\{\\{ trigger.entity_id.split(\u0026#39;.\u0026#39;)|join(\u0026#39;_\u0026#39;) \\}\\}\u0026#34; What we\u0026rsquo;re doing here it taking the entity id that fired the automation (e.g. what switch did we turn on) from the trigger.entity_id value then we swap out the . for a _ which gives us the name of our timer. So here we have one automation, that does one very simple thing which makes it kinda hard to break this automation.\nSo we\u0026rsquo;ve turned on the hallway light, and started our timer. What happens when the timer runs out? Well, we have an automation that is triggered by that:\n- alias: Trigger - Kill the lights id: \u0026#39;1625267293\u0026#39; description: \u0026#39;When a timer runs out, kill its corresponding light\u0026#39; trigger: - platform: event event_type: timer.finished event_data: entity_id: timer.switch_bar_light - platform: event event_type: timer.finished event_data: entity_id: timer.switch_closet_light - platform: event event_type: timer.finished event_data: entity_id: timer.switch_dining_room_light - platform: event event_type: timer.finished event_data: entity_id: timer.switch_entryway_light - platform: event event_type: timer.finished event_data: entity_id: timer.switch_hallway_light condition: [] action: - service: homeassistant.turn_off target: entity_id: \u0026#34;switch.\\{\\{ trigger.event.data.entity_id.split(\u0026#39;_\u0026#39;)[1:4]|join(\u0026#39;_\u0026#39;) \\}\\}\u0026#34; As you can see, it\u0026rsquo;s basically the opposite of the previous automation. We list which timers we care about, and when they finish, we munge the timer name a bit to get the switch\u0026rsquo;s name and then we turn that switch off. Simplicity.\nHopefully, you can see the utility of timers in Home Assistant. This is just a very basic example of how I use them, but I think it illustrates their user pretty well. I still have just over 50 automations, but that\u0026rsquo;s way down from what I had, and the automations themselves are now way simpler. Overall, I consider this a win.\nHappy hacking!\n","permalink":"https://hunleyd.github.io/posts/automations--timers--and-you/","tags":["smarthome","howto"],"title":"Automations, timers, and you"},{"categories":["computers","howto"],"contents":"DNS is one of those thing most people never think about. It\u0026rsquo;s one of those things in the background that quietly does its job and no one pays it no mind. Which is why it\u0026rsquo;s surprising to people when they discover that if their DNS traffic can be logged, a very informative picture of them can be created.\nTo combat this user fingerprinting/tracking, enhancements to DNS like DNS over HTTPS and DNS over TLS come into existence. And while I\u0026rsquo;m not knocking those solutions, I would like to point out that the endpoint still knows what lookups you performed. Unfortunately, there isn\u0026rsquo;t really any way currently to ask a server \u0026lsquo;hey, whats the IP address for google.com\u0026rsquo;\u0026rsquo; and not have the other end know that you asked for Google\u0026rsquo;s website. It\u0026rsquo;s literally the nature of the task for the other end to know all the websites you asked for name resolution of.\nSo what can a person do to make it harder for the other end to create a useful picture of your surfing habits? Well, data is only as useful as it is clean. If you have a list of 1000 DNS requests from the user, then you can be fairly certain where that user was surfing (more technically, where the source IP was surfing, but let\u0026rsquo;s not get pedantic). However, if you had a list of 1000000 DNS requests, of which 1000 were the original legitimate DNS requests, and the remaining 999000 were randomly generated by a process, then how easily could you determine where the user was actually surfing? Essentially, you\u0026rsquo;ve reduced the Signal-to-Noise so low that the signal is \u0026rsquo;lost\u0026rsquo;.\nWith this goal in mind, I set out to introduce some noise into my DNS requests as my weekend project this weekend. It turned out to be easier than I expected thanks to some existing work by others (always stand on other\u0026rsquo;s shoulders when you can). The first thing I did was clone down a copy of noisy. After playing with it a bit, I felt like it was a decent base, but I didn\u0026rsquo;t care for the default \u0026ldquo;root_urls\u0026rdquo; (The Pirate Bay? Really? I don\u0026rsquo;t need my ISP sending me cease-and-desist letters, thank you), I didn\u0026rsquo;t think there were enough \u0026ldquo;root_urls\u0026rdquo;, and I didn\u0026rsquo;t like that the \u0026ldquo;root_urls\u0026rdquo; were never updated. Thankfully, Cisco has a project called Umbrella that seems like a good fit for all three concerns.\nAt this point, it was just a matter of gluing all these pieces together. A sudo later, and it was time to start working. The first thing I did was create /etc/noisy to hold my modified config and to provide a working directory for the daemon. Daemon, you say? Yup, for the second step, we create a systemd service file to run our noise process. The /etc/systemd/system/noisy.service file ends up looking like:\n[Unit] Description=Simple random DNS, HTTP/S internet traffic noise generator [Service] WorkingDirectory=/etc/noisy ExecStart=/usr/bin/python /srv/repos/noisy/noisy.py --config /etc/noisy/config.json SyslogIdentifier=noisy [Install] WantedBy=multi-user.target With this incredibly simple setup in place, systemd will launch the Python script in our copy of the repo (/srv/repos/noisy on my system), use the config file I write in /etc/noisy, and will log to journald while identifying itself as noisy in the journal.\nNow we just need to write our config file to use the hosts from Umbrella. Thankfully, noisy uses JSON for its config file, so rewriting it is trivial. I created a Bash script to handle this for me:\n#!/bin/bash mv /etc/noisy/top-1m.csv.zip /etc/noisy/top-1m.csv.zip.old wget -q -O /etc/noisy/top-1m.csv.zip http://s3-us-west-1.amazonaws.com/umbrella-static/top-1m.csv.zip unzip -q -d /etc/noisy -o /etc/noisy/top-1m.csv.zip cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/noisy/config.json { \u0026#34;max_depth\u0026#34;: 25, \u0026#34;min_sleep\u0026#34;: 3, \u0026#34;max_sleep\u0026#34;: 6, \u0026#34;timeout\u0026#34;: false, \u0026#34;root_urls\u0026#34;: [ EOF cat /etc/noisy/top-1m.csv | cut -d, -f2 | while read url; do echo -e \u0026#34;\\t\\t\\\u0026#34;http://${url}\\\u0026#34;,\u0026#34; \u0026gt;\u0026gt; /etc/noisy/config.json done echo -e \u0026#34;\\t\\t\\\u0026#34;https://medium.com\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; /etc/noisy/config.json cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/noisy/config.json ], \u0026#34;blacklisted_urls\u0026#34;: [ \u0026#34;https://t.co\u0026#34;, \u0026#34;t.umblr.com\u0026#34;, \u0026#34;messenger.com\u0026#34;, \u0026#34;itunes.apple.com\u0026#34;, \u0026#34;l.facebook.com\u0026#34;, \u0026#34;bit.ly\u0026#34;, \u0026#34;mediawiki\u0026#34;, \u0026#34;.css\u0026#34;, \u0026#34;.ico\u0026#34;, \u0026#34;.xml\u0026#34;, \u0026#34;intent/tweet\u0026#34;, \u0026#34;twitter.com/share\u0026#34;, \u0026#34;dialog/feed?\u0026#34;, \u0026#34;.json\u0026#34;, \u0026#34;zendesk\u0026#34;, \u0026#34;clickserve\u0026#34;, \u0026#34;.png\u0026#34;, \u0026#34;.iso\u0026#34; ], \u0026#34;user_agents\u0026#34;: [ EOF cat /srv/repos/noisy/config.json|jq .user_agents | tail -n +2 \u0026gt;\u0026gt; /etc/noisy/config.json echo \u0026#39;}\u0026#39; \u0026gt;\u0026gt; /etc/noisy/config.json sed -i \u0026#39;s/^M//\u0026#39; /etc/noisy/config.json systemctl restart noisy This script moves the previously downloaded Umbrella file out of the way, downloads the latest version from S3, and unzips it. This gives us a CSV file that we\u0026rsquo;ll use in a moment. The script then starts writing the config file by setting some defaults and starting the \u0026lsquo;root_urls\u0026rsquo; key. We then look over every line in the CSV file, and pull the URL from the 2nd field. We prepend \u0026lsquo;http://\u0026rsquo; to the URL and write the result out to our config file. To close out the \u0026lsquo;root_urls\u0026rsquo; section, we write medium.com to it (just to always have a known quantity as the \u0026rsquo;end marker\u0026rsquo; if I need to debug anything). Up next, we copy over the \u0026lsquo;blacklisted_urls\u0026rsquo; keys from the default config, and finally we use jq to pull all the \u0026lsquo;user_agents\u0026rsquo; into our config. Because Cisco uses DOS line-endings, we run a sed on the resulting file to remove these and we\u0026rsquo;re good to go. I\u0026rsquo;m sure there\u0026rsquo;s a \u0026ldquo;better\u0026rdquo; way to do all this, but it wasn\u0026rsquo;t worth optimizing for now, imho.\nNow that we\u0026rsquo;ve got our file in shape, we tell systemd to restart the daemon and we\u0026rsquo;re off to the races, as journalctl shows:\nAug 14 17:09:02 nuc noisy[240416]: INFO:root:Visiting https://www.potpourrigift.com Aug 14 17:09:07 nuc noisy[240416]: INFO:root:Visiting https://www.potpourrigift.com/ShopCategory.aspx?ID=320,362\u0026amp;ITEMS=RE9032%7CR0D067%7CR82102%7CRD9008\u0026amp;HPLoc=MB18 Aug 14 17:09:13 nuc noisy[240416]: INFO:root:Visiting https://www.potpourrigift.com/CustomerService.aspx?page=Free+Catalog And now I\u0026rsquo;m sure my ISP hates me ;)\n","permalink":"https://hunleyd.github.io/posts/generating-dns-noise/","tags":["computers","howto"],"title":"Generating DNS noise"},{"categories":["computers","linux","howto"],"contents":"One of the reasons that I find myself going back to Gentoo is that you compile the entire system for your hardware which, in theory, leads to the best performance possible. So the first task that I undertook when switching the NUC over to it was to figure out what compile options ClearLinux uses. Once I had figured those settings out, I then decided to use LTO optimization for all packages that support it. However, I didn\u0026rsquo;t want to use the LTO overlay.\nAfter doing a lot of digging around and some experimentation, I finally settled on the following configuration:\n# /etc/portage/make.conf CFLAGS=\u0026#34;-march=skylake -mtune=skylake -O3 -pipe -w -falign-functions=32\u0026#34; CFLAGS=\u0026#34;${CFLAGS} -fgraphite-identity -floop-nest-optimize -floop-parallelize-all\u0026#34; CFLAGS=\u0026#34;${CFLAGS} -flto=auto -flto-partition=one -fuse-linker-plugin\u0026#34; CHOST=\u0026#34;x86_64-pc-linux-gnu\u0026#34; CPU_FLAGS_X86=\u0026#34;aes avx avx2 f16c fma3 mmx mmxext pclmul popcnt rdrand sse sse2 sse3 sse4_1 sse4_2 ssse3\u0026#34; CXXFLAGS=\u0026#34;${CFLAGS}\u0026#34; LDFLAGS=\u0026#34;-Wl,-O3 -Wl,--sort-common -Wl,--as-needed\u0026#34; As you can see, I start with a base CFLAGS, then I add Graphite optimizations, and finally the LTO optimizations are added. Occasionally, LTO will cause undefined symbols and the build will fails, so for that I have:\n# /etc/portage/env/cflags-ffat-lto-objects CFLAGS = \u0026#34;${CFLAGS} -ffat-lto-objects\u0026#34; And occasionally a package just won\u0026rsquo;t compile with LTO, so that I use:\n# /etc/portage/env/cflags-fno-lto CFLAGS=\u0026#34;-march=skylake -mtune=skylake -O3 -pipe -w -falign-functions=32\u0026#34; CFLAGS=\u0026#34;${CFLAGS} -fgraphite-identity -floop-nest-optimize -floop-parallelize-all\u0026#34; CXXFLAGS = \u0026#34;${CFLAGS}\u0026#34; Whenever I run into a package that has compile issues, I simply create a file for it in /etc/portage/package.env that looks like:\npackage_atom cflags-ffat-lto-objects or\npackage_atom cflags-fno-lto This little setup has, so far, worked like a charm for me. Out of the 63 packages in my world file, only 12 need an env file to compile properly on my ~amd64 install. And everything feels snappy during my day to day. I\u0026rsquo;m quite pleased.\n","permalink":"https://hunleyd.github.io/posts/ricing-it-up/","tags":["computers","linux","howto"],"title":"Ricing it up"},{"categories":["smarthome"],"contents":"An update on my use Home Assistant, I\u0026rsquo;m no longer running it inside Podman as detailed here. In fact, I\u0026rsquo;m no longer running it on the NUC at all. Instead, I bought myself a Home Assistant Blue. I bought the \u0026lsquo;dev mode\u0026rsquo; version just in case I want to tinker with things further, but I could have gone with the \u0026lsquo;Zen mode\u0026rsquo; version just as easily.\nThe ODROID-N2+ is remarkably capable and having the entire stack being HA-managed (Home Assistant OS, running Home Assistant Supervisor, running Home Assistant Core) is a dream. It really Just Works‚Ñ¢ and I\u0026rsquo;ve not had any issues over the few months it\u0026rsquo;s been running. I used the add-on store to have it launch a MariaDB container and moved the \u0026lsquo;recorder\u0026rsquo; component it with a simple config tweak (and sped up startup time almost 4x in the process). I\u0026rsquo;ve since added the community store (HACS) and a bunch of automations and it still just keeps chugging along. At the moment, I\u0026rsquo;ve got 22 integrations, 49 devices, 10 areas, and ~372 entities and my system dashboard looks like:\nIf you\u0026rsquo;re considering taking the plunge into home automation, I can very easily recommend getting a Blue.\n","permalink":"https://hunleyd.github.io/posts/i-m-blue-da-ba-dee-da-ba-di/","tags":["smarthome"],"title":"I'm blue da ba dee da ba di"},{"categories":["computers","linux"],"contents":"I posted a year ago about dropping ClearLinux and switching to Ubuntu Server on my NUC. While that little experiment was fun, it didn\u0026rsquo;t last long. I\u0026rsquo;m not entirely sure what it is about my brain/personality, but I just do not like any of the binary Linux distros out there for my own long-term use so I went back to my old friend, Gentoo.\nIt\u0026rsquo;s been a few months now, and I must say that running Gentoo on the NUC has been a pleasant experience. It took a little work to get up to speed on recent changes in Gentoo, and I had to dig into things to get an optimized install going on the NUC, but I\u0026rsquo;m pretty happy with the results.\nMaybe I\u0026rsquo;ll actually blog more this time about what I\u0026rsquo;m doing with this install. Or maybe I\u0026rsquo;ll lapse back into old habits and forget to. Only time will tell.\n","permalink":"https://hunleyd.github.io/posts/hello-again-old-friend/","tags":["computers","linux"],"title":"Hello again old friend"},{"categories":["computers","linux"],"contents":"As mentioned (very) briefly in other posts, I run Home Assistant as the \u0026lsquo;control hub\u0026rsquo; for all my \u0026ldquo;smart\u0026rdquo; devices in my home. I originally ran it via their \u0026lsquo;HASSOS\u0026rsquo; Docker image but was never really happy with it. So when I most recently rebuilt my NUC, I decided to give this Podman thing a look. And so far, I seem to like it better. I\u0026rsquo;m still not entirely sold on this whole container bullshit, but whatever ;)\nWhen I first decided to use Podman, the install directions for Ubuntu on the Podman site were not updated to point to the proper Apt source, and I had to dig around on various GitHub issues for the project before finding the correct info:\n. /etc/os-release echo \u0026#34;deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /\u0026#34; | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/Release.key | sudo apt-key add - sudo apt-get update -qq sudo apt-get -qq -y install podman These are now on the site and they work for today but if you\u0026rsquo;re visiting this\u0026quot;post\u0026quot;in the future it might have changed.\nSo anyway, after installing and playing with Podman for a while, I ended up deciding to run three containers (pods?) under rootfull Podman (technically, two of them could run rootless, but I\u0026rsquo;d rather be consistent). My next step was to use podman-generate-systemd to write some systemd service scripts for me. I can\u0026rsquo;t recall exactly why now, but I ended up tweaking the service files by hand and they now look like this:\nHome Assistant:\n[Unit] description=homeassistant system monitor Podman container [Service] Type=simple TimeoutStartSec=30s ExecStartPre=-/usr/bin/podman rm homeassistant ExecStart=/usr/bin/podman run --name=homeassistant -v /root/podman/hassio:/config --net=host -v /proc :/host/proc:ro -v /sys:/host/sys:ro --cap-add SYS_PTRACE --security-opt apparmor=unconfined -v /:/mnt -v /home:/srv homeassistant/home-assistant ExecReload=-/usr/bin/podman stop homeassistant ExecReload=-/usr/bin/podman rm homeassistant ExecStop=-/usr/bin/podman stop homeassistant Restart=never RestartSec=30 [Install] WantedBy=multi-user.target Netdata:\n[Unit] description=netdata system monitor Podman container [Service] Type=simple TimeoutStartSec=30s ExecStartPre=-/usr/bin/podman rm netdata ExecStart=/usr/bin/podman run --name netdata -p 19999:19999 -v /proc:/host/proc:ro -v /sys:/host/sys:ro -v /root/podman/netdata:/etc/netdata --cap-add SYS_PTRACE --security-opt apparmor=unconfined netdata/netdata ExecReload=-/usr/bin/podman stop netdata ExecReload=-/usr/bin/podman rm netdata ExecStop=-/usr/bin/podman stop netdata Restart=never RestartSec=30 [Install] WantedBy=multi-user.target BOINC:\n[Unit] description=boinc system monitor Podman container [Service] Type=simple TimeoutStartSec=30s ExecStartPre=-/usr/bin/podman rm boinc ExecStart=/usr/bin/podman run --name boinc --net=host -v /root/podman/boinc:/var/lib/boinc --security-opt apparmor=unconfined -e BOINC_GUI_RPC_PASSWORD=\u0026#34;123\u0026#34; -e BOINC_CMD_LINE_OPTIONS=\u0026#34;--allow_remote_gui_rpc\u0026#34; boinc/client ExecReload=-/usr/bin/podman stop boinc ExecReload=-/usr/bin/podman rm boinc ExecStop=-/usr/bin/podman stop boinc Restart=never RestartSec=30 [Install] WantedBy=multi-user.target It\u0026rsquo;s been running like this now for a while and I\u0026rsquo;m pretty happy with things. I was eagerly watching the planned Podman feature that would self-update the pods, but I think I\u0026rsquo;m gonna leave that process manual for now since Home Assistant has been pushing a lot of \u0026lsquo;breaking changes\u0026rsquo; lately and I need to review my configs before updating anyway.\nIn any case, I kinda like Podman for when I need to use a container. So if you\u0026rsquo;re running Docker at home and don\u0026rsquo;t have a specific need for Docker itself, see if Podman will meet your needs.\n","permalink":"https://hunleyd.github.io/posts/it-s-podman-man/","tags":["computers","linux"],"title":"It's Podman, man"},{"categories":["computers","linux"],"contents":"About a year ago (Sep 2019 to be precise) I decided to end my Raspberry Pi experiment and begin a new experiment with an Intel NUC. It\u0026rsquo;s not that the Pi is incapable or anything. I really like the platform and will probably find something else to use it for. But my computing needs/desires had changed and I was looking at having a small fleet of them (I already had 2 and was contemplating more) and I really didn\u0026rsquo;t want to go that route. So I looked around, did some research, saved up some money and ended up with:\nAn Intel NUC 8 Performance-G Kit. This is a Core i7-based kit (NUC8i7HVK) that just needs RAM and storage added. It\u0026rsquo;s a Radeon RX Vega M GH 32GB video card, an 802.11{a,b,g,n} device, 2x Gb Ethernet, 2x USB 2.0 ports, 5x USB 3.0 ports. 2x HDMI ports, 2x mini-Display ports, 2x Thunderbolt 3 ports, and a Bluetooth 4.2 radio. All inside a 6.4 lb 10.8\u0026quot;x7.8\u0026quot;x4.8\u0026quot; case. I fricking love it.\nI put a pair of HyperX Kingston Technology Impact 16GB 2400MHz DDR4 sticks in it after talking myself down from a pair of 32GB sticks.\nAnd finally, I put in a pair of Samsung 970 PRO SSD 1TB M.2 NVMe SSDs (OK, that\u0026rsquo;s a lie. I went with just one initially, but bought a second one like a month later).\nGiven the nature of the beast I\u0026rsquo;d assembled, I started with ClearLinux as my OS. It\u0026rsquo;s stupid fast. And it\u0026rsquo;s neat and interesting in that \u0026rsquo;not like other girls\u0026rsquo; kinda way. Ultimately though, I ended up ditching it due to the state of packaging. I\u0026rsquo;ve read that they have some new packaging initiatives though so you shouldn\u0026rsquo;t rule it out based on my experience last year. I then moved to Ubuntu LTS for a short while, realized my mistake, and am currently on Ubuntu Server. So far, it\u0026rsquo;s doing what I need it to do but, protip:\nMake sure you google \u0026lsquo;how to X on ubuntu server\u0026rsquo;\nIf you just Google \u0026lsquo;how to X ubuntu\u0026rsquo; then a lot of the time what you find won\u0026rsquo;t apply. Networking, for example, using something called netplan which is nothing like regular Ubuntu desktop networking.\nAnyway, that\u0026rsquo;s all for now. I just wanted to \u0026ldquo;post\u0026rdquo; this as background info cause I plan on posting about my travails w/ the Ubuntu Server as I go forward.\n","permalink":"https://hunleyd.github.io/posts/bye-pi-hello-nuc/","tags":["computers","linux"],"title":"Bye Pi, Hello NUC"},{"categories":["postgresql","linux","howto"],"contents":"We\u0026rsquo;ve had a small flurry of customers asking about tuning their OS for the best PostgreSQL performance. While the answer to this question is always \u0026rsquo;that depends on your hardware and workload\u0026rsquo; and involves a lot of iteration between changing a setting and benchmarking, I thought I\u0026rsquo;d take a moment to point out that once you do manage to dial-in the settings, you should be writing a profile and deploying to your systems for tuned to make use of. Please, for the love of $diety, stop editing sysctl.conf and friends!\nIf you\u0026rsquo;re running RedHat (or a RedHat-derived) OS, tuned is probably already installed and may even be already running. If not, it\u0026rsquo;s a simple yum install tuned to rectify. If you\u0026rsquo;re on Debian (or a Debian-derived) OS, simply apt install tuned. Now that you have it installed, you can ask it to recommend a profile for you by issuing tuned-adm recommend and you can see which profile, if any, is in use by issuing tuned-adm active. You can also list all the available profiles with tuned-adm list.\nThat\u0026rsquo;s great and all, but there\u0026rsquo;s no PostgreSQL profile that ships with tuned. So, let\u0026rsquo;s build one! First, decide which of the profiles in tuned-adm list is the most appropriate starting point for you. If you\u0026rsquo;re running on bare metal, this is probably throughput-performance. If your system is virtualized, you probably want virtual-guest. We\u0026rsquo;re going to use this profile as the \u0026lsquo;base\u0026rsquo; of our profile. Profiles are stored in /etc/tuned, so let\u0026rsquo;s start setting ours up:\n$ mkdir /etc/tuned/postgresql $ cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/tuned/postgresql/tuned.conf [main] include= throughput-performance EOF As you can see, we\u0026rsquo;re going to start with the throughput-performance profile as a base, and then make tweaks from there. The first tweak? Disable Transparent Huge Pages (THP):\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/tuned/postgresql/tuned.conf [vm] transparent_hugepages=never EOF If you\u0026rsquo;re not aware, THP on a dedicated PostgreSQL server really don\u0026rsquo;t play nicely. It\u0026rsquo;s best to avoid them completely.\nNow, we want to set the \u0026lsquo;standard database VM config\u0026rsquo; so we:\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/tuned/postgresql/tuned.conf [sysctl] vm.overcommit_memory = 2 vm.swappiness = 1 EOF As pointed out in the comments, vm.overcommit_memory and vm.dirty_ratio (which I set below) interplay. vm.dirty_ratio is the value that represents the percentage of MemTotal that can consume dirty pages before all processes must write dirty buffers back to disk and when this value is reached all I/O is blocked for any new writes until dirty pages have been flushed. By default this set to 50% which means that at most only half of your MemTotal can ever only be dirty at once. Please see the kernel documentation for details on all these settings.\nI, personally, also then add (after benchmarking, of course):\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/tuned/postgresql/tuned.conf kernel.sched_autogroup_enabled = 0 kernel.sched_migration_cost_ns = 50000000 vm.dirty_background_bytes = 134217728 vm.dirty_expire_centisecs = 499 vm.dirty_ratio = 90 vm.overcommit_ratio = 90 vm.zone_reclaim_mode = 0 EOF This gives you a complete PostgreSQL tuned profile that looks like:\n[main] include= throughput-performance [sysctl] kernel.sched_autogroup_enabled = 0 kernel.sched_migration_cost_ns = 50000000 vm.dirty_background_bytes = 134217728 vm.dirty_expire_centisecs = 499 vm.dirty_ratio = 90 vm.overcommit_memory = 2 vm.overcommit_ratio = 90 vm.swappiness = 1 vm.zone_reclaim_mode = 0 [vm] transparent_hugepages=never (well, in a slightly different order, but you get the drift).\nNow that we have our profile, run a quick tuned-adm list to ensure that it is listed as available. Assuming it is, you can enable it with tuned-adm profile postgresql and then double-check that it took effect with tuned-adm active.\nOnce activated, you should be all set. Happy computing, or something. ;)\n","permalink":"https://hunleyd.github.io/posts/tuned-pg-and-you/","tags":["postgresql","linux","howto"],"title":"tuned, PG, and you"},{"categories":["general","ott"],"contents":"As a belated update to this post, Mom has told DirecTV to completely go screw and she‚Äôs moved over to Philo where she‚Äôs currently loving life.\nI had initially proposed DirecTV Now to her since it was an uphill battle to convince her to abandon DirecTV satellite. I figured it was the path of least resistance since she‚Äôd know the UI and the channel line-up . And, at first, it was exactly that. However, after about three months, I had grown tired of hearing the following:\nwhen will I get my locals, not yours when will I be able to record stuff why are the previous episodes so inconsistent why can‚Äôt I record show X does your picture look all blocky why is the guide so slow they raised the price?! That‚Äôs right, in the quick span of just three months, she‚Äôd seen that while DirecTV Now was better than her old DirecTV, it was still plagued by, well, AT\u0026amp;T incompetence. It took them forever to launch the DVR, and it still didn‚Äôt work right even after the beta. I actually had to change the account‚Äôs billing address to get the correct locals cause they can‚Äôt seem to figure out GeoIP. Their licensing of channels/shows makes no goddamn sense. As an example, record whatever you want off the Discovery channel, just not Dirty Jobs. I‚Äôm willing to grant that the licensing is on the fault of the network/studio but you‚Äôre AT\u0026amp;T/DirecTV for fuck‚Äôs sake! Spend some damn money and make it happen for your users. And then there was the app itself. I‚Äôve never seen another Roku app be so unresponsive, so slow, and so bandwidth sensitive. When mom can be on her Chromebook doing whatever, while I‚Äôm watching a movie on her TV via Google Play, and my niece is watching Youtube on her iPhone and there are no issues for any of us the problem is not Mom‚Äôs connection. Yet Mom could be the only one in the house and she‚Äôd get desync on the audio, or pixelation/blocking in the video. WTF, AT\u0026amp;T? Buy a real CDN!\nAnd then, of course, they raised the price on her. That‚Äôs right. With all the issues above (and trust me, I checked /r/directvnow religiously. This was not unique to Mom) they had the gall to raise prices. And not just for new sign-ups. Oh no no no. Existing and even grandfathered accounts got hit in the wallet! It‚Äôs like they don‚Äôt know how to be anything but a greedy cable company.\nSo, I signed up for Philo and installed it on Mom‚Äôs TV. I asked her to use it for the free week and see how it went. I kid you not, she called me on day 5 and said she liked it so much better that she agreed to me canceling DirecTV Now right then and there. To say I was floored is an understatement.\nI will grant you that losing the locals and having to use the OTA antenna isn‚Äôt ideal. We‚Äôre keeping an eye on Locast to see if we can solve that ‚Äúsoon‚Äù. But Mom was already using the antenna to watch a few of the local sub-channels, so it wasn‚Äôt a deal breaker. So far, Mom has adapted to the Philo interface easily (I prefer it), we‚Äôve setup a profile for her, me, and guests, and she‚Äôs been recording shows and Hallmark Christmas movies (ugh) all without needing to call me.\nAnd since switching, NOT ONCE has she had a desync or pixelation issue. Even when Steph and I both were there for Thanksgiving and beating the crap outta her wifi.\nThe kicker here? It‚Äôs only $16/mo. Yeah, you read that right. She went from $100/mo w/ DTV to $45/mo with DTVN (at the end) to $16/mo. Unreal. And on top of that, Philo recently added an extra MTV channel for the hell of it.\nAll around a WIN WIN.\n","permalink":"https://hunleyd.github.io/posts/the-continuing-adventures-of-mom-cutting-the-cord/","tags":["general","ott"],"title":"The continuing adventures of Mom cutting the cord"},{"categories":["postgresql","osx","howto"],"contents":"UPDATE: My reasoning was incorrect below. It wasn‚Äôt the moving of some of the lock code to C that caused the issue. It was moving -D_POSIX_C_SOURCE up to the Makefile that caused the problem. The solution below is still the same though.\nThe team has released pgBackRest 2.08 today. As part of a continuing effort, more bits have been moved from Perl to C. Sadly, this adds a new wrinkle for those of us on OSX, as when compiling, you now get:\ngcc -I. -I../libc -std=c99 -D_POSIX_C_SOURCE=200112L -O2 -Wfatal-errors -Wall -Wextra -Wwrite-strings -Wswitch-enum -Wconversion -Wformat=2 -Wformat-nonliteral -Wno-clobbered -Wno-missing-field-initializers -Wstrict-prototypes -Wpointer-arith -Wvla `xml2-config --cflags` `perl -MExtUtils::Embed -e ccopts` -DWITH_PERL -DNDEBUG -c common/lock.c -o common/lock.o warning: unknown warning option \u0026#39;-Wno-clobbered\u0026#39;; did you mean \u0026#39;-Wno-consumed\u0026#39;? [-Wunknown-warning-option] common/lock.c:76:21: warning: implicit declaration of function \u0026#39;flock\u0026#39; is invalid in C99 [-Wimplicit-function-declaration] if (flock(result, LOCK_EX | LOCK_NB) == -1) ^ common/lock.c:76:21: warning: this function declaration is not a prototype [-Wstrict-prototypes] common/lock.c:76:35: fatal error: use of undeclared identifier \u0026#39;LOCK_EX\u0026#39; if (flock(result, LOCK_EX | LOCK_NB) == -1) ^ 3 warnings and 1 error generated. make: *** [common/lock.o] Error 1 To fix this, you will need to edit src/Makefile and change line 12 from:\nCSTD = -std=c99 -D_POSIX_C_SOURCE=200112L to:\nCSTD = -std=c99 -D_DARWIN_C_SOURCE Then, you can follow the other steps on my previous post and everything should compile and function properly.\nEnjoy!\n","permalink":"https://hunleyd.github.io/posts/pgbackrest-2.08-and-macos-mojave/","tags":["postgresql","osx","howto"],"title":"pgBackRest 2.08 and macOS Mojave"},{"categories":["postgresql","osx","howto"],"contents":"pgBackRest 2.07 was announced today. As usual, I immediately downloaded it and tried to get it up and running on my MacBook (currently running Mojave). It wasn‚Äôt as straightforward as one might hope, and the online instructions assume a Linux system, so I figured I‚Äôd write this up for anyone else attempting the same.\nSince this is OSX, we have to do some work to make things right before we even start with the pgBackRest code. First up, get a real OpenSSL install. We‚Äôll use Homebrew for this:\n$ brew install openssl [output snipped] $ openssl version -a LibreSSL 2.6.4 built on: date not available platform: information not available options: bn(64,64) rc4(ptr,int) des(idx,cisc,16,int) blowfish(idx) compiler: information not available OPENSSLDIR: \u0026#34;/private/etc/ssl\u0026#34; $ /usr/local/opt/openssl/bin/openssl version -a OpenSSL 1.0.2p 14 Aug 2018 built on: reproducible build, date unspecified platform: darwin64-x86_64-cc options: bn(64,64) rc4(ptr,int) des(idx,cisc,16,int) idea(int) blowfish(idx) compiler: clang -I. -I.. -I../include -fPIC -fno-common -DOPENSSL_PIC -DOPENSSL_THREADS -D_REENTRANT -DDSO_DLFCN -DHAVE_DLFCN_H -arch x86_64 -O3 -DL_ENDIAN -Wall -DOPENSSL_IA32_SSE2 -DOPENSSL_BN_ASM_MONT -DOPENSSL_BN_ASM_MONT5 -DOPENSSL_BN_ASM_GF2m -DSHA1_ASM -DSHA256_ASM -DSHA512_ASM -DMD5_ASM -DAES_ASM -DVPAES_ASM -DBSAES_ASM -DWHIRLPOOL_ASM -DGHASH_ASM -DECP_NISTZ256_ASM OPENSSLDIR: \u0026#34;/usr/local/etc/openssl\u0026#34; As you can see, the default SSL from OSX is in /usr/bin while the newly installed OpenSSL is in /usr/local/opt/openssl. In my testing, this is enough to proceed with pgBackRest but I prefer to have the openssl binary match the libs and I‚Äôm a glutton for punishment, so I replace the OSX binary with the Homebrew one:\n$ sudo mv /usr/bin/openssl /usr/bin/openssl.old $ sudo ln -s /usr/local/opt/openssl/bin/openssl /usr/bin $ ls -ld /usr/bin/openssl* lrwxr-xr-x 1 root wheel 34 Nov 16 11:39 /usr/bin/openssl -\u0026amp;gt; /usr/local/opt/openssl/bin/openssl* -rwxr-xr-x 1 root wheel 1.2M Sep 21 00:16 /usr/bin/openssl.old* OK, so now we have an SSL that pgBackRest knows how to speak to. We need to install some Perl modules that it needs. If you haven‚Äôt run CPAN before, just accept the defaults when it asks you things:\n$ perl -MCPAN -e shell cpan[1]\u0026gt; install DBI [output snipped] cpan[2]\u0026gt; install DBD::Pg [output snipped] cpan[3]\u0026gt; install IO::Socket::SSL [output snipped] cpan[4]\u0026gt; install XML::LibXML [output snipped] cpan[5]\u0026gt; q Now we‚Äôve got our Perl modules installed, we have to tell our shell where to find them:\n$ export PERL5LIB=/Users/doug/perl5/lib/perl5 $ export PERL_LOCAL_LIB_ROOT=/Users/doug/perl5 $ export PERL_MB_OPT=--install_base \u0026#34;/Users/doug/perl5\u0026#34; $ export PERL_MM_OPT=INSTALL_BASE=/Users/doug/perl5 Obviously, you will change ‚Äòdoug‚Äô to your OSX username. You will also need to add these to your shell‚Äôs startup file (.profile, .bash_profile, etc) to make them permanent.\nNow, we can actually get down to the business of compiling and installing pgBackRest. Download the 2.07 tarball from the releases tab on GitHub, and let‚Äôs get busy:\n$ tar xvf Downloads/pgbackrest-release-2.07.tar.gz [output snipped] $ cd pgbackrest-release-2.07 Now, we need to make some edits to the included Makefile. So vi src/Makefile and jump to line 42. Edit this line to be:\nLDEXTRA = -L /usr/local/opt/openssl/lib This tells the build process where to find the OpenSSL that we installed. Then, jump to line 149 and change it to:\ninstall -m 755 pgbackrest $(DESTDIR)/usr/local/bin This makes it install into /usr/local/bin which is where Homebrew puts everything else and you shouldn‚Äôt need to use sudo to write to it.\nNow, compile and install it:\n$ make -s -C ./src \u0026amp;amp;\u0026amp;amp; make -s -C ./src/install Assuming that goes well, you can now run pgBackRest to verify it‚Äôs installed and functional:\n$ /usr/local/bin/pgbackrest pgBackRest 2.07 - General help Usage: pgbackrest [options] [command] Commands: archive-get Get a WAL segment from the archive. archive-push Push a WAL segment to the archive. backup Backup a database cluster. check Check the configuration. expire Expire backups that exceed retention. help Get help. info Retrieve information about backups. restore Restore a database cluster. stanza-create Create the required stanza data. stanza-delete Delete a stanza. stanza-upgrade Upgrade a stanza. start Allow pgBackRest processes to run. stop Stop pgBackRest processes from running. version Get version. Use \u0026#39;pgbackrest help [command]\u0026#39; for more information. You should now be able to follow the online docs and setup pgBackRest.\nEnjoy!\n","permalink":"https://hunleyd.github.io/posts/pgbackrest-2.07-and-macos-mojave/","tags":["postgresql","osx","howto"],"title":"pgBackRest 2.07 and macOS Mojave"},{"categories":["postgresql","howto"],"contents":"After a lively discussion at work today about monitoring tools and use cases, I decided to see if I could use tail_n_mail, which I already use to monitor my PostgreSQL logs, to monitor my pgBackRest logs. It turns out that it can, and can do so fairly trivially.\nFor reference, our .tail_n_mail.conf looks like this:\n## This file is automatically updated LOG_LINE_PREFIX: %t P%p EMAIL: you@gme.com MAILSUBJECT: HOST pgBackRest errors NUMBER INCLUDE: WARN: INCLUDE: ERROR: INCLUDE: FATAL: INCLUDE: PANIC: FILE1: /var/log/pgbackrest/pgbackrest.log You would need to change the EMAIL and FILE1 and the rest should just work.\nTo actually invoke the monitoring, simply add a cron entry that looks like:\n* * * * * /path/to/tail_n_mail ~/.tailnmail.conf --pgmode=0 (Obviously, you should adjust the periodicity. I highly doubt you need to check every minute.)\nOnce you‚Äôve saved the crontab, you should be all good. If you doubt the setup, you can add --mailzero to the invocation to get an email even if everything is OK.\nSee? Easy.\n","permalink":"https://hunleyd.github.io/posts/monitoring-pgbackrest-with-tail_n_mail/","tags":["postgresql","howto"],"title":"Monitoring pgBackRest with tail_n_mail"},{"categories":["postgresql"],"contents":"I\u0026rsquo;ve noticed several individuals inquiring lately about pgBouncer and how they can avoid putting all users and their passwords in it\u0026rsquo;s auth_file. After the most recent such inquiry (hi Richard!) I decided I\u0026rsquo;d write this\u0026quot;post\u0026quot;to hopefully make it clearer how to use \u0026lsquo;pass-through auth\u0026rsquo; and avoid maintaining your users and their passwords in an external file. So let\u0026rsquo;s see what this takes, shall we?\nFirst, install pgBouncer as per your OS (yum, apt, brew, etc):\n$ brew install pgbouncer Updating Homebrew... ==\u0026gt; Auto-updated Homebrew! Updated 1 tap (homebrew/core). ==\u0026gt; Updated Formulae \u0026lt;snip\u0026gt; ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/pgbouncer-1.8.1.high_sierra ######################################################################## 100.0% ==\u0026gt; Pouring pgbouncer-1.8.1.high_sierra.bottle.tar.gz ==\u0026gt; Caveats The config file: /usr/local/etc/pgbouncer.ini is in the \u0026#34;ini\u0026#34; format and you will need to edit it for your particular setup. See: https://pgbouncer.github.io/config.html The auth_file option should point to the /usr/local/etc/userlist.txt file which can be populated by the /usr/local/opt/pgbouncer/bin/mkauth.py script. To have launchd start pgbouncer now and restart at login: brew services start pgbouncer Or, if you do not want/need a background service you can just run: pgbouncer -q /usr/local/etc/pgbouncer.ini ==\u0026gt; Summary üç∫ /usr/local/Cellar/pgbouncer/1.8.1: 17 files, 399.9KB Great, so now we have pgBouncer installed.\nTo make life easier on ourselves, we\u0026rsquo;re going to temporarily enable trusted local socket connections in our pg_hba.conf:\n# TYPE DATABASE USER ADDRESS METHOD # \u0026#34;local\u0026#34; is for Unix domain socket connections only local all all trust Right now, this is the only line in my pg_hba.conf. Let\u0026rsquo;s SIGHUP the postmaster so it takes affect:\n$ pg_ctl -D $PGDATA reload server signaled And test it:\n$ unset PGPASSWORD ; psql -U doug -d doug -c \u0026#34;select now();\u0026#34; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ now ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 2018-08-07 13:19:06.343245-04 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (1 row) Time: 1.959 ms OK, we can connect without issue.\nLet\u0026rsquo;s configure pgBouncer now. As per the output above, I need to edit /usr/local/etc/pgbouncer.ini but yours is probably in plain old /etc:\n[databases] ; any db over Unix socket * = [pgbouncer] logfile = /Users/doug/pgbouncer.log pidfile = /Users/doug/pgbouncer.pid ; IP address or * which means all IPs listen_addr = 127.0.0.1 listen_port = 6432 unix_socket_dir = /tmp ; any, trust, plain, crypt, md5, cert, hba, pam auth_type = trust auth_file = /Users/doug/userlist.txt admin_users = doug stats_users = doug pool_mode = transaction server_reset_query = DISCARD ALL max_client_conn = 100 default_pool_size = 20 As you can see, we\u0026rsquo;re gonna pass connections to any db back to the postmaster via a local socket. I put the logs in my $HOME for ease of use. I put the auth_file in my $HOME as well. Then I set myself up as both an admin and stats user. I changed the mode into transaction which is usually the mode you want. Now, I add myself to the auth_file:\n$ echo \u0026#39;\u0026#34;doug\u0026#34; \u0026#34;md5540094bd8172cd963fdfa773fe44b488\u0026#34;\u0026#39; \u0026gt; userlist.txt (NOTE: I did a select on pg_shadow as a SUPERUSER to get these values.)\nAnd start pgBouncer:\n$ pgbouncer /usr/local/etc/pgbouncer.ini 2018-08-07 13:43:46.453 92057 LOG File descriptor limit: 7168 (H:-1), max_client_conn: 90, max fds possible: 100 2018-08-07 13:43:46.455 92057 LOG listening on 127.0.0.1:6432 2018-08-07 13:43:46.455 92057 LOG listening on unix:/tmp/.s.PGSQL.6432 2018-08-07 13:43:46.455 92057 LOG process up: pgbouncer 1.8.1, libevent 2.1.8-stable (kqueue), adns: evdns2, tls: OpenSSL 1.0.2o 27 Mar 2018 Now, we see if we can connect to pgBouncer\u0026rsquo;s internal db:\n$ psql -h 127.0.0.1 -p 6432 -d pgbouncer -X psql (10.4, server 1.8.1/bouncer) Type \u0026#34;help\u0026#34; for help. pgbouncer=# show pools; database | user | cl_active | cl_waiting | sv_active | sv_idle | sv_used | sv_tested | sv_login | maxwait | maxwait_us | pool_mode +++++++++--++++++++++--++++++++++--+++++++++++++++++++++++--++++++++++++++++++++++++++++++--++++++++++-+++++++++++++++++++++++++++++++++-- pgbouncer | pgbouncer | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | statement (1 row) Success!\nNow, can we connect to one of our PostgreSQL dbs through pgBouncer:\n$ psql -h 127.0.0.1 -p 6432 -d doug psql (10.4) Type \u0026#34;help\u0026#34; for help. (doug@127.0.0.1:6432/doug[92825]) # Huzzah!\nWe will now alter pgbouncer.ini and set auth_type = md5 and edit pg_hba.conf to use md5 as well to make sure we\u0026rsquo;re not passing around plaintext passwords. Our retest looks like:\n$ grep ^local $PGDATA/pg_hba.conf local all all md5 $ pg_ctl -D $PGDATA reload server signaled doug@ReturnOfTheMac ~\u0026gt; psql -h 127.0.0.1 -p 6432 -d doug Password: psql (10.4) Type \u0026#34;help\u0026#34; for help. (doug@127.0.0.1:6432/doug[97966]) \\q $ psql -h 127.0.0.1 -p 6432 -d pgbouncer -X Password: psql (10.4, server 1.8.1/bouncer) Type \u0026#34;help\u0026#34; for help. pgbouncer= \\q Which, as you can see, we were now password prompted both times!\nNow, that we know it all works, we can go about changing things to not expose users through the auth_file. First, we\u0026rsquo;ll create a pgbouncer user on our db, then we will create a SECURITY DEFINER function will allow the pgbouncer user to (essentially) \u0026lsquo;sudo\u0026rsquo; as a superuser to look at the pg_shadow table, and finally we will ensure only our pgbouncer user can execute that function:\n(doug@127.0.0.1:6432/doug[92825]) # CREATE ROLE pgbouncer ENCRYPTED PASSWORD \u0026#39;secret\u0026#39;; CREATE ROLE Time: 4.102 ms (doug@127.0.0.1:6432/doug[92825]) # GRANT CONNECT ON DATABASE doug TO pgbouncer; GRANT Time: 13.531 ms (doug@[local]:5432/doug[92825]) # ALTER ROLE pgbouncer LOGIN; ALTER ROLE Time: 6.457 ms (doug@127.0.0.1:6432/doug[92825]) # CREATE OR REPLACE FUNCTION public.user_lookup(in i_username text, out uname text, out phash text) [more] - \u0026gt; RETURNS record AS $$ [more] $ \u0026gt; BEGIN [more] $ \u0026gt; SELECT usename, passwd FROM pg_catalog.pg_shadow [more] $ \u0026gt; WHERE usename = i_username INTO uname, phash; [more] $ \u0026gt; RETURN; [more] $ \u0026gt; END; [more] $ \u0026gt; $$ LANGUAGE plpgsql SECURITY DEFINER; CREATE FUNCTION Time: 21.219 ms (doug@127.0.0.1:6432/doug[92825]) # REVOKE ALL ON FUNCTION public.user_lookup(text) FROM public, pgbouncer; REVOKE Time: 7.330 ms (doug@127.0.0.1:6432/doug[92825]) # GRANT EXECUTE ON FUNCTION public.user_lookup(text) TO pgbouncer; GRANT Time: 7.572 ms (NOTE: Astute readers will note I\u0026rsquo;m connected as \u0026lsquo;doug\u0026rsquo; to the db. This works cause in my setup that is a SUPERUSER. You should probably use the \u0026lsquo;postgres\u0026rsquo; account.)\nAnd, let\u0026rsquo;s tell PgBouncer to use this function:\n$ grep ^auth /usr/local/etc/pgbouncer.ini auth_type = md5 auth_file = /Users/doug/userlist.txt auth_user = pgbouncer auth_query = SELECT * FROM public.user_lookup($1); And let\u0026rsquo;s edit our auth_file to only contain the pgbouncer user\u0026rsquo;s info:\ndoug@ReturnOfTheMac ~\u0026gt; cat userlist.txt \u0026#34;pgbouncer\u0026#34; \u0026#34;md509d12ff67352814e4c467c7f55a3a1d7\u0026#34; Restart pgBouncer, and let\u0026rsquo;s recheck:\ndoug@ReturnOfTheMac ~\u0026gt; psql -h 127.0.0.1 -p 6432 -d doug 2 Password: psql (10.4) Type \u0026#34;help\u0026#34; for help. (doug@127.0.0.1:6432/doug[7076]) # select now(); ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ now ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 2018-08-07 14:43:45.938919-04 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (1 row) Time: 0.438 ms It works! But what about the pgBouncer internal db?\n$ psql -h 127.0.0.1 -p 6432 -d pgbouncer psql: ERROR: No such user: doug Well, that makes sense. The auth_file only has a pgbouncer user. So, let\u0026rsquo;s edit the pgbouncer.ini:\n$ grep \u0026#39;_users\u0026#39; /usr/local/etc/pgbouncer.ini admin_users = pgbouncer stats_users = pgbouncer Retart pgBouncer once more and check:\ndoug@ReturnOfTheMac ~\u0026gt; psql -h 127.0.0.1 -p 6432 -d pgbouncer -U pgbouncer -X Password for user pgbouncer: psql (10.4, server 1.8.1/bouncer) Type \u0026#34;help\u0026#34; for help. pgbouncer=# show clients; type | user | database | state | addr | port | local_addr | local_port | connect_time | request_time | wait | wait_us | ptr | link | remote_pid | tls ++++++++++++++++--++++++++++--+++++++--++++++++++--+++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++-- C | pgbouncer | pgbouncer | active | 127.0.0.1 | 54191 | 127.0.0.1 | 6432 | 2018-08-07 14:48:03 | 2018-08-07 14:48:06 | 0 | 0 | 0x7fa082005010 | | 0 | (1 row) And we\u0026rsquo;re golden!\nYou can connect to pgBouncer internally using the pgbouncer user, and you can connect to our normal PostgreSQL db as any valid user and it uses our function to do the auth!\nTo complete this setup, we\u0026rsquo;re gonna move PostgreSQL to port 5433 and pgBouncer to port 5432:\n$ grep port /usr/local/etc/pgbouncer.ini listen_port = 5432 doug@ReturnOfTheMac ~\u0026gt; grep port $PGDATA/postgresql.conf port = 5433\t# (change requires restart) So now, if someone tries to connect to our PostgreSQL on the default TCP/IP port, it goes through PgBouncer transparently (and then pgBouncer connects locally via a socket). Our users/apps are none the wiser, and us DBAs can always ssh into the box and connect directly to PostgreSQL via socket if needed. And we\u0026rsquo;re not exposing any user/app passwords in a text file on the OS.\nWIN WIN\nOne final note: this is only working for the \u0026lsquo;doug\u0026rsquo; database currently. If I wanted this to also work for another database, say \u0026lsquo;postgres\u0026rsquo; or \u0026lsquo;prod_app\u0026rsquo; then I would need to GRANT CONNECT on those dbs to \u0026lsquo;pgbouncer\u0026rsquo; and would need to create my function in them as well.\n‚òÆÔ∏è\n","permalink":"https://hunleyd.github.io/posts/pgbouncer-and-auth-pass-thru/","tags":["postgresql"],"title":"pgBouncer and auth pass-through"},{"categories":["smarthome","review"],"contents":"I finally saved up and bought myself a smart lock at the end of April. I ended up getting the August Smart Lock Pro with the Connect module. Now that I\u0026rsquo;ve used it for a month and a half, I feel like I can finally review it.\nTL;DR I love this lock.\nHonestly, the hardest part of this whole process was picking the lock to buy. I looked at the August lock, I looked at the Kwikset Bluetooth lock, I even looked at the Nest lock and debating waiting for its release. In the end, I chose the August for two reasons: it integrates with Google Home natively (no \u0026rsquo;talk to August\u0026rsquo; bullshit) and it uses the existing lock\u0026rsquo;s keys so my landlord can\u0026rsquo;t complain.\nInstalling the lock was stupid simple. You are directed to download their app for your phone, create an account, and then it gives you full-screen step-by-step directions. There\u0026rsquo;s nothing unusual about this deadlock, so if you\u0026rsquo;ve ever changed one before, you know how to do this. You\u0026rsquo;ll need a screwdriver and maybe 10 minutes.\nInstallation is basically 4 steps:\nRemove the old back on the deadbolt Find the correct shaft adaptor Install the new August back Connect to the wifi I don\u0026rsquo;t leave the house very often, if I\u0026rsquo;m honest, but the occasions I have since installing this have been very nice. I leave, the door locks (and thanks to other integrations my Blink cameras turn on); I return, the door unlocks (and the cameras turn off). I don\u0026rsquo;t have to carry a key with me. I don\u0026rsquo;t have to empty a hand to fish out my key to unlock the door. When I go to bed at night, I just tell Google to lock the door. The only minor annoyance is that Google won\u0026rsquo;t voice unlock the door (for security reasons).\nI\u0026rsquo;ve yet to use the app to give anyone a \u0026rsquo;temporary key\u0026rsquo; but I have used the app to check on (and change) the lock status. I\u0026rsquo;ve also used the app to update the firmware on the lock.\nIf you\u0026rsquo;re considering a smart lock, I highly recommend you look at the August Pro.\n‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\n","permalink":"https://hunleyd.github.io/posts/locking-it-down/","tags":["smarthome","review"],"title":"Locking it down"},{"categories":["general","ott"],"contents":"OK folks, it finally happened. My mother has cut the cord and now gets all her TV needs satisfied via OTA and OTT. Yes, you heard right. My mother. The same woman who spent the better part of a decade (I cut the cord in late 2008) saying how she didn\u0026rsquo;t understand how I could not have cable. Someone check the temperature in Hell will ya? :)\nAll kidding aside, Mom (and Dad before he passed) has spent the past several years canceling Dish and DirecTV and switching back-n-forth between them to get the best \u0026rsquo;new customer\u0026rsquo; rates and deals. But as Mom starts eyeing retirement and DirecTV keeps upping her bill $2 at a time for various fees, she\u0026rsquo;d had enough.\nSo, over the course of the last few months, I walked Mom through getting a TCL Roku TV:\n(Note, that\u0026rsquo;s actually a picture of the 55\u0026quot; which I bought last Aug. Mom bought the 32\u0026quot; even though I argued for the 37\u0026quot;)\nThen I walked her through using the 1byone 50-mile HDTV antenna I had previously bought her. And finally, I signed up for the DirecTVNow beta on Roku and waited for the DVR functionality to launch.\nWith all the pieces in place, we spent the afternoon setting everything up during my recent visit. We had a few little bumps that weekend, and I\u0026rsquo;m going to be getting her a Tablo DUO in the near future so we can use the 1byone on both TVs, but otherwise she adjusted to it all pretty easily. I even hooked up a Roku stick to the TV in the spare bedroom and showed her its all exactly the same as in the living room (minus the OTA locals, of course). Listening to her argue w/ the DirecTV phone rep during the cancellation call was extremely satisfying and made me proud. Mom still can\u0026rsquo;t seem to believe that for $35/mo she is getting everything she used to pay almost $100/mo for. But she\u0026rsquo;s already been telling her friends about it all :)\nSo .. if you still haven\u0026rsquo;t cut the cord, what are you waiting for?\n","permalink":"https://hunleyd.github.io/posts/mom-cuts-the-cord/","tags":["general","ott"],"title":"Mom cuts the cord"},{"categories":["postgresql","howto"],"contents":"I recently helped a customer upgrade their PostgreSQL instance from 9.4.x on RHEL to 10.x on Ubuntu. While it initially sounded daunting, the use of pglogical and some planning actually made it rather straightforward. While there\u0026rsquo;s nothing new or original in this post, I still felt compelled to write it up both for posterity\u0026rsquo;s sake and for anyone else that might find the info useful as an example in their own endeavors.\npglogical is a logical replication system implemented entirely as a PostgreSQL extension. Fully integrated, it requires no triggers or external programs. This makes it faster than Slony, Londiste, et al. It is also (roughly) the basis upon which logical replication in Pg 10 core is built.\nInstalling pglogical pglogical is available from 2ndQuadrant in both a YUM repository for RedHat-based distros and in an APT repository for Debian-based distros. It will need to be installed on both the source (old Pg version) and destination servers (new Pg version).\nThe instructions for installing their repo and the needed packages can be found here.\nConfiguring pglogical Tweaking the cluster config You will need to adjust the postgresql.conf file to accommodate pglogical. On both the source and destination servers, do the following:\n$ echo \u0026#34;include \u0026#39;pglogical.conf\u0026#39;\u0026#34; \u0026gt;\u0026gt; $PGDATA/postgresql.conf $ echo \u0026#34;wal_level = \u0026#39;logical\u0026#39;\u0026#34; \u0026gt;\u0026gt; $PGDATA/pglogical.conf $ echo \u0026#34;max_worker_processes = 10\u0026#34; \u0026gt;\u0026gt; $PGDATA/pglogical.conf $ echo \u0026#34;max_replication_slots = 10\u0026#34; \u0026gt;\u0026gt; $PGDATA/pglogical.conf $ echo \u0026#34;max_wal_senders = 10\u0026#34; \u0026gt;\u0026gt; $PGDATA/pglogical.conf $ echo \u0026#34;shared_preload_libraries = \u0026#39;pglogical\u0026#39;\u0026#34; \u0026gt;\u0026gt; $PGDATA/pglogical.conf NOTE: If you already have one or more values in shared_preload_libraries, simply append pglogical to the list of values already there.\nEnsure the presence of PKs Logical replication doesn\u0026rsquo;t work without primary keys. Identify all tables that do not have one:\nSELECT n.nspname as schema, c.relname as table FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026#39;r\u0026#39; AND NOT EXISTS ( SELECT 1 FROM pg_constraint con WHERE con.conrelid = c.oid AND con.contype = \u0026#39;p\u0026#39; ) AND n.nspname \u0026lt;\u0026gt; ALL ( ARRAY [ \u0026#39;pg_catalog\u0026#39;, \u0026#39;sys\u0026#39;, \u0026#39;dbo\u0026#39;, \u0026#39;information_schema\u0026#39; ] ); Create the pglogical extension On both the source and destination Pg instances, create the pglogical extension in every database you wish to replicate:\nCREATE EXTENSION pglogical; NOTE: On Pg 9.4 only you will need to CREATE EXTENSION pglogical_origin; FIRST.\nRunning pglogical Ensure global objects are copied The pglogical tool runs at the database level which means that global objects like roles are not copied. Therefore, you need to ensure these objects are created yourself.\nOn the source Pg server:\n$ pg_dumpall -g -f globals.sql Then copy globals.sql to the destination server and run:\n$ psql -f globals.sql Prep the destination schema At this time, pglogical doesn\u0026rsquo;t replicate DDL, so it is necessary to ensure that both the source and destination have matching schema object definitions before attempting to replicate.\nAs such, for each source database that you want to replicate, you need to run a \u0026lsquo;schema only\u0026rsquo; dump:\n$ pg_dump -Fc -s -f dbname_schema.dmp dbname Now copy the dbname_schema.dmp file(s) to the destination server, and run for each database:\n$ pg_restore -d dbname dbname_schema.dmp Create a replication user We\u0026rsquo;ll need a user that has the replication permission for this all to work, so let\u0026rsquo;s create one:\nCREATE ROLE pglogical LOGIN REPLICATION SUPERUSER ENCRYPTED PASSWORD \u0026#39;secret\u0026#39;; Do this on both the source and destination Pg instances.\nTweak the pg_hba.conf on both the source and destination Pg instances to allow the replication user to connect:\nlocal replication pglogical trust host replication pglogical 0.0.0.0/0 md5 local dbname pglogical trust host dbname pglogical 0.0.0.0/0 md5 NOTE: Make sure to edit 0.0.0.0/0 to match your actual CIDR or IP address and dbname to match the db you wish to replicate.\nCreate your publication Now, we\u0026rsquo;re ready to actually setup and start the replication. First, we need to SIGHUP the postmaster so it sees all the config changes we made on both the source and target Pg instances:\n$ pg_ctl -D $(ps -efw|grep -- \u0026#34;[p]ost.*-D\u0026#34;|cut -d\\- -f2|cut -d\u0026#34; \u0026#34; -f2) reload On the source Pg instance, we need to create a publication to \u0026lsquo;push\u0026rsquo; the data to the new instance:\nSELECT pglogical.create_node(node_name := \u0026#39;dbname_provider\u0026#39;, dsn := \u0026#39;host=127.0.0.1 port=5432 dbname=test user=pglogical\u0026#39;); Adjust the port= and dbname= parameters to match your source Pg instance. If replicating more than one database, repeat this command for each database, changing dbname and dbname_provider accordingly.\nAdd your tables to the publication Now that we have a publication channel, we need content to publish. Let\u0026rsquo;s add that now:\n1: Add all your tables:\nSELECT pglogical.replication_set_add_all_tables(\u0026#39;default\u0026#39;, \u0026#39;{public}\u0026#39;::text[]); 2: Add all your sequences:\nSELECT pglogical.replication_set_add_all_sequences(set_name := \u0026#39;default\u0026#39;, schema_names := \u0026#39;{public}\u0026#39;::text[], synchronize_data := true ); Obviously, you should change public in both the above if you are using a different schema for your objects. If you are using multiple schemas, simply repeat the above and change public appropriately.\nNOTE: The nextval of sequences will be synced every 60-70s roughly.\nCreate your subscription Now that we have a publication channel and its content defined, we need to setup a subscriber on the target Pg instance to consume the channel:\nSELECT pglogical.create_node(node_name := \u0026#39;subscriber\u0026#39;, dsn := \u0026#39;host=127.0.0.1 port=5432 dbname=test user=pglogical\u0026#39;); Adjust the dbname= parameter to match your target Pg instance. If replicating more than one database, repeat this command for each database.\nNow, tell the subscriber what to subscribe to:\nSELECT pglogical.create_subscription(subscription_name := \u0026#39;subscription\u0026#39;, provider_dsn := \u0026#39;host=172.28.173.18 port=5432 dbname=test user=pglogical\u0026#39;, replication_sets := \u0026#39;{default}\u0026#39;::text[] ); Adjust host=, port=, and dbname= parameters to match your source Pg instance. If replicating more than one database, repeat this command for each database, changing dbname and subscription_name accordingly.\nConclusion At this point, data should be replicating and (if not already) it will catch up to \u0026lsquo;current\u0026rsquo; quickly. Once caught up, replication will maintain sync between the source and target instances in almost real time. You can easily determine the current state of pglogical by issuing this SQL on the subscriber:\nSELECT subscription_name, status FROM pglogical.show_subscription_status(); If the query returns initializing then it is copying the original source data to the destination. If the query returns replicating then the initial synchronization has completed and replicating is now happening in real time as data changes.\nWhen ready, you can simply stop any applications pointing at the source Pg instance, wait a few minutes to ensure replication drains any outstanding items, force an update of your sequences:\nSELECT pglogical.synchronize_sequence( seqoid ) FROM pglogical.sequence_state; and then re-point your applications at the target instance.\nPost-upgrade, if you wish to clean everything up, simply:\n1: Remove the subscription:\nSELECT pglogical.drop_subscription(\u0026#39;subscription\u0026#39;, true); 2: Remove the subscriber:\nSELECT pglogical.drop_node(\u0026#39;subscriber\u0026#39;, true); 3: Remove the extension:\nDROP EXTENSION pglogical CASCADE; 4: Remove the user:\nDROP ROLE pglogical; 5: Remove any pglogical lines in pg_hba.conf\n6: Remove $PGDATA/pglogical.conf\n7: Reload PostgreSQL\n8: Remove the OS packages using yum or apt\n","permalink":"https://hunleyd.github.io/posts/upgrading-postgresql-from-9.4-to-10.3-with-pglogical/","tags":["postgresql","howto"],"title":"Upgrading PostgreSQL from 9.4 to 10.3 with pglogical"},{"categories":["general"],"contents":"Last year, at the beginning of March, I turned in my 2014 Kia Optima LX at the end of its lease and decided to go \u0026lsquo;car free\u0026rsquo;. I wanted to see exactly how much of a PITA it would be, how many services I\u0026rsquo;d need to subscribe to to get things done, and how much money it would actually save me. It\u0026rsquo;s now been a year, and I can definitively answer these questions.\nSo, Let\u0026rsquo;s break things down shall we?\nI was paying $440.88/mo for the car, or $5290.56/yr. A not-insignificant chunk of change. That\u0026rsquo;s not including insurance, gas, oil changes, etc. This was purely for the lease itself.\nOnce I was \u0026lsquo;sans auto\u0026rsquo;, I started taking Uber whenever I needed to go somewhere (my monthly PUG meeting, shopping, out to eat, etc). Looking at my past year\u0026rsquo;s bank statements, I\u0026rsquo;ve given $1674.83 to Uber. A decent chunk of change, to be sure, but less than I expected if I\u0026rsquo;m honest.\nSince I could no longer drive to the store, I signed up for Shipt to have my groceries delivered. Since I wasn\u0026rsquo;t sure if I would like the service (or if I\u0026rsquo;d give up and get another car) I paid the monthly rate of $14/mo instead of the $99 yearly rate. So I spent $168 on Shipt in the past year. Please keep in mind, there is a slight per-item markup with Shipt, but I didn\u0026rsquo;t track it and just budgeted \u0026lsquo;groceries\u0026rsquo; as a total bill.\nAnd finally, if I wanted to go visit friends or family outside Columbus, I needed to rent a car. I have a decent reward membership with Enterprise and there is one close to home, so that\u0026rsquo;s where I rent. In the past year, I\u0026rsquo;ve given them $1041.14 of my hard earned income. I\u0026rsquo;ve also had to put a tank of gas or two into those rentals, but I didn\u0026rsquo;t think to track them.\nAnd, honestly, that\u0026rsquo;s it. I really figured I\u0026rsquo;d have to sign up for a ton of things to \u0026ldquo;replace\u0026rdquo; my car, but that\u0026rsquo;s really all there is. It helps (tremendously) that I\u0026rsquo;m able to telecommute for my dayjob. Of course, that\u0026rsquo;s also why I was thinking that having a car is a waste. I paid (a lot) of money, monthly, for a car to sit in my driveway.\nFor those still with me, the math looks like this:\n5290.56 -- yearly car payments - 168.00 -- yearly Shipt -1674.83 -- Uber for the year -1041.14 -- car rentals for the year ~~~~~~~~ $2,406.59 money in my pocket And bam, just like that I\u0026rsquo;m ~$2500/yr richer. And that\u0026rsquo;s without considering fuel, oil, tires, maintenance, or insurance. Over a standard 5 yr car loan, that\u0026rsquo;s ~$12500 I save! So yeah, a good decision for me, I think.\n","permalink":"https://hunleyd.github.io/posts/car-free-in-columbus/","tags":["general"],"title":"Car(e) free in Columbus"},{"categories":["postgresql","osx","howto"],"contents":"With the release of PostgreSQL 10, I\u0026rsquo;ve updated my pg script. You might recall from previous posts that this script is for Homebrew users that have tapped Peter\u0026rsquo;s brew recipes. It allows for installing and switching between multiple version of PostgreSQL seemlessly. While I was in there adding v10 support, I tweaked and tuned the code a bit and tidyied up the output significantly. I\u0026rsquo;m pretty pleased with the new version actually.\nAs always, it\u0026rsquo;s been added as a gist:\n#!/bin/zsh wanted_ver=$1 no_restart= # is the version requested installed? brew ls --version postgresql@${wanted_ver} \u0026amp;\u0026gt;/dev/null if [[ $? -eq 0 ]] ; then # yes, carry on : else # nope, so install it echo \u0026#34;Installing PostgreSQL ${wanted_ver}... \u0026#34; brew install petere/postgresql/postgresql@${wanted_ver} fi # is postgresql is running? for i in /usr/local/var/postgres/* do check_ver=$(basename ${i}) is_running=$(ps -few|egrep -- \u0026#34;[p]ostgres.*-D.*${check_ver}\u0026#34;) if [[ -z ${is_running} ]] ; then # nope, carry on : else # it is. is it the requested version? if [[ \u0026#34;${wanted_ver}\u0026#34; = \u0026#34;${check_ver}\u0026#34; ]] ; then # yup, carry on no_restart=t else # nope, so kill it echo -n \u0026#34;Stopping PostgreSQL ${check_ver}... \u0026#34; /usr/local/opt/postgresql@${check_ver}/bin/pg_ctl \\ -D /usr/local/var/postgres/${check_ver} \\ stop -w -mf | grep \u0026#39;stopped\u0026#39; fi fi done # what version is active? if [[ -e /usr/local/bin/psql ]] ; then active_ver=$(/usr/bin/stat -f %Y /usr/local/bin/psql | cut -d\\/ -f3 | cut -d\\- -f2 | cut -d\\@ -f2) else active_ver=0 fi # is the active version the requested version? if [[ \u0026#34;${active_ver}\u0026#34; = \u0026#34;${wanted_ver}\u0026#34; ]] ; then # yup, carry on : else # nope, so deactivate it echo -n \u0026#34;Deactivating PostgreSQL ${active_ver}... \u0026#34; brew unlink --force --overwrite postgresql@${active_ver} | cut -d\\ -f3- # and activate the correct version echo -n \u0026#34;Activating PostgreSQL ${wanted_ver}... \u0026#34; brew link --force --overwrite postgresql@${wanted_ver} | grep \u0026#39;created\u0026#39; | cut -d\\ -f3- fi # point to the correct data dir and port PGDATA=/usr/local/var/postgres/${wanted_ver} PGPORT=\u0026#34;54$(echo ${wanted_ver} | tr -d .)\u0026#34; # should we be starting a cluster? if [[ \u0026#34;${no_restart}\u0026#34; = \u0026#34;t\u0026#34; ]] ; then # nope, carry on : else # yup. has the cluster been initialized? if [[ ! -d ${PGDATA} ]] ; then # nope, so let\u0026#39;s do that echo \u0026#34;Initializing PostgreSQL ${wanted_ver} cluster... \u0026#34; initdb -k ${PGDATA} || initdb ${PGDATA} echo \u0026#34;port = ${PGPORT}\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_destination = \u0026#39;stderr\u0026#39;\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;logging_collector = on\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_filename = \u0026#39;postgresql-%a.log\u0026#39;\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_truncate_on_rotation = on\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_rotation_age = 1d\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_rotation_size = 0\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_timezone = \u0026#39;US/Michigan\u0026#39;\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_line_prefix = \u0026#39;%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h \u0026#39;\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf else # yup, carry on : fi # start the cluster echo -n \u0026#34;Starting PostgreSQL ${wanted_ver}... \u0026#34; pg_ctl -D ${PGDATA} start \u0026gt; /tmp/pg.start 2\u0026gt;\u0026amp;1 grep \u0026#39;server start\u0026#39; /tmp/pg.start if [[ -x /usr/local/bin/pg_isready ]] ; then ret=1 while [[ ${ret} -eq 1 ]] do # wait for the cluster to be available before exiting pg_isready -q ret=$? done fi fi echo \u0026#34;export PGDATA=${PGDATA}\u0026#34; echo \u0026#34;export PGPORT=${PGPORT}\u0026#34; Enjoy.\n","permalink":"https://hunleyd.github.io/posts/updated-postgresql-homebrew-script/","tags":["postgresql","osx","howto"],"title":"updated PostgreSQL homebrew script"},{"categories":["postgresql"],"contents":"Connecting to a PostgreSQL instance isn\u0026rsquo;t hard generally, but sometimes you can run into issues. Sometimes a port isn\u0026rsquo;t open on a firewall, or the server is in a VLAN that you can\u0026rsquo;t get to, or perhaps the server isn\u0026rsquo;t running on the network interface you think it is. More commonly, you can reach the PostgreSQL instance but you\u0026rsquo;re connection isn\u0026rsquo;t authorized (which is not the same as being unable to authenticate). Fortunately, the error messages returned in these different failure scenarios are fairly verbose and distinct so you can easily tell which scenario you\u0026rsquo;re facing. Let\u0026rsquo;s dive into each scenario and see what the error looks like, shall we?\nScenario 1 - Bad password Let\u0026rsquo;s first assume that everything is working and you can actually connect to the PostgreSQL instance, but you can\u0026rsquo;t authenticate. The error will look like this:\nAs you can see, the message makes it pretty clear that you were able to connect, but your credentials were wrong (you were authorized to connect, but failed to authenticate). Did you type the password incorrectly? Is there a ~/.pgpass file that is providing the password for you? Do you have $PGPASSWORD defined in your environment? Fix the password being passed to PostgreSQL, and you won\u0026rsquo;t have further issues in this scenario.\nScenario 2 - pg_hba.conf rejects you For our second scenario, we\u0026rsquo;re going to assume that you can actually connect to the PostgreSQL instance, but there is an entry in pg_hba.conf denying you access. First, we\u0026rsquo;ll try connecting via a local Unix socket:\nAs you can see, it straight up tells you that you have been explicitly denied access. It may be rejecting connections via Unix sockets completely, or it may be rejecting connection as the specified user via Unix socket, or it may be rejecting connections to the specified database via Unix sock. You could determine which of these scenarios is true by trying a different user on the same database and then trying the same user on a different database. In any case, the problem is not networking or firewall related. The DBA needs to adjust pg_hba.conf to allow connections of this type, or you need to connect via a TCP port instead of a Unix socket. Discuss with your DBA.\nNow, let\u0026rsquo;s try via a TCP port:\nAgain, you can see that it pretty plainly tells you that you have been explicitly denied. In this case though, it may be rejecting your IP address specifically, your entire network segment, your id, that database, or the fact that you didn\u0026rsquo;t make an SSL connection. You can whittle this down by trying a different user on the same db, trying the same user on a different db, or switching to an SSL connection and repeating these tests. (I assume that you can\u0026rsquo;t change your IP address. But perhaps you could make the same test cases from another computer). Again, you\u0026rsquo;ll probably need the DBA to resolve this with you.\nScenario 3 - pg_hba.conf doesn\u0026rsquo;t allow you I know what you\u0026rsquo;re thinking. \u0026ldquo;Isn\u0026rsquo;t this the same as above?\u0026rdquo;. And the answer is \u0026ldquo;no it is not\u0026rdquo;. Above, the pg_hba.conf file had an entry that matched your incoming connection and said to explicitly reject it. In this scenario, there is no entry that matches your connection and you end up implicitly denied.\nAgain, we\u0026rsquo;ll start by using a Unix socket:\nOnce more, the message pretty clearly tells you what is wrong. As it says, there is no entry in pg_hba.conf that matches your incoming connection. And since PostgreSQL tries to err on the side of caution, when it can\u0026rsquo;t find an entry stating definitively what to do, it rejects you. The same troubleshooting steps as above apply (change the user/db, etc). And also like above, the DBA is going to need to edit pg_hba.conf to add an entry for your connection.\nNow, what does it look like over a TCP port:\nIt\u0026rsquo;s the same error message as above but showing your IP address instead of [local]. And the same debugging applies. Once again, the DBA will need to add an entry to pg_hba.conf to resolve this.\nScenario 4 - everything else (no, really) By default, PostgreSQL listens for connections on port 5432. Sometimes, your DBA (or your vendor) has chosen to run PostgreSQL on a different port for some reason. If you are not trying to connect to the correct port, you\u0026rsquo;ll get an error from psql.\nIf you were trying to connect via a Unix socket, you\u0026rsquo;ll see:\nIf you were trying a TCP port connection, you\u0026rsquo;d see:\nThis error indicates that it cannot establish a network connection with PostgreSQL at the IP address (or UNix socket) you specified on the default port. Check to make sure you don\u0026rsquo;t have $PGPORT set incorrectly in your environment.\nIf you are using the correct port, and you still see one of two errors above, then the issue will be one of the following:\nPostgreSQL isn\u0026rsquo;t running PostgreSQL is running, but on a different IP address than you\u0026rsquo;re trying to connect to PostgreSQL is running, but you cannot establish a network connection from here to there If PostgreSQL isn\u0026rsquo;t running, talk to your DBA about why (maybe it died, maybe it\u0026rsquo;s a maintenance window). If you are affected by the 2nd bullet item, you\u0026rsquo;ll have to talk with your DBA about the proper IP address to use when connecting. Note that, by default, PostgreSQL only listens on localhost and none of your other interfaces. If this hasn\u0026rsquo;t been changed (listen_addresses in postgresql.conf) then you\u0026rsquo;ll fall into this failure category. And if you\u0026rsquo;re plagued by the 3rd bullet item, you\u0026rsquo;ll have to talk to your network admin (and probably your DBA) as there may be a firewall blocking you, or your VLAN can\u0026rsquo;t connect to the other VLAN, or some other layer 3/4 tomfoolery.\nSo now you know all the (common) ways that connecting to PostgreSQL can fail and how to distinguish between them. With just a little bit of knowledge, you can communicate exactly what is happening to your DBA and make it easier for him/her to rectify the issue. Go forth, and happy PostgreSQL-ing.\n","permalink":"https://hunleyd.github.io/posts/when-you-cannot-get-there-from-here/","tags":["postgresql"],"title":"When you cannot get there from here"},{"categories":["postgresql","osx","howto"],"contents":"If you\u0026rsquo;ve followed my previous posts (here and here), then you already have one or more versions of PostgreSQL installed on your Mac. Maybe these are solely for test or dev purposes and you don\u0026rsquo;t really care about any of the data therein, but if you do, let me guide you to pgBackRest.\npgBackRest aims to be a simple, reliable backup and restore system that can seamlessly scale up to the largest databases and workloads.\nInstead of relying on traditional backup tools like tar and rsync, pgBackRest implements all backup features internally and uses a custom protocol for communicating with remote systems. Removing reliance on tar and rsync allows for better solutions to database-specific backup challenges. The custom remote protocol allows for more flexibility and limits the types of connections that are required to perform a backup which increases security.\npgBackRest is written in Perl, but don\u0026rsquo;t hold that against it. As of the 1.19 release, pgBackRest can now use S3 buckets as the storage backend. I really like pgBackRest and tend to use it for myself and customers over any of the other tools in the PostgreSQL ecosphere. So, let\u0026rsquo;s get started by downloading the latest release from their site, and then installing it. For some reason, no one has added pgBackRest to Homebrew yet (someone, pls!) so let\u0026rsquo;s do it the manual way:\n$ wget -o Downloads/pgbackrest-release-1.19.tar.gz https://github.com/pgbackrest/pgbackrest/archive/release/1.19.tar.gz $ tar xvf Downloads/pgbackrest-release-1.19.tar.gz $ cd pgbackrest-release-1.19 $ sudo cp -r lib/pgBackRest /Library/Perl/5.18 $ sudo find /Library/Perl/5.18/pgBackRest -type f -exec chmod 644 {} + $ sudo find /Library/Perl/5.18/pgBackRest -type d -exec chmod 755 {} + $ sudo mv bin/pgbackrest /usr/local/bin $ sudo chmod 755 /usr/local/bin/pgbackrest $ sudo mkdir -m 770 /var/log/pgbackrest $ sudo mkdir -m 770 /var/spool/pgbackrest $ sudo chown doug /var/{log,spool}/pgbackrest (Keep in mind that I already had Perl setup to connect to PostgreSQL for other uses. You might need to install DBD::Pg.)\nNow that pgBackRest is installed, let\u0026rsquo;s configure it. First, we\u0026rsquo;ll want to set some of the global properties that affect all pgBackRest operations:\n[global] log-level-console=info repo-s3-bucket=hunleyd-pgbackrest repo-s3-endpoint=s3.amazonaws.com repo-s3-key=XX repo-s3-key-secret=XXX repo-s3-region=us-east-2 repo-type=s3 retention-full=2 start-fast=y As you can see, we set the following:\nforce the log level for all console output to \u0026lsquo;info\u0026rsquo; define the S3 bucket we want to use define the S3 endpoint to connect to define our S3 key define our S3 secret key set which region our bucket is in tell pgBackRest that we\u0026rsquo;re using S3 as the backend configure retention of full backups tell pgBackRest to issue a CHECKPOINT so backups can start right away instead of waiting for the next regular checkpoint Now, we need to tell pgBackRest which instance of PostgreSQL we want to backup and where to find it. Again, if you used my previous posts to install multiple versions via Homebrew, this should look familiar:\n[96] db-path=/usr/local/var/postgres/9.6 db-port=5496 repo-path=/96 [95] db-path=/usr/local/var/postgres/9.5 db-port=5495 repo-path=/95 [94] db-path=/usr/local/var/postgres/9.4 db-port=5494 repo-path=/94 [93] db-path=/usr/local/var/postgres/9.3 db-port=5493 repo-path=/93 [92] db-path=/usr/local/var/postgres/9.2 db-port=5492 repo-path=/92 [91] db-path=/usr/local/var/postgres/9.1 db-port=5491 repo-path=/91 [90] db-path=/usr/local/var/postgres/9.0 db-port=5490 repo-path=/90 You can see for each pg cluster, we define:\nthe path to the $PGDATA directory the port the cluster listens on and the path we want to store the backups in on our backend When you put this all together, we\u0026rsquo;ll be connecting to an S3 bucket called, creatively enough, hunleyd-pgbackrest and then we will create a top-level directory (\u0026lsquo;96\u0026rsquo;, \u0026lsquo;95\u0026rsquo;, etc) to store each cluster\u0026rsquo;s backups in.\nNow that we\u0026rsquo;ve got our configuration complete, let\u0026rsquo;s do an initial backup of one of the clusters. First, we have to create the appropriate directories and metadata on the backend:\n$ pgbackrest --stanza=92 --config $HOME/.config/pgbackrest/pgbackrest.conf stanza-create 2017-06-14 14:03:46.643 P00 INFO: stanza-create command begin 1.19: --config=/Users/doug/.config/pgbackrest/pgbackrest.conf --db-path=/usr/local/var/postgres/9.2 --db-port=5492 --log-level-console=info --repo-path=/92 --repo-s3-bucket=hunleyd-pgbackrest --repo-s3-endpoint=s3.amazonaws.com --repo-s3-region=us-east-2 --repo-type=s3 --stanza=92 2017-06-14 14:03:57.971 P00 INFO: stanza-create command end: completed successfully Then, we have pgBackRest verify that everything is properly setup. Note that this includes checking to ensure you tweaked postgresql.conf according to the directions on their site (I\u0026rsquo;m not going to repeat them here):\n$ pgbackrest --stanza=92 --config $HOME/.config/pgbackrest/pgbackrest.conf check 2017-06-14 14:04:17.991 P00 INFO: check command begin 1.19: --config=/Users/doug/.config/pgbackrest/pgbackrest.conf --db-path=/usr/local/var/postgres/9.2 --db-port=5492 --log-level-console=info --repo-path=/92 --repo-s3-bucket=hunleyd-pgbackrest --repo-s3-endpoint=s3.amazonaws.com --repo-s3-region=us-east-2 --repo-type=s3 --stanza=92 2017-06-14 14:04:32.576 P00 INFO: WAL segment 000000010000000000000067 successfully stored in the archive at \u0026#39;/92/archive/92/9.2-1/0000000100000000/000000010000000000000067-24adde40a35b1f3ed17f545153f0e01c44b0ada5.gz\u0026#39; 2017-06-14 14:04:32.576 P00 INFO: check command end: completed successfully And since that all worked, we can take our first actual backup:\n$ pgbackrest --stanza=92 --config $HOME/.config/pgbackrest/pgbackrest.conf backup 2017-06-14 14:10:44.378 P00 INFO: backup command begin 1.19: --config=/Users/doug/.config/pgbackrest/pgbackrest.conf --db-path=/usr/local/var/postgres/9.2 --db-port=5492 --log-level-console=info --repo-path=/92 --repo-s3-bucket=hunleyd-pgbackrest --repo-s3-endpoint=s3.amazonaws.com --repo-s3-region=us-east-2 --repo-type=s3 --retention-full=2 --stanza=92 --start-fast WARN: no prior backup exists, incr backup has been changed to full 2017-06-14 14:10:49.435 P00 INFO: execute exclusive pg_start_backup() with label \u0026#34;pgBackRest backup started at 2017-06-14 14:10:45\u0026#34;: backup begins after the requested immediate checkpoint completes 2017-06-14 14:10:49.846 P00 INFO: backup start archive = 000000010000000000000068, lsn = 0/68000020 2017-06-14 14:10:57.090 P01 INFO: backup file /usr/local/var/postgres/9.2/pg_log/2017/week-16/index.html (1.3MB, 3%) checksum dcc73afe15f48863eb019b9cfbb1e24cbd2a4d7f \u0026lt;snip\u0026gt; 2017-06-14 14:24:23.277 P01 INFO: backup file /usr/local/var/postgres/9.2/base/1/12040 (0B, 100%) 2017-06-14 14:24:23.964 P01 INFO: backup file /usr/local/var/postgres/9.2/base/1/12031 (0B, 100%) 2017-06-14 14:24:24.747 P01 INFO: backup file /usr/local/var/postgres/9.2/base/1/12021 (0B, 100%) 2017-06-14 14:24:24.778 P00 INFO: full backup size = 38.1MB 2017-06-14 14:24:24.778 P00 INFO: execute exclusive pg_stop_backup() and wait for all WAL segments to archive 2017-06-14 14:24:41.428 P00 INFO: backup stop archive = 000000010000000000000068, lsn = 0/68000178 2017-06-14 14:24:45.320 P00 INFO: new backup label = 20170614-141045F 2017-06-14 14:24:47.844 P00 INFO: backup command end: completed successfully 2017-06-14 14:24:47.920 P00 INFO: expire command begin 1.19: --config=/Users/doug/.config/pgbackrest/pgbackrest.conf --log-level-console=info --repo-path=/92 --repo-s3-bucket=hunleyd-pgbackrest --repo-s3-endpoint=s3.amazonaws.com --repo-s3-region=us-east-2 --repo-type=s3 --retention-archive=2 --retention-full=2 --stanza=92 2017-06-14 14:24:50.496 P00 INFO: full backup total \u0026lt; 2 - using oldest full backup for 9.2-1 archive retention 2017-06-14 14:24:51.796 P00 INFO: expire command end: completed successfully Neat!\nNow, let\u0026rsquo;s check our S3 bucket, shall we?\nYou can see here the top-level contents of my hunleyd-pgbackrest bucket. As stated before, each cluster gets its own sub-dir. Since we just backed up the \u0026lsquo;92\u0026rsquo; cluster, let\u0026rsquo;s look inside it\u0026rsquo;s dir.\nYou can see that pgBackRest has created as directory for the WALs to be stored in whenever archive_command fires and another directory for the actual cluster backups. Peeking into the archive dir, we see:\nThis shows us some metadata, and shows that pgBackRest creates a directory for each timeline of the cluster. Since we are on timeline 1 in our 92 cluster, we have a 9.2-1 directory inside of which, we find:\nOur archived WALs have been compressed and uploaded. Hurray!\nNow, let\u0026rsquo;s check inside the backup directory:\nWe can see some metadata, and we can see a folder named the same as the backup label that was used when we ran our full backup. Inside that folder, we can see:\nHey look, more metadata! And another folder! :) So, let\u0026rsquo;s dive into the pg_data folder where we see:\nHoly crap! It\u0026rsquo;s a basebackup of our $PGDATA data directory. And all the files have been nicely compressed for us. Rock on, pgBackRest!\nAnd just in case you wanted to see the current backup catalog:\n$ pgbackrest --stanza=92 --config $HOME/.config/pgbackrest/pgbackrest.conf info stanza: 92 status: ok wal archive min/max: 000000010000000000000068 / 000000010000000000000068 full backup: 20170614-141045F timestamp start/stop: 2017-06-14 14:10:45 / 2017-06-14 14:24:43 wal start/stop: 000000010000000000000068 / 000000010000000000000068 database size: 38.1MB, backup size: 38.1MB repository size: 5.6MB, repository backup size: 5.6MB (look at that compression!)\n","permalink":"https://hunleyd.github.io/posts/installing-pgbackrest-on-osx/","tags":["postgresql","osx","howto"],"title":"Installing pgBackRest on OSX"},{"categories":["general"],"contents":"Thank you, Loui, for bringing so much joy and happiness to our lives these past ten years. I will miss you.\n","permalink":"https://hunleyd.github.io/posts/goodbye-loui-boy/","tags":["general"],"title":"Goodbye, Loui boy"},{"categories":["blog"],"contents":"UPDATE: I\u0026rsquo;ve switched out the theme entirely (again) so the CSS issues should no longer be present. The rest of this post still sands.\nAs you\u0026rsquo;ve probably noticed, I\u0026rsquo;ve launched the new blog design. Everything that was here should still be here, but it might have moved. Sorry, but I broke the old permalinks. Bad blogger! However, I now have everything exactly where I want it, so it shouldn\u0026rsquo;t ever change again. Famous last words, right there :)\nSo anyway, let me know if you find issues w/ the site. I\u0026rsquo;m aware of some small CSS fuckery here and there, but I got tired of beating my head against it honestly. If anyone wants to tell me what\u0026rsquo;s wrong with it and how to fix it, I\u0026rsquo;ll buy you a virtual beer. If you want to let your opinion known about the new design (nice! it sucks! meh.) feel free to hit me up too. If enough people kvetch, I\u0026rsquo;ll tweak it.\nEnjoy or something.\n","permalink":"https://hunleyd.github.io/posts/new-look-same-content/","tags":["blog"],"title":"New look, same content"},{"categories":["blog"],"contents":"In case you haven\u0026rsquo;t already noticed, I\u0026rsquo;ve made some changes to my blog recently. Nothing really significant or drastic, but things that have been on the TODO list for a while.\nI guess the most obvious change would be that I added pagination finally. If you scroll the homepage all the way down, instead of it going back through the entire blog\u0026rsquo;s history, it stops after five posts, and then gives a clickable page counter. Once I had this implemented, I then had to un-paginate the archive pages which was interesting.\nThe next most visible change would probably be the change I made to the CSS concerning CODE and CODEBLOCK. I\u0026rsquo;m still not 100% sure this is the final form, but I like it better than it was. For reference, it now looks like this.\nMy third change was to tweak the site\u0026rsquo;s header a bit. If you cast your gaze upwards, you can see that I now have \u0026lsquo;About\u0026rsquo;, \u0026lsquo;Archive\u0026rsquo;, and \u0026lsquo;Feeds\u0026rsquo; in the header. The \u0026lsquo;Feeds\u0026rsquo; link is new, as is the page that it links to. The actual RSS feeds themselves have existed for a while now, I just didn\u0026rsquo;t announce them.\nAnd finally, I swapped out my photo for my new Bitmoji-based avatar.\nSo, uh, enjoy. Or something.\n","permalink":"https://hunleyd.github.io/posts/changes-to-the-blog/","tags":["blog"],"title":"Changes to the blog"},{"categories":["postgresql"],"contents":" We\u0026rsquo;ve all gotten the dreaded email/call from a user stating that a query is \u0026ldquo;slow sometimes\u0026rdquo;. If you\u0026rsquo;re lucky, the \u0026ldquo;sometimes\u0026rdquo; actually ends up being fairly consistent and you can fairly easily determine what\u0026rsquo;s happening (an errant cron job, for example). All too often though, the issue really is sporadic, fleeting, and indeterministic. So how do you track these down? And more importantly what do you do about them once found?\nFor starters, you as the DBA should have your PostgreSQL logging configured to log these slow performing queries. After all, you and the devs and the users can agree that all queries should complete in some measure of time (1 sec, 1 minute, etc). So, once you know what this acceptable elapsed time is, you can easily log any query that runs longer by just setting this in your postgresql.conf:\nlog_min_duration_statement = 1000 # log anything running longer than 1s And now, you have all queries with long run times logged automatically. And these show up nicely in your pgBadger reports too!\nIf you\u0026rsquo;re lucky, you\u0026rsquo;ll be able to use EXPLAIN to see why the query is behaving poorly. However, if your life is like mine, the explain plan will be reasonable and won\u0026rsquo;t have any smoking guns to speak of. Which means the performance is either load dependent or being influenced by other processes (something is blowing out your caches, for example). In these cases, what you really need is the EXPLAIN output from the very instant that it performed poorly. However, you can\u0026rsquo;t go back in time to get it. But what you can do is make use of the auto_explain module that ships with PostgreSQL.\nIn case the name wasn\u0026rsquo;t obvious enough, the auto_explain module causes PostgreSQL to automatically run EXPLAIN on queries according to thresholds that you configure. These automatically generated plans are then logged into the normal PostgreSQL logs. Let\u0026rsquo;s walk through setting it up and see how it works.\nFirst, in your postgresql.conf we want to enable the module:\nshared_preload_libraries = \u0026#39;auto_explain\u0026#39; # change requires restart As stated, you will have to restart the postmaster to get the module to load. However, let\u0026rsquo;s configure it in postgresql.conf first:\n# Add settings for extensions here # # auto_explain # http://www.postgresql.org/docs/current/static/auto-explain.html auto_explain.log_analyze = true auto_explain.log_timing = true auto_explain.log_verbose = true auto_explain.log_min_duration = \u0026#39;1000ms\u0026#39; auto_explain.log_nested_statements = true auto_explain.log_buffers = true # auto_explain What we\u0026rsquo;ve done here is configure auto_explain to\nuse EXPLAIN ANALYZE1 to use the TIMING option of EXPLAIN to use the VERBOSE option of EXPLAIN to log the plan for anything running longer than 1 second (matches log_min_duration_statement, above) to include statements inside a function to also be logged to use the BUFFERS option of EXPLAIN As with most GUC in PostgreSQL, these can all be changed using SET in a given session, but we\u0026rsquo;re setting the defaults here. Now that we have them setup, let\u0026rsquo;s see what it looks like in practice.\n\u0026gt; CREATE TABLE x(t text); CREATE TABLE Time: 6.022 ms \u0026gt; INSERT INTO x(t) SELECT generate_series(1,10000); INSERT 0 10000 Time: 23.565 ms We connected to PostgreSQL, created a test table, and then used generate_series to insert 10k rows. In our logs, the following were added:\n2016-11-28 13:20:30 EST [28838]: [18-1] user=doug,db=doug,app=psql,client=[local] LOG: duration: 33.987 ms statement: CREATE TABLE x(t text); 2016-11-28 13:20:59 EST [28838]: [19-1] user=doug,db=doug,app=psql,client=[local] LOG: duration: 16.461 ms plan: Query Text: INSERT INTO x(t) SELECT generate_series(1,10000); Insert on public.x (cost=0.00..50.02 rows=1000 width=32) (actual time=16.459..16.459 rows=0 loops=1) Buffers: shared hit=10085 read=47 dirtied=45 I/O Timings: read=0.012 -\u0026gt; Subquery Scan on \u0026#34;*SELECT*\u0026#34; (cost=0.00..50.02 rows=1000 width=32) (actual time=0.010..4.755 rows=10000 loops=1) Output: \u0026#34;*SELECT*\u0026#34;.generate_series -\u0026gt; Result (cost=0.00..15.02 rows=1000 width=4) (actual time=0.007..1.364 rows=10000 loops=1) Output: generate_series(1, 10000) 2016-11-28 13:20:59 EST [28838]: [20-1] user=doug,db=doug,app=psql,client=[local] LOG: duration: 23.374 ms statement: INSERT INTO x(t) SELECT generate_series(1,10000); 2016-11-28 13:21:00 EST [30079]: [1-1] user=,db=,app=,client= LOG: automatic analyze of table \u0026#34;doug.public.x\u0026#34; system usage: CPU 0.00s/0.11u sec elapsed 0.14 sec (Note that for illustrative purposes, I issued SET auto_explain.log_min_duration = '0ms')\nSo, you can see that the CREATE TABLE didn\u0026rsquo;t log anything through the auto_explain module, but the INSERT INTO did. This is a boring example, so let\u0026rsquo;s try a SELECT against our table:\n\u0026gt; SELECT * FROM x ORDER BY t LIMIT 10; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ t ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 1 ‚îÇ ‚îÇ 10 ‚îÇ ‚îÇ 100 ‚îÇ ‚îÇ 1000 ‚îÇ ‚îÇ 10000 ‚îÇ ‚îÇ 1001 ‚îÇ ‚îÇ 1002 ‚îÇ ‚îÇ 1003 ‚îÇ ‚îÇ 1004 ‚îÇ ‚îÇ 1005 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (10 rows) Time: 11.982 ms and the logs look like:\n2016-11-28 13:27:38 EST [322]: [7-1] user=,db=,app=,client= LOG: checkpoint starting: time 2016-11-28 13:27:46 EST [322]: [8-1] user=,db=,app=,client= LOG: checkpoint complete: wrote 75 buffers (0.0%); 0 transaction log file(s) added, 0 removed, 0 recycled; write=7.569 s, sync=0.092 s, total=7.920 s; sync files=23, longest=0.092 s, average=0.004 s; distance=685 kB, estimate=685 kB 2016-11-28 13:28:48 EST [28838]: [21-1] user=doug,db=doug,app=psql,client=[local] LOG: duration: 11.120 ms plan: Query Text: SELECT * FROM x ORDER BY t LIMIT 10; Limit (cost=561.10..561.12 rows=10 width=4) (actual time=11.073..11.073 rows=10 loops=1) Output: t Buffers: shared hit=45 -\u0026gt; Sort (cost=561.10..586.10 rows=10000 width=4) (actual time=11.072..11.072 rows=10 loops=1) Output: t Sort Key: x.t Sort Method: top-N heapsort Memory: 25kB Buffers: shared hit=45 -\u0026gt; Seq Scan on public.x (cost=0.00..345.00 rows=10000 width=4) (actual time=0.018..1.224 rows=10000 loops=1) Output: t Buffers: shared hit=45 2016-11-28 13:28:48 EST [28838]: [22-1] user=doug,db=doug,app=psql,client=[local] LOG: duration: 11.813 ms statement: SELECT * FROM x ORDER BY t LIMIT 10; (You can safely ignore the checkpoint lines at the top there)\nThere you have both the statement we ran, and the full EXPLAIN plan. You can see we did a sequential scan on the table (looks like it was all in the shared_buffers too) and then we passed that up to a sort node for an in-memory sort, and then passed that result set up to the limit node.\nWhile this is a stupid simple example, I hope you can see that having this in production for large, complicated queries will allow you to better diagnose issues. For example, simply doing a manual EXPLAIN ANALYZE on the same query and seeing that you get a different plan is potentially enough to rule out (or in) certain culprits for the intermittent performance issue. 1 - This option causes the postmaster to collect info on every statement executed, even if auto_explain isn\u0026rsquo;t going to log it. It has a measurable impact on overall performance. Please test on your workload and decide for yourself if the overhead is worth the trade-off\n","permalink":"https://hunleyd.github.io/posts/explaining-intermittent-perf-problems/","tags":["postgresql"],"title":"EXPLAINing intermittent perf problems"},{"categories":["postgresql"],"contents":"PostgreSQL has a pretty extensive logging facility. I\u0026rsquo;ve talked briefly about configuring it to get the most out of pgBadger before, but today I\u0026rsquo;m gonna talk a bit about the naming of the log file itself. The chosen filename doesn\u0026rsquo;t have to be static. You can, in fact, have the name dynamically created by using strftime() escapes. But what exactly are those?\nThe strftime() function formats the information from timeptr into the buffer s, according to the string pointed to by format. The format string consists of zero or more conversion specifications and ordi- nary characters. All ordinary characters are copied directly into the buffer. A conversion specification consists of a percent sign ```%\u0026#39;\u0026#39;\u0026#39; and one other character. Say what? Essentially, given a timestamp and a format specification, you\u0026rsquo;ll get the timestamp back in a different formatted output. So what are these format specifications? Well, they are defined by your systems libc implementation. On Linux, this is (typically) glibc, whereas on OSX and the BSDs, it\u0026rsquo;s BSD libc. You really shouldn\u0026rsquo;t see a difference between these two, but it could happen. On macOS Sierra, they are (for the examples below, we\u0026rsquo;ll use today\u0026rsquo;s date of Monday, November 21, 2016 with a timestamp of 1pm EST):\n%A is replaced by national representation of the full weekday name (\u0026#34;Monday\u0026#34;) %a is replaced by national representation of the abbreviated weekday name (\u0026#34;Mon\u0026#34;) %B is replaced by national representation of the full month name (\u0026#34;November\u0026#34;) %b is replaced by national representation of the abbreviated month name (\u0026#34;Nov\u0026#34;) %C is replaced by (year / 100) as decimal number; single digits are preceded by a zero (\u0026#34;20\u0026#34;) %c is replaced by national representation of time and date (\u0026#34;Mon Nov 21 13:00:00 2016\u0026#34;) %D is equivalent to ``%m/%d/%y\u0026#39;\u0026#39; (\u0026#34;11/21/16\u0026#34;) %d is replaced by the day of the month as a decimal number (01-31) (\u0026#34;21\u0026#34;) %e is replaced by the day of the month as a decimal number (1-31); single digits are preceded by a blank (\u0026#34;21\u0026#34;) %F is equivalent to ``%Y-%m-%d\u0026#39;\u0026#39; (\u0026#34;2016-11-21\u0026#34;) %G is replaced by a year as a decimal number with century. This year is the one that contains the greater part of the week (Monday as the first day of the week) (\u0026#34;2016\u0026#34;) %g is replaced by the same year as in ``%G\u0026#39;\u0026#39;, but as a decimal number without century (00-99) (\u0026#34;16\u0026#34;) %H is replaced by the hour (24-hour clock) as a decimal number (00-23) (\u0026#34;12\u0026#34;) %h the same as %b (\u0026#34;Nov\u0026#34;) %I is replaced by the hour (12-hour clock) as a decimal number (01-12) (\u0026#34;12\u0026#34;) %j is replaced by the day of the year as a decimal number (001-366) (\u0026#34;326\u0026#34;) %k is replaced by the hour (24-hour clock) as a decimal number (0-23); single digits are preceded by a blank (\u0026#34;13\u0026#34;) %l is replaced by the hour (12-hour clock) as a decimal number (1-12); single digits are preceded by a blank (\u0026#34; 1\u0026#34;) %M is replaced by the minute as a decimal number (00-59) (\u0026#34;00\u0026#34;) %m is replaced by the month as a decimal number (01-12) (\u0026#34;11\u0026#34;) %n is replaced by a newline %p is replaced by national representation of either \u0026#34;ante meridiem\u0026#34; (a.m.) or \u0026#34;post meridiem\u0026#34; (p.m.) as appropriate (\u0026#34;PM\u0026#34;) %R is equivalent to ``%H:%M\u0026#39;\u0026#39; (\u0026#34;13:00\u0026#34;) %r is equivalent to ``%I:%M:%S %p\u0026#39;\u0026#39; (\u0026#34;01:00:00 PM\u0026#34;) %S is replaced by the second as a decimal number (00-60) (\u0026#34;00\u0026#34;) %s is replaced by the number of seconds since the Epoch, UTC (\u0026#34;1479751200\u0026#34;) %T is equivalent to ``%H:%M:%S\u0026#39;\u0026#39; (\u0026#34;13:00:00\u0026#34;) %t is replaced by a tab %U is replaced by the week number of the year (Sunday as the first day of the week) as a decimal number (00-53) (\u0026#34;47\u0026#34;) %u is replaced by the weekday (Monday as the first day of the week) as a decimal number (1-7) (\u0026#34;1\u0026#34;) %V is replaced by the week number of the year (Monday as the first day of the week) as a decimal number (01-53). If the week containing January 1 has four or more days in the new year, then it is week 1; otherwise it is the last week of the previous year, and the next week is week 1 (\u0026#34;47\u0026#34;) %v is equivalent to ``%e-%b-%Y\u0026#39;\u0026#39; (\u0026#34;21-Nov-2016\u0026#34;) %W is replaced by the week number of the year (Monday as the first day of the week) as a decimal number (00-53) (\u0026#34;47\u0026#34;) %w is replaced by the weekday (Sunday as the first day of the week) as a decimal number (0-6) (\u0026#34;1\u0026#34;) %X is replaced by national representation of the time (\u0026#34;13:00:00\u0026#34;) %x is replaced by national representation of the date (\u0026#34;11/21/2016\u0026#34;) %Y is replaced by the year with century as a decimal number (\u0026#34;2016\u0026#34;) %y is replaced by the year without century as a decimal number (00-99) (\u0026#34;16\u0026#34;) %Z is replaced by the time zone name (\u0026#34;EST\u0026#34;) %z is replaced by the time zone offset from UTC; a leading plus sign stands for east of UTC, a minus sign for west of UTC, hours and minutes follow with two digits each and no delimiter between them (\u0026#34;-0500\u0026#34;) %% is replaced by `%\u0026#39; Phew! That was a lot wasn\u0026rsquo;t it? And where exactly does this come into play? As the postgresql.conf says:\nlog_filename = \u0026#39;postgresql-%a.log\u0026#39; # log file name pattern, # can include strftime() escapes So, you can use any of the escapes above to craft a filename that automagically gets updated according to the current timestamp. In my example above, I\u0026rsquo;m getting PostgreSQL to create a new logfile with the local weekday abbreviation. So my $PGDATA/pg_log directory will only ever contain:\npostgresql-Sun.log postgresql-Mon.log postgresql-Tue.log postgresql-Wed.log postgresql-Thu.log postgresql-Fri.log postgresql-Sat.log But wait! The key to all this is that PostgreSQL only evaluates the filename when it first opens/creates the logfile. So, if you start your cluster on Mon and do not restart it and don\u0026rsquo;t have it configured to log rotate, you\u0026rsquo;ll still be writing to postgresql-Mon.log forever. Fortunately, PostgreSQL has you covered here too:\nlog_rotation_age = 1d # Automatic rotation of logfiles will # happen after that time. 0 disables. log_rotation_size = 10MB # Automatic rotation of logfiles will # happen after that much log output. # 0 disables. So, using my log_filename from above, I enable log_rotation_age (set to 1 day) and disable log_rotation_size and I automatically get a new log every day. If I wanted hourly logs, I could do something like:\nlog_filename = \u0026#39;postgresql-%a-%H\u0026#39; log_rotation_age = 1h which would give me logs of postgresql-Mon-00, postgresql-Mon-01, etc. You should be able to see how combining these three postgresql.conf parameters and some crafty strftime() escapes gives you a ton of flexibility in your logging. So go forth and tweak those logs!\n","permalink":"https://hunleyd.github.io/posts/postgresql-logging-strftime-and-you/","tags":["postgresql"],"title":"PostgreSQL logging, strftime, and you"},{"categories":["postgresql"],"contents":" Upgrading your PostgreSQL database from one major version (e.g. 9.4.x) to another major version (e.g. 9.5.x) used to a painful and exceedingly slow process. You essentially had two options: dump / reload the data or use one of the complex logical replication tools.\nThankfully, the PostgreSQL team introduced pg_upgrade back in version 9.0. Because the way data is stored internally in its datafiles in PostgreSQL rarely changes, pg_upgrade is able to re-use the existing datafiles (while manipulating some catalog entries) to \u0026ldquo;short circuit\u0026rdquo; the upgrade process. While this isn\u0026rsquo;t (yet) a true \u0026ldquo;in place upgrade\u0026rdquo; as done by some other databases, it\u0026rsquo;s pretty close. And it\u0026rsquo;s stupid fast. In my testing on my overworked Macbook Pro, it took 1/5 as long to upgrade as a traditional dump and reload. So, let\u0026rsquo;s look at this process shall we?\nFirst, we assume that we have both PostgreSQL 9.5 and 9.6 installed and both have initialized (empty) clusters (see here if you need to do this). We\u0026rsquo;re going to use pgbench to create some data in our PostgreSQL 9.5 instance:\n$ pg 9.5 $ createdb bench1; createdb bench2; createdb bench3 $ pgbench -i -s 15 bench1 ; pgbench -i -s 70 bench2 ; pgbench -i -s 600 bench3 $ pgbench -c 4 -j 2 -T 600 bench1 ; pgbench -c 4 -j 2 -T 600 bench2 ; pgbench -c 4 -j 2 -T 600 bench3 Now that we\u0026rsquo;ve got data in our cluster, we can do the dump. If this were a production instance, this is where you\u0026rsquo;d have to stop your application(s).\n$ time pg_dumpall \u0026gt; data.sql pg_dumpall \u0026gt; data.sql 20.57s user 30.63s system 4% cpu 18:43.70 total We\u0026rsquo;ve now dumped out all our data, and spent 18 minutes with the application(s) down. Let\u0026rsquo;s restore our data to the PostgreSQL 9.6 cluster now:\n$ pg 9.6 $ time psql -f data.sql psql -f data.sql 14.53s user 18.30s system 1% cpu 37:48.49 total After 37 minutes, our data is back and we can start our applications back up. An outage of approximately 56.5 minutes.\nNow, let\u0026rsquo;s blow away our PostgreSQL 9.6 cluster and use pg_upgrade to complete the same task. You would do this with the application(s) down as well!\n$ rm -fr $PGDATA/* $ initdb $PGDATA $ export OPGDATA=$PGDATA/../9.5 $ time pg_upgrade -d $OPGDATA -D $PGDATA -b /usr/local/opt/postgresql-9.5/bin -B /usr/local/opt/postgresql-9.6/bin pg_upgrade -d $OPGDATA -D $PGDATA -b /usr/local/opt/postgresql-9.5/bin -B 0.40s user 12.12s system 1% cpu 10:26.64 total And we\u0026rsquo;re done in 10.5 minutes. It took 1/5 the outage of the dump / load method. And that\u0026rsquo;s on my puny dataset with my overworked laptop! Pretty impressive, no?\nFor the curious, the pg_upgrade output that I omitted above for readability\u0026rsquo;s sake is:\nPerforming Consistency Checks +++++++++++++++++++++++++++-- Checking cluster versions ok Checking database user is the install user ok Checking database connection settings ok Checking for prepared transactions ok Checking for reg* system OID user data types ok Checking for contrib/isn with bigint-passing mismatch ok Checking for roles starting with \u0026#39;pg_\u0026#39; ok Creating dump of global objects ok Creating dump of database schemas ok Checking for presence of required libraries ok Checking database user is the install user ok Checking for prepared transactions ok If pg_upgrade fails after this point, you must re-initdb the new cluster before continuing. Performing Upgrade ++++++++++++++++++ Analyzing all rows in the new cluster ok Freezing all rows on the new cluster ok Deleting files from new pg_clog ok Copying old pg_clog to new server ok Setting next transaction ID and epoch for new cluster ok Deleting files from new pg_multixact/offsets ok Copying old pg_multixact/offsets to new server ok Deleting files from new pg_multixact/members ok Copying old pg_multixact/members to new server ok Setting next multixact ID and offset for new cluster ok Resetting WAL archives ok Setting frozenxid and minmxid counters in new cluster ok Restoring global objects in the new cluster ok Restoring database schemas in the new cluster ok Copying user relation files ok Setting next OID for new cluster ok Sync data directory to disk ok Creating script to analyze new cluster ok Creating script to delete old cluster ok Upgrade Complete +++++++++++++++- Optimizer statistics are not transferred by pg_upgrade so, once you start the new server, consider running: ./analyze_new_cluster.sh Running this script will delete the old cluster\u0026#39;s data files: ./delete_old_cluster.sh ","permalink":"https://hunleyd.github.io/posts/upgrading-postgresql-5x-faster/","tags":["postgresql"],"title":"Upgrading PostgreSQL 5x faster"},{"categories":["postgresql","osx","howto"],"contents":" Following on from this post, you probably have multiple versions of PostgreSQL installed on your Mac. In that post, I added an example function to help you manage all these concurrent installs. Today, I\u0026rsquo;m back with a full-fledged shell script to help manage all this. Without further ado, the script:\n#!/bin/zsh wanted_ver=$1 no_restart= # is the version requested installed? brew ls --version postgresql@${wanted_ver} \u0026amp;\u0026gt;/dev/null if [[ $? -eq 0 ]] ; then # yes, carry on : else # nope, so install it echo \u0026#34;Installing PostgreSQL ${wanted_ver}... \u0026#34; brew install petere/postgresql/postgresql@${wanted_ver} fi # is postgresql is running? for i in /usr/local/var/postgres/* do check_ver=$(basename ${i}) is_running=$(ps -few|egrep -- \u0026#34;[p]ostgres.*-D.*${check_ver}\u0026#34;) if [[ -z ${is_running} ]] ; then # nope, carry on : else # it is. is it the requested version? if [[ \u0026#34;${wanted_ver}\u0026#34; = \u0026#34;${check_ver}\u0026#34; ]] ; then # yup, carry on no_restart=t else # nope, so kill it echo -n \u0026#34;Stopping PostgreSQL ${check_ver}... \u0026#34; /usr/local/opt/postgresql@${check_ver}/bin/pg_ctl \\ -D /usr/local/var/postgres/${check_ver} \\ stop -w -mf | grep \u0026#39;stopped\u0026#39; fi fi done # what version is active? if [[ -e /usr/local/bin/psql ]] ; then active_ver=$(/usr/bin/stat -f %Y /usr/local/bin/psql | cut -d\\/ -f3 | cut -d\\- -f2 | cut -d\\@ -f2) else active_ver=0 fi # is the active version the requested version? if [[ \u0026#34;${active_ver}\u0026#34; = \u0026#34;${wanted_ver}\u0026#34; ]] ; then # yup, carry on : else # nope, so deactivate it echo -n \u0026#34;Deactivating PostgreSQL ${active_ver}... \u0026#34; brew unlink --force --overwrite postgresql@${active_ver} | cut -d\\ -f3- # and activate the correct version echo -n \u0026#34;Activating PostgreSQL ${wanted_ver}... \u0026#34; brew link --force --overwrite postgresql@${wanted_ver} | grep \u0026#39;created\u0026#39; | cut -d\\ -f3- fi # point to the correct data dir and port PGDATA=/usr/local/var/postgres/${wanted_ver} PGPORT=\u0026#34;54$(echo ${wanted_ver} | tr -d .)\u0026#34; # should we be starting a cluster? if [[ \u0026#34;${no_restart}\u0026#34; = \u0026#34;t\u0026#34; ]] ; then # nope, carry on : else # yup. has the cluster been initialized? if [[ ! -d ${PGDATA} ]] ; then # nope, so let\u0026#39;s do that echo \u0026#34;Initializing PostgreSQL ${wanted_ver} cluster... \u0026#34; initdb -k ${PGDATA} || initdb ${PGDATA} echo \u0026#34;port = ${PGPORT}\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_destination = \u0026#39;stderr\u0026#39;\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;logging_collector = on\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_filename = \u0026#39;postgresql-%a.log\u0026#39;\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_truncate_on_rotation = on\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_rotation_age = 1d\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_rotation_size = 0\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_timezone = \u0026#39;US/Michigan\u0026#39;\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf echo \u0026#34;log_line_prefix = \u0026#39;%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h \u0026#39;\u0026#34; \u0026gt;\u0026gt; ${PGDATA}/postgresql.conf else # yup, carry on : fi # start the cluster echo -n \u0026#34;Starting PostgreSQL ${wanted_ver}... \u0026#34; pg_ctl -D ${PGDATA} start \u0026gt; /tmp/pg.start 2\u0026gt;\u0026amp;1 grep \u0026#39;server start\u0026#39; /tmp/pg.start if [[ -x /usr/local/bin/pg_isready ]] ; then ret=1 while [[ ${ret} -eq 1 ]] do # wait for the cluster to be available before exiting pg_isready -q ret=$? done fi fi echo \u0026#34;export PGDATA=${PGDATA}\u0026#34; echo \u0026#34;export PGPORT=${PGPORT}\u0026#34; But what does it do? It\u0026rsquo;s pretty simple actually. When you call this script, you tell it what version of PostgreSQL you want:\n$ pg 9.6 and then the script does the following:\nchecks if the requested version is installed, and installs it if not checks if another version of PostgreSQL is running, and stops it checks if another version is linked as the active version, and unlinks it links the requested version as the active version sets PGDATA to point to the requested version\u0026rsquo;s data cluster does an initdb for the requested version if needed starts the requested version\u0026rsquo;s cluster I\u0026rsquo;ll be the first to admit that the script could use additional work, but it\u0026rsquo;s functional enough to start using today. As I continue to improve the script, I\u0026rsquo;ll update the\u0026quot;post\u0026quot;with those changes, so check back every so often.\nEnjoy.\n","permalink":"https://hunleyd.github.io/posts/managing-multiple-postgresql-installs-via-homebrew/","tags":["postgresql","osx","howto"],"title":"Managing multiple PostgreSQL installs via Homebrew"},{"categories":["osx","howto"],"contents":" Like most geeks, I have scripts that I\u0026rsquo;ve written that I like to have run from cron on a regular basis. And since the running of these scripts might be in the middle of the night, I like for them to email their output to me so I know if they succeeded or failed. As such, I need an MTA on my computer that can actually deliver these emails to GMail. For me, this is trivial using Sendmail or SSMTP on a Linux box, but I can never remember how to do this using Postfix on OSX. So after having to Google everything to get this running once more, I\u0026rsquo;m going to commit the steps here for my future self to reference :)\nThe first thing we\u0026rsquo;re going to do is open an iTerm and sudo -i to become root. Then we\u0026rsquo;re going to vi /etc/postfix/main.cf and we\u0026rsquo;re going to add/set the following:\nmyhostname = smtp.gmail.com relayhost = [smtp.gmail.com]:587 smtp_sasl_auth_enable = yes smtp_sasl_password_maps= hash:/etc/postfix/sasl_passwd smtp_sasl_security_options = noanonymous smtp_sasl_mechanism_filter = plain smtp_use_tls = yes I have Two-Factor Auth (2FA) enabled on my Google account (you should too!), so for me, the next step is to log into Google and create an \u0026lsquo;app password\u0026rsquo;. Once I have that password in hand, we vi /etc/postfix/sasl_passwd and add:\n[smtp.gmail.com]:587 username@gmail.com:app_passwd Secure that file by running chmod 0600 /etc/postfix/sasl_passwd and then have Postfix hash it by doing postmap /etc/postfix/sasl_passwd. Finally, restart Postfix:\nlaunchctl stop org.postfix.master launchctl start org.postfix.master And everything should work.\n","permalink":"https://hunleyd.github.io/posts/routing-email-through-gmail-on-osx/","tags":["osx","howto"],"title":"Routing email through GMail on OSX"},{"categories":["osx","postgresql","howto"],"contents":" In our previous post, we installed PostgreSQL via Homebrew and got our own little cluster up and running under our userid. That is probably good enough for 90% of the users out there who just want to play with or devel on PostgreSQL, but for those of us who need a little more flexibility in our installs, we\u0026rsquo;re going to take the next step.\nFirst, if you have any data in your existing cluster that you want to preserve, let\u0026rsquo;s backup it up. We\u0026rsquo;re just going to use pg_dumpall here since it\u0026rsquo;s quick-n-dirty:\n$ pg_dumpall -g \u0026gt; my_roles.sql $ pg_dumpall \u0026gt; my_data.sql Now, we can stop our cluster:\n$ brew services stop postgresql Stopping `postgresql`... (might take a while) ==\u0026gt; Successfully stopped `postgresql` (label: homebrew.mxcl.postgresql) And uninstall the current PostgreSQL:\n$ brew uninstall postgresql Uninstalling /usr/local/Cellar/postgresql/9.5.5... (3,154 files, 35.1M) And remove the remnants of our cluster:\n$ rm -rf /usr/local/var/postgres Now we\u0026rsquo;ve got a clean slate. So, let\u0026rsquo;s tell Homebrew to stop using the default PostgreSQL and use Peter Eisentraut\u0026rsquo;s version (if you don\u0026rsquo;t know, Peter actually contributes to PostgreSQL):\n$ brew tap petere/postgresql Updating Homebrew... ==\u0026gt; Auto-updated Homebrew! Updated 1 tap (homebrew/core). ==\u0026gt; New Formulae [snip] ==\u0026gt; Updated Formulae [snip] ==\u0026gt; Deleted Formulae [snip] ==\u0026gt; Tapping petere/postgresql Cloning into \u0026#39;/usr/local/Homebrew/Library/Taps/petere/homebrew-postgresql\u0026#39;... remote: Counting objects: 15, done. remote: Compressing objects: 100% (11/11), done. remote: Total 15 (delta 9), reused 5 (delta 4), pack-reused 0 Unpacking objects: 100% (15/15), done. Checking connectivity... done. Tapped 11 formulae (49 files, 71.7K) As of this writing, Peter makes PostgreSQL 8.3 and above available. However, only PostgreSQL 9.2 and above are officially supported by the PostgreSQL community. So let\u0026rsquo;s install the supported versions:\n$ for i in $(seq 2 6) \\ do \\ brew install postgresql-9.${i} \\ done ==\u0026gt; Installing postgresql-9.2 from petere/postgresql ==\u0026gt; Tapping homebrew/dupes Cloning into \u0026#39;/usr/local/Homebrew/Library/Taps/homebrew/homebrew-dupes\u0026#39;... remote: Counting objects: 42, done. remote: Compressing objects: 100% (42/42), done. remote: Total 42 (delta 0), reused 4 (delta 0), pack-reused 0 Unpacking objects: 100% (42/42), done. Checking connectivity... done. Tapped 38 formulae (103 files, 121.4K) ==\u0026gt; Installing dependencies for petere/postgresql/postgresql-9.2: gettext, homebrew/dupes/openldap, ossp-uuid, homebrew/dupes/tcl-tk ==\u0026gt; Installing petere/postgresql/postgresql-9.2 dependency: get ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/gettext-0.19.8.1.sierra.bot ######################################################################## 100.0% ==\u0026gt; Pouring gettext-0.19.8.1.sierra.bottle.tar.gz ==\u0026gt; Caveats This formula is keg-only, which means it was not symlinked into /usr/local. macOS provides the BSD gettext library and some software gets confused if both are in the library path. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/gettext/lib CPPFLAGS: -I/usr/local/opt/gettext/include ==\u0026gt; Summary üç∫ /usr/local/Cellar/gettext/0.19.8.1: 1,934 files, 16.9M ==\u0026gt; Installing petere/postgresql/postgresql-9.2 dependency: homebrew/dupes/ ==\u0026gt; Downloading https://homebrew.bintray.com/bottles-dupes/openldap-2.4.44.sierr ######################################################################## 100.0% ==\u0026gt; Pouring openldap-2.4.44.sierra.bottle.tar.gz ==\u0026gt; Caveats This formula is keg-only, which means it was not symlinked into /usr/local. macOS already provides this software and installing another version in parallel can cause all kinds of trouble. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/openldap/lib CPPFLAGS: -I/usr/local/opt/openldap/include ==\u0026gt; Summary üç∫ /usr/local/Cellar/openldap/2.4.44: 295 files, 6.3M ==\u0026gt; Installing petere/postgresql/postgresql-9.2 dependency: ossp-uuid ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/ossp-uuid-1.6.2_2.sierra.bo ######################################################################## 100.0% ==\u0026gt; Pouring ossp-uuid-1.6.2_2.sierra.bottle.tar.gz üç∫ /usr/local/Cellar/ossp-uuid/1.6.2_2: 17 files, 206K ==\u0026gt; Installing petere/postgresql/postgresql-9.2 dependency: homebrew/dupes/ ==\u0026gt; Using the sandbox ==\u0026gt; Downloading https://downloads.sourceforge.net/project/tcl/Tcl/8.6.6/tcl8.6.6 ==\u0026gt; Downloading from http://iweb.dl.sourceforge.net/project/tcl/Tcl/8.6.6/tcl8.6 ######################################################################## 100.0% ==\u0026gt; ./configure --prefix=/usr/local/Cellar/tcl-tk/8.6.6 --mandir=/usr/local/Cell ==\u0026gt; make ==\u0026gt; make install ==\u0026gt; make install-private-headers ==\u0026gt; Downloading https://downloads.sourceforge.net/project/tcl/Tcl/8.6.6/tk8.6.6- ==\u0026gt; Downloading from http://heanet.dl.sourceforge.net/project/tcl/Tcl/8.6.6/tk8. ######################################################################## 100.0% ==\u0026gt; ./configure --prefix=/usr/local/Cellar/tcl-tk/8.6.6 --mandir=/usr/local/Cell ==\u0026gt; make TK_LIBRARY=/usr/local/Cellar/tcl-tk/8.6.6/lib ==\u0026gt; make install ==\u0026gt; make install-private-headers ==\u0026gt; Downloading https://github.com/tcltk/tcllib/archive/tcllib_1_18.tar.gz ==\u0026gt; Downloading from https://codeload.github.com/tcltk/tcllib/tar.gz/tcllib_1_18 ######################################################################## 100.0% ==\u0026gt; ./configure --prefix=/usr/local/Cellar/tcl-tk/8.6.6 --mandir=/usr/local/Cell ==\u0026gt; make install ==\u0026gt; Caveats This formula is keg-only, which means it was not symlinked into /usr/local. Tk installs some X11 headers and OS X provides an (older) Tcl/Tk. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/tcl-tk/lib CPPFLAGS: -I/usr/local/opt/tcl-tk/include PKG_CONFIG_PATH: /usr/local/opt/tcl-tk/lib/pkgconfig ==\u0026gt; Summary üç∫ /usr/local/Cellar/tcl-tk/8.6.6: 2,847 files, 29.2M, built in 5 minutes 31 seconds ==\u0026gt; Installing petere/postgresql/postgresql-9.2 ==\u0026gt; Downloading https://ftp.postgresql.org/pub/source/v9.2.19/postgresql-9.2.19. ######################################################################## 100.0% ==\u0026gt; Patching patching file contrib/uuid-ossp/uuid-ossp.c ==\u0026gt; ./configure --prefix=/usr/local/Cellar/postgresql-9.2/9.2.19 --enable-dtrace ==\u0026gt; make install-world ==\u0026gt; Caveats To use this PostgreSQL installation, do one or more of the following: - Call all programs explicitly with /usr/local/opt/postgresql-9.2/bin/... - Add /usr/local/opt/postgresql-9.2/bin to your PATH - brew link -f postgresql-9.2 - Install the postgresql-common package To access the man pages, do one or more of the following: - Refer to them by their full path, like `man /usr/local/opt/postgresql-9.2/share/man/man1/psql.1` - Add /usr/local/opt/postgresql-9.2/share/man to your MANPATH - brew link -f postgresql-9.2 This formula is keg-only, which means it was not symlinked into /usr/local. The different provided versions of PostgreSQL conflict with each other. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/postgresql-9.2/lib CPPFLAGS: -I/usr/local/opt/postgresql-9.2/include ==\u0026gt; Summary üç∫ /usr/local/Cellar/postgresql-9.2/9.2.19: 3,120 files, 40M, built in 3 minutes 28 seconds Updating Homebrew... ==\u0026gt; Auto-updated Homebrew! Updated 1 tap (homebrew/core). ==\u0026gt; Updated Formulae [snip] ==\u0026gt; Installing postgresql-9.3 from petere/postgresql ==\u0026gt; Downloading https://ftp.postgresql.org/pub/source/v9.3.15/postgresql-9.3.15. ######################################################################## 100.0% ==\u0026gt; Patching patching file contrib/uuid-ossp/uuid-ossp.c ==\u0026gt; ./configure --prefix=/usr/local/Cellar/postgresql-9.3/9.3.15 --enable-dtrace ==\u0026gt; make install-world ==\u0026gt; Caveats To use this PostgreSQL installation, do one or more of the following: - Call all programs explicitly with /usr/local/opt/postgresql-9.3/bin/... - Add /usr/local/opt/postgresql-9.3/bin to your PATH - brew link -f postgresql-9.3 - Install the postgresql-common package To access the man pages, do one or more of the following: - Refer to them by their full path, like `man /usr/local/opt/postgresql-9.3/share/man/man1/psql.1` - Add /usr/local/opt/postgresql-9.3/share/man to your MANPATH - brew link -f postgresql-9.3 This formula is keg-only, which means it was not symlinked into /usr/local. The different provided versions of PostgreSQL conflict with each other. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/postgresql-9.3/lib CPPFLAGS: -I/usr/local/opt/postgresql-9.3/include PKG_CONFIG_PATH: /usr/local/opt/postgresql-9.3/lib/pkgconfig ==\u0026gt; Summary üç∫ /usr/local/Cellar/postgresql-9.3/9.3.15: 3,190 files, 41M, built in 3 minutes 17 seconds ==\u0026gt; Installing postgresql-9.4 from petere/postgresql ==\u0026gt; Installing dependencies for petere/postgresql/postgresql-9.4: e2fsprogs ==\u0026gt; Installing petere/postgresql/postgresql-9.4 dependency: e2fsprogs ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/e2fsprogs-1.42.13.sierra.bo ######################################################################## 100.0% ==\u0026gt; Pouring e2fsprogs-1.42.13.sierra.bottle.tar.gz ==\u0026gt; Caveats This formula is keg-only, which means it was not symlinked into /usr/local. This brew installs several commands which override macOS-provided file system commands. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/e2fsprogs/lib CPPFLAGS: -I/usr/local/opt/e2fsprogs/include PKG_CONFIG_PATH: /usr/local/opt/e2fsprogs/lib/pkgconfig ==\u0026gt; Summary üç∫ /usr/local/Cellar/e2fsprogs/1.42.13: 135 files, 5.0M ==\u0026gt; Installing petere/postgresql/postgresql-9.4 ==\u0026gt; Downloading https://ftp.postgresql.org/pub/source/v9.4.10/postgresql-9.4.10. ######################################################################## 100.0% ==\u0026gt; ./configure --prefix=/usr/local/Cellar/postgresql-9.4/9.4.10 --enable-dtrace ==\u0026gt; make install-world ==\u0026gt; Caveats To use this PostgreSQL installation, do one or more of the following: - Call all programs explicitly with /usr/local/opt/postgresql-9.4/bin/... - Add /usr/local/opt/postgresql-9.4/bin to your PATH - brew link -f postgresql-9.4 - Install the postgresql-common package To access the man pages, do one or more of the following: - Refer to them by their full path, like `man /usr/local/opt/postgresql-9.4/share/man/man1/psql.1` - Add /usr/local/opt/postgresql-9.4/share/man to your MANPATH - brew link -f postgresql-9.4 This formula is keg-only, which means it was not symlinked into /usr/local. The different provided versions of PostgreSQL conflict with each other. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/postgresql-9.4/lib CPPFLAGS: -I/usr/local/opt/postgresql-9.4/include PKG_CONFIG_PATH: /usr/local/opt/postgresql-9.4/lib/pkgconfig ==\u0026gt; Summary üç∫ /usr/local/Cellar/postgresql-9.4/9.4.10: 3,261 files, 42.9M, built in 3 minutes 17 seconds ==\u0026gt; Installing postgresql-9.5 from petere/postgresql ==\u0026gt; Downloading https://ftp.postgresql.org/pub/source/v9.5.5/postgresql-9.5.5.ta ######################################################################## 100.0% ==\u0026gt; ./configure --prefix=/usr/local/Cellar/postgresql-9.5/9.5.5 --enable-dtrace ==\u0026gt; make install-world ==\u0026gt; Caveats To use this PostgreSQL installation, do one or more of the following: - Call all programs explicitly with /usr/local/opt/postgresql-9.5/bin/... - Add /usr/local/opt/postgresql-9.5/bin to your PATH - brew link -f postgresql-9.5 - Install the postgresql-common package To access the man pages, do one or more of the following: - Refer to them by their full path, like `man /usr/local/opt/postgresql-9.5/share/man/man1/psql.1` - Add /usr/local/opt/postgresql-9.5/share/man to your MANPATH - brew link -f postgresql-9.5 This formula is keg-only, which means it was not symlinked into /usr/local. The different provided versions of PostgreSQL conflict with each other. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/postgresql-9.5/lib CPPFLAGS: -I/usr/local/opt/postgresql-9.5/include PKG_CONFIG_PATH: /usr/local/opt/postgresql-9.5/lib/pkgconfig ==\u0026gt; Summary üç∫ /usr/local/Cellar/postgresql-9.5/9.5.5: 3,395 files, 45M, built in 3 minutes 30 seconds ==\u0026gt; Installing postgresql-9.6 from petere/postgresql ==\u0026gt; Downloading https://ftp.postgresql.org/pub/source/v9.6.1/postgresql-9.6.1.ta ######################################################################## 100.0% ==\u0026gt; ./configure --prefix=/usr/local/Cellar/postgresql-9.6/9.6.1 --enable-dtrace ==\u0026gt; make install-world ==\u0026gt; Caveats To use this PostgreSQL installation, do one or more of the following: - Call all programs explicitly with /usr/local/opt/postgresql-9.6/bin/... - Add /usr/local/opt/postgresql-9.6/bin to your PATH - brew link -f postgresql-9.6 - Install the postgresql-common package To access the man pages, do one or more of the following: - Refer to them by their full path, like `man /usr/local/opt/postgresql-9.6/share/man/man1/psql.1` - Add /usr/local/opt/postgresql-9.6/share/man to your MANPATH - brew link -f postgresql-9.6 This formula is keg-only, which means it was not symlinked into /usr/local. The different provided versions of PostgreSQL conflict with each other. Generally there are no consequences of this for you. If you build your own software and it requires this formula, you will need to add to your build variables: LDFLAGS: -L/usr/local/opt/postgresql-9.6/lib CPPFLAGS: -I/usr/local/opt/postgresql-9.6/include PKG_CONFIG_PATH: /usr/local/opt/postgresql-9.6/lib/pkgconfig ==\u0026gt; Summary üç∫ /usr/local/Cellar/postgresql-9.6/9.6.1: 3,485 files, 46.6M, built in 3 minutes 38 seconds As it says in the output, these are \u0026lsquo;keg-only\u0026rsquo;, which has the nice side effect that they are automatically installed in side-by-side directories /usr/local/opt/postgresql-9.1/ etc.\nPeter configures PostgreSQL with just about all the options turned on:\n$ pg_config |grep ^CONF CONFIGURE = \u0026#39;--prefix=/usr/local/Cellar/postgresql-9.6/9.6.1\u0026#39; \u0026#39;--enable-dtrace\u0026#39; \u0026#39;--enable-nls\u0026#39; \u0026#39;--with-bonjour\u0026#39; \u0026#39;--with-gssapi\u0026#39; \u0026#39;--with-ldap\u0026#39; \u0026#39;--with-libxml\u0026#39; \u0026#39;--with-libxslt\u0026#39; \u0026#39;--with-openssl\u0026#39; \u0026#39;--with-uuid=e2fs\u0026#39; \u0026#39;--with-pam\u0026#39; \u0026#39;--with-perl\u0026#39; \u0026#39;--with-python\u0026#39; \u0026#39;--with-tcl\u0026#39; \u0026#39;--with-includes=/usr/local/opt/gettext/include:/usr/local/opt/openldap/include:/usr/local/opt/openssl/include:/usr/local/opt/readline/include:/usr/local/opt/tcl-tk/include\u0026#39; \u0026#39;--with-libraries=/usr/local/opt/gettext/lib:/usr/local/opt/openldap/lib:/usr/local/opt/openssl/lib:/usr/local/opt/readline/lib:/usr/local/opt/tcl-tk/lib\u0026#39; \u0026#39;CC=clang\u0026#39; and you get all the extensions that are in \u0026lsquo;contrib\u0026rsquo; as well. However, if you find that you need to install another extension, I advocate using pex. You would install it like this:\n$ brew install pex ==\u0026gt; Installing dependencies for pex: postgresql ==\u0026gt; Installing pex dependency: postgresql ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/postgresql-9.5.5.sierra.bot Already downloaded: /Users/doug/Library/Caches/Homebrew/postgresql-9.5.5.sierra.bottle.tar.gz ==\u0026gt; Pouring postgresql-9.5.5.sierra.bottle.tar.gz ==\u0026gt; Using the sandbox ==\u0026gt; /usr/local/Cellar/postgresql/9.5.5/bin/initdb /usr/local/var/postgres ==\u0026gt; Caveats If builds of PostgreSQL 9 are failing and you have version 8.x installed, you may need to remove the previous version first. See: https://github.com/Homebrew/homebrew/issues/2510 To migrate existing data from a previous major version (pre-9.0) of PostgreSQL, see: https://www.postgresql.org/docs/9.5/static/upgrading.html To migrate existing data from a previous minor version (9.0-9.4) of PostgreSQL, see: https://www.postgresql.org/docs/9.5/static/pgupgrade.html You will need your previous PostgreSQL installation from brew to perform `pg_upgrade`. Do not run `brew cleanup postgresql` until you have performed the migration. 1 To have launchd start postgresql now and restart at login: brew services start postgresql Or, if you do not want/need a background service you can just run: pg_ctl -D /usr/local/var/postgres start ==\u0026gt; Summary üç∫ /usr/local/Cellar/postgresql/9.5.5: 3,154 files, 35.1M ==\u0026gt; Installing pex ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/pex-1.20140409.sierra.bottl ######################################################################## 100.0% ==\u0026gt; Pouring pex-1.20140409.sierra.bottle.tar.gz ==\u0026gt; Caveats If installing for the first time, perform the following in order to setup the necessary directory structure: pex init ==\u0026gt; Summary üç∫ /usr/local/Cellar/pex/1.20140409: 5 files, 31.6K $ $ pex init Sadly, it will reinstall the default PostgreSQL brew recipe, but we\u0026rsquo;ll simply pretend that it\u0026rsquo;s not there going forward. Once installed, use pex like so:\n$ pex -g /usr/local/opt/postgresql-9.4 install ip4r ==\u0026gt; Downloading ip4r from https://github.com/RhodiumToad/ip4r/archive/2.0.3.tar.gz ######################################################################## 100.0% ==\u0026gt; Unpacking ip4r [snip] ==\u0026gt; Building ip4r [snip] ==\u0026gt; Installing ip4r [snip] /usr/bin/install -c -m 755 ip4r.so \u0026#39;/usr/local/Cellar/postgresql-9.4/9.4.10/lib/ip4r.so\u0026#39; /usr/bin/install -c -m 644 ip4r.control \u0026#39;/usr/local/Cellar/postgresql-9.4/9.4.10/share/extension/\u0026#39; /usr/bin/install -c -m 644 ip4r--2.0.sql ip4r--unpackaged2.0--2.0.sql ip4r--unpackaged1--2.0.sql ‚Ü™\\ \u0026#39;/usr/local/Cellar/postgresql-9.4/9.4.10/share/extension/\u0026#39; [snip] ==\u0026gt; Package ip4r installed successfully As you can see, the ip4r extension was installed into my PostgreSQL 9.4 instance.\nNow, you might be wondering how to deal with all these concurrent versions. If you type pg_dump which one will you get? How can you force it to use a specific version? Right now, you won\u0026rsquo;t get any version, since it\u0026rsquo;s not in your path. You will need to specify exactly what you want:\n$ /usr/local/opt/postgresql-9.6/bin/psql --version psql (PostgreSQL) 9.6.1 which is a huge pita. So, let\u0026rsquo;s make a little convenience function in our shell. Edit your .zshrc or your .bashrc (or whatever) and add a function like this:\nfunction pg() { ver=$1 for i in /usr/local/var/postgres/* do running_ver=$(basename ${i}) is_running=$(ps -few|egrep -- \u0026#34;[p]ostgres.*-D.*${running_ver}\u0026#34;) if [[ ! -z ${is_running} ]] ; then echo -n \u0026#34;Stopping PostgreSQL ${running_ver}... \u0026#34; /usr/local/opt/postgresql-${running_ver}/bin/pg_ctl \\ -D $i stop -mf \u0026amp;\u0026gt;/dev/null echo \u0026#34;done!\u0026#34; fi done echo -n \u0026#34;Activating PostgreSQL ${ver}... \u0026#34; brew unlink --force --overwrite postgresql-${ver} \u0026amp;\u0026gt;/dev/null brew link --force --overwrite postgresql-${ver} \u0026amp;\u0026gt;/dev/null echo \u0026#34;done!\u0026#34; export PGDATA=/usr/local/var/postgres/${ver} if [[ ! -d ${PGDATA} ]] ; then echo -n \u0026#34;Initializing PostgreSQL ${ver} cluster... \u0026#34; mkdir ${PGDATA} initdb -k ${PGDATA} \u0026amp;\u0026gt;/dev/null || initdb ${PGDATA} \u0026amp;\u0026gt;/dev/null echo \u0026#34;done!\u0026#34; fi echo -n \u0026#34;Starting PostgreSQL ${ver}... \u0026#34; pg_ctl -D ${PGDATA} start \u0026amp;\u0026gt;/dev/null echo \u0026#34;done!\u0026#34; } and then simply call pg 9.5 to set PostgreSQL 9.5 as your \u0026lsquo;active\u0026rsquo; instance:\n$ pg 9.5 Stopping PostgreSQL 9.4... done! Activating PostgreSQL 9.5... done! Initializing PostgreSQL 9.5 cluster... done! Starting PostgreSQL 9.5... done! Restore your data:\n$ psql -d template1 -f my_roles.sql $ psql -d template1 -f my_data.sql and there you be. Ain\u0026rsquo;t it beautiful? :)\n","permalink":"https://hunleyd.github.io/posts/getting-fancy-with-postgresql-and-homebrew/","tags":["osx","postgresql","howto"],"title":"Getting fancy with PostgreSQL and Homebrew"},{"categories":["osx","postgresql","howto"],"contents":" If you need to install PostgreSQL onto your Macbook, you have several options available to you nowadays. You could use the BigSQL package, or you could use Postgres.app, or several others. However, if you\u0026rsquo;re a geek running OSX, you\u0026rsquo;ve probably already installed Homebrew and it has a wonderful PostgreSQL package. So let\u0026rsquo;s use it, shall we?\nI\u0026rsquo;m not going to walk you through installing Homebrew, so let\u0026rsquo;s just assume it\u0026rsquo;s already up and running and you\u0026rsquo;ve followed all the directions. At this point, you have /usr/local/bin/ in your $PATH and brew is up and running. So, let\u0026rsquo;s tell brew to install PostgreSQL:\n$ brew install postgresql ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/postgresql-9.5.4_1.sierra.b Already downloaded: /Users/doug/Library/Caches/Homebrew/postgresql-9.5.4_1.sierra.bottle.tar.gz ==\u0026gt; Pouring postgresql-9.5.4_1.sierra.bottle.tar.gz ==\u0026gt; Using the sandbox ==\u0026gt; /usr/local/Cellar/postgresql/9.5.4_1/bigsqln/initdb /usr/local/var/postgres ==\u0026gt; Caveats If builds of PostgreSQL 9 are failing and you have version 8.x installed, you may need to remove the previous version first. See: https://github.com/Homebrew/homebrew/issues/2510 To migrate existing data from a previous major version (pre-9.0) of PostgreSQL, see: https://www.postgresql.org/docs/9.5/static/upgrading.html To migrate existing data from a previous minor versionn (9.0-9.4) of PostgreSQL, see: https://www.postgresql.org/docs/9.5/static/pgupgrade.html You will need your previous PostgreSQL installation from brew to perform `pg_upgrade`. Do not run `brew cleanup postgresql` until you have performed the migration. To have launchd start postgresql now and restart at login: brew services start postgresql Or, if you don\u0026#39;t want/need a background service you can just run: pg_ctl -D /usr/local/var/postgres start ==\u0026gt; Summary üç∫ /usr/local/Cellar/postgresql/9.5.4_1: 3,147 files, 35M As you can see, it downloaded the package, installed the binaries, and ran initdb for us! As the output tells us, we can set PostgreSQL to auto-start when we login by issuing:\n$ brew services start postgresql ==\u0026gt; Successfully started `postgresql` (label: homebrew.mxcl.postgresql) And if you check your process listing, you can see that the cluster is up and running under your id:\n$ ps -efw|grep postgres 501 6808 1 0 11:03AM ?? 0:00.02 /usr/local/opt/postgresql/bin/postgres -D /usr/local/var/postgres 501 6817 6808 0 11:03AM ?? 0:00.00 postgres: logger process 501 6819 6808 0 11:03AM ?? 0:00.00 postgres: checkpointer process 501 6820 6808 0 11:03AM ?? 0:00.00 postgres: writer process 501 6821 6808 0 11:03AM ?? 0:00.00 postgres: wal writer process 501 6822 6808 0 11:03AM ?? 0:00.00 postgres: autovacuum launcher process 501 6823 6808 0 11:03AM ?? 0:00.00 postgres: stats collector process And just like that, you have PostgreSQL installed and running! Set $PGDATA to /usr/local/var/postgres and you\u0026rsquo;re all set.\nEnjoy!\n","permalink":"https://hunleyd.github.io/posts/postgresql-homebrew-and-you/","tags":["osx","postgresql","howto"],"title":"PostgreSQL, Homebrew, and You"},{"categories":["pgcmh","postgresql"],"contents":" As announced on our Twitter, we\u0026rsquo;ve scheduled our inaugural meeting for Jan 24, 2017 at 1800 hrs. The folks at CoverMyMeds will graciously provide the meeting space (and parking garage) while OpenSCG is buying the pizza!\nAt this first meeting, we\u0026rsquo;ll be discussing what you, the members, would like to get from the meetings, we\u0026rsquo;ll talk about future topics, and general organizational things. I know it\u0026rsquo;s not exciting, but I need everyone\u0026rsquo;s input to make this group something you get value from.\nOur first mtg is happening on Jan 24 at @covermymeds with @openscg buying pizza. Bring a friend! RSVP at https://t.co/600ZNbPV8e\n\u0026mdash; pgCMH (@pgCMH) October 19, 2016 Please RSVP via the MeetUP event so we have sufficient food and drink!\nSee you there!\n","permalink":"https://hunleyd.github.io/posts/inaugural-pgcmh-mtg/","tags":["pgcmh","postgresql"],"title":"Inaugural pgCMH mtg scheduled"},{"categories":["android","review"],"contents":"I\u0026rsquo;m a fan of Android TV. While I didn\u0026rsquo;t blog about it, I ran out and bought a Nexus Player on day 1. And I loved it! OK, I loved it more after the upgrade to Android 6.x, but still.\nWell, now I\u0026rsquo;ve upgraded from the Nexus Player to the Mi Box. It\u0026rsquo;s a bit of a risk, since no one knows the upgrade history for Xiaomi, but I\u0026rsquo;m OK with that. Especially at a mere $70 for this thing. As much as I loved the Player, I am enjoying this device so much more. It\u0026rsquo;s stupid fast, the remote is so much better, and I feel like I get a better picture out of it (which is silly, I don\u0026rsquo;t have a 4k TV so it should be the same 1080p HD picture I got from the Player).\nThis tiny little box (101 x 101 x 19.5mm) comes with an Amlogic Quad-core Cortex-A53 processor running at 2.0GHz, a Mali 450 GPU running at 750MHz, 2GB of ram, and 8GB of storage. On the back is an HDMI port, a headphone jack (why?), a power adaptor port (sadly, not a standard USB-C or MicroUSB), a single USB 2.0 port. It\u0026rsquo;s pretty sparse port wise, but really that\u0026rsquo;s all you need. There\u0026rsquo;s a subtly white LED on the very front of the device to show you that it\u0026rsquo;s powered on too. Otherwise, it\u0026rsquo;s just a sleek little black hockey puck sitting in my entertainment center.\nOverall, I\u0026rsquo;m so pleased with this little thing that I convinced Stephanie to buy one, gave my Player to Tina, and I\u0026rsquo;m considering buying one of these for Emily when she moves out.\nReview: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ\n","permalink":"https://hunleyd.github.io/posts/mi-box/","tags":["android","review"],"title":"Mi Box"},{"categories":["pgcmh","postgresql"],"contents":" I\u0026rsquo;ve been kicking around the idea of founding a Columbus-based PostgreSQL User Group for a while now. I even went so far as to float the idea to people at OLF in \u0026lsquo;14. After much hemming and hawing (and no one else stepping up in the interim), I\u0026rsquo;ve finally gone and done it.\npgCMH is the name of my newly formed group, and we\u0026rsquo;re good to go. We\u0026rsquo;ve got our own Twitter (@pgCMH):\n#PostgreSQL 9.6.1, 9.5.5, 9.4.10, 9.3.15, 9.2.19 and 9.1.24 Released! https://t.co/RH27To33gh\n\u0026mdash; pgCMH (@pgCMH) October 27, 2016 as well as our own MeetUp page. We\u0026rsquo;ve got a sponsor providing food, and another providing the meeting location. Our first meeting will be in Jan, thanks to all the scheduling conflicts the upcoming holidays create.\nWatch this space for updates, follow our Twitter, and join the mailing list on MeetUp. I\u0026rsquo;d love to get your participation and input. Let\u0026rsquo;s make this group as wildly successful as we can!\n","permalink":"https://hunleyd.github.io/posts/announcing-pgcmh/","tags":["pgcmh","postgresql"],"title":"Announcing pgCMH"},{"categories":["postgresql","howto"],"contents":" You\u0026rsquo;re probably already running pgBadger to monitor your PostgreSQL logs. However, you\u0026rsquo;re probably not running it incrementally throughout the day. Most likely, you\u0026rsquo;ve setup a cron.daily job that runs pgBadger against yesterday\u0026rsquo;s log(s). And that\u0026rsquo;s great. Except when you get the dreaded \u0026ldquo;what just happened on the db?\u0026rdquo; email. Are you going to wait until tonight\u0026rsquo;s normal run of pgBadger to see what happened? Are you going to run a \u0026lsquo;one off\u0026rsquo; pgBadger against today\u0026rsquo;s logfile and wait for it to process the entire log? Or are you going to copy the log off somewhere, edit it to cut it down, and then run pgBadger against this cut-down version (hoping you left enough in the log to see proper trending)?\nNo, most likely you\u0026rsquo;re going to look at your actual monitoring tool that does real-time monitoring of your db and try to figure things out from there. You are running some kind of db monitoring tool, right?\nHowever, let\u0026rsquo;s say that for, uh, reasons, you only have pgBadger at your disposal right this instant. Well, if you were making use of pgBadger\u0026rsquo;s incremental mode you could simply fire off the next scheduled run and it would only process those log entries that were new since the last run. So, for example, if you had a cron.hourly run of pgBadger it would only process the last hour\u0026rsquo;s worth of entries to update today\u0026rsquo;s report. No waiting to process multiple hours of info that you don\u0026rsquo;t need, no editing of the logfile to remove things outside the window you care about, just run it and done.\nSounds nice, right? So let\u0026rsquo;s set this up shall we? I\u0026rsquo;m assuming you\u0026rsquo;ve already setup postgresql.conf appropriately, but if you haven\u0026rsquo;t please go that first. The pgBadger website has good documentation on how to do so. According to the docs:\n-I | --incremental : use incremental mode, reports will be generated by days in a separate directory, --outdir must be set. is how we turn on incremental mode. You\u0026rsquo;ll note that we also need to specify an output dir:\n-O | --outdir path : directory where out file must be saved I usually stick the pgBadger output into the pg_log directory. In my mind, having the logs and the report on the logs next to each makes sense, but feel free to stick yours wherever.\nFinally, we probably don\u0026rsquo;t need pgBadger reports that are too old, and the docs say we can cull the cruft automatically:\n-R | --retention N : number of week to keep in incremental mode. Default to 0, disabled. Used to set the number of weel to keep in output directory. Older weeks and days directory are automatically removed. (Ignore the typo, it\u0026rsquo;s that way in the code)\nOn my servers, I have PostgreSQL setup to log into a different file for each day of the week, with automatic rotation and truncation:\nlog_filename = \u0026#39;postgresql-%a.log\u0026#39; log_truncate_on_rotation = on log_rotation_age = 1d so my cron.hourly pgBadger looks like:\npgbadger \\ -I \\ -O $PGDATA/pg_log \\ -R 12 \\ -q \\ $PGDATA/pg_log/postgresql-$(date --date yesterday +%a) \\ $PGDATA/pg_log/postgresql-$(date +%a) which as you can see always feeds both yesterday\u0026rsquo;s and today\u0026rsquo;s log into pgBadger (since the cron runs at 2300 and then again at 0000, we need yesterday\u0026rsquo;s log to catch that last hour). Since we\u0026rsquo;re running in incremental mode, it knows at every run where it left off in the files the last time and does a seek to skip over that data. This cuts the run time down significantly even with the PostgreSQL logging cranked up. You can see it here:\n... DEBUG: Starting reading file postgresql-Wed.log... DEBUG: Start parsing at offset 412677131 of file postgresql-Wed.log to 433543395 [======================\u0026gt; ] Parsed 413815537 bytes of 433543395 (95.45%), queries ... As you can see, it jumps right in at 95% of the file and only processes the newest 5%. In fact, this takes a mere 20 seconds:\nstatistics gathering took:20 wallclock secs (19.44 usr + 0.17 sys = 19.61 CPU) on my overloaded Macbook!\nSo there you have it. Not counting the time it takes you to ssh to your server, it would have taken all of 20 seconds to have an updated report of what just happened on your database!\nKeep in mind, this is also with a single thread. pgBadger has the ability to run multi-threaded. See the --help for details.\nEnjoy!\n","permalink":"https://hunleyd.github.io/posts/incremental-pgbadger/","tags":["postgresql","howto"],"title":"Incremental pgBadger"},{"categories":["work","postgresql"],"contents":"Someone at work thought it would be a good idea to give me access to the corporate blog so that I might post PostgreSQL-related things there and have them syndicated to Planet PostgreSQL. So my PostgreSQL ramblings will show up there now instead of here. This should be fun!\n","permalink":"https://hunleyd.github.io/posts/im-syndicated/","tags":["work","postgresql"],"title":"I'm syndicated"},{"categories":["kickstarter","biking"],"contents":"My Linka smart bike lock from Kickstarter showed up today. Holy Hell is this thing solid and hefty! Like for reals, this is gonna throw the balance of the bike off :)\nI know what you\u0026rsquo;re thinking. A smart bike lock? Why? Yeah, I know. However, even without the smart gimmicks, this is a nice self-contained bike lock. It mounts directly to the frame, is hella hefty and thick (1.6 pounds), has a built in \u0026lsquo;someone is fucking with me\u0026rsquo; alarm (that is quite ear piercing), it\u0026rsquo;s made of through-hardened steel, has a square cross-section, and it doesn\u0026rsquo;t stick out like a sore thumb when mounted.\nAnyway, I\u0026rsquo;ll edit this post later with thoughts after use.\n","permalink":"https://hunleyd.github.io/posts/linka/","tags":["kickstarter","biking"],"title":"Linka"},{"categories":["general","gaming"],"contents":" I recently discovered the online game NationStates thanks to a Reddit thread and was intrigued. So I logged in, created the nation of Cypheri and then used BlogTrottr to feed the RSS feed into my inbox (which allows me offline play essentially. I ignore it until I get a notice). So far, it\u0026rsquo;s been a neat little experience. I\u0026rsquo;m pretty sure if you know me well enough to be reading this blog that I know you well enough to say you\u0026rsquo;ll enjoy it too.\nWithout further ado, let me welcome you to the Republic of Cypheri, a left-leaning college state. Our motto is \u0026ldquo;strength through freedom\u0026rdquo;.\nThe Republic of Cypheri is a tiny, genial nation, renowned for its compulsory military service and irreverence towards religion. The compassionate population of 7 million Cypherians enjoy extensive civil freedoms, particularly in social issues, while business tends to be more regulated.\nThe large government prioritizes Education, although Welfare, Defense, and Law \u0026amp; Order are also considered important, while Spirituality and Industry are ignored. The average income tax rate is 39.8%, but much higher for the wealthy.\nThe Cypherian economy, worth 251 billion dollars a year, is fairly diversified and led by the Tourism industry, with major contributions from Automobile Manufacturing, Basket Weaving, and Retail. State-owned companies are common. Average income is 35,928 dollars, and evenly distributed, with the richest citizens earning only 2.0 times as much as the poorest.\nThanks for visiting, we hope you enjoyed your time here!\n","permalink":"https://hunleyd.github.io/posts/welcome-to-cypheri/","tags":["general","gaming"],"title":"Welcome to Cypheri"},{"categories":["general"],"contents":"After an afternoon dicking around with Jekyll templates and Google searching, I\u0026rsquo;ve fixed up this site\u0026rsquo;s archive pages and tag cloud pages. I now have functional archive pages for everything, each year, each month of said year, and each day of said month. And they should all continue to function perfectly well on their own as time progresses (unless I bork something up later trying to be clever). I\u0026rsquo;ve also modified the tag cloud page to fit in the archive pages\u0026rsquo; look an feel. It\u0026rsquo;s not a true tag cloud in that I don\u0026rsquo;t increase the weight or height of the font based on frequency, but the cloud does contain every tag used across the blog and each tag has it\u0026rsquo;s own page showing all posts tagged as such.\nSo, yeah. Enjoy, or something.\n","permalink":"https://hunleyd.github.io/posts/functional-blog-archives/","tags":["general"],"title":"Functional Blog Archives"},{"categories":["postgresql"],"contents":"From the PostgreSQL docs:\nTablespaces in PostgreSQL allow database administrators to define locations in the file system where the files representing database objects can be stored. Once created, a tablespace can be referred to by name when creating database objects.\nBy using tablespaces, an administrator can control the disk layout of a PostgreSQL installation. This is useful in at least two ways. First, if the partition or volume on which the cluster was initialized runs out of space and cannot be extended, a tablespace can be created on a different partition and used until the system can be reconfigured.\nSecond, tablespaces allow an administrator to use knowledge of the usage pattern of database objects to optimize performance. For example, an index which is very heavily used can be placed on a very fast, highly available disk, such as an expensive solid state device. At the same time a table storing archived data which is rarely used or not performance critical could be stored on a less expensive, slower disk system.\nAs you can see, while not as powerful as tablespaces in, say, Oracle, they do still have their uses in PostgreSQL. You can use them to make use of different filesystems, or different mount options, or different disk types and, in doing so, intelligently apply performance characteristics to subsets of your data. For example, you could put your highest volume tables in a tablespace that is mounted from SSDs while the rest of your db is mounted from spinning rust.\nSounds decent, right? Now you before you go off and be \u0026ldquo;clever\u0026rdquo; and create an SSD-backed mountpoint for your new tablespace, understand that there are places you should not create the tablespace. You shouldn\u0026rsquo;t create tablespaces on any kind of ephemeral storage, for example on a tmpfs or a ramfs or similar. You also should not create your new tablespaces under $PGDATA. Yes, I\u0026rsquo;m aware there is $PGDATA/pg_tblspc but that directory is not for you. The system will auto-populate that directory with pointers to the real location of your tablespaces!\nSo what happens when you create a tablespace inside $PGDATA? Let\u0026rsquo;s find out. First, we\u0026rsquo;ll create the directory for the tablespace:\n$ mkdir $PGDATA/tablespaces $ cd $PGDATA/tablespaces $ pwd /Users/doug.hunley/pgdata/tablespaces And we see that nothing bad has happened yet. So, let\u0026rsquo;s pop over into psql and actually create the tablespace:\n\u0026gt; CREATE TABLESPACE ts1 LOCATION \u0026#39;/Users/doug.hunley/pgdata/tablespaces\u0026#39;; WARNING: 42P17: tablespace location should not be inside the data directory LOCATION: CreateTableSpace, tablespace.c:295 CREATE TABLESPACE Time: 7.797 ms We get a warning (not an error, for some reason) but it works and all appears fine. Now you can spend minutes/days/months/years using your new tablespace and never notice that you\u0026rsquo;ve got a problem. So where does the problem come in?\nLet\u0026rsquo;s try to make a backup of our cluster:\n$ pg_basebackup -D pgdata2 -Fp -R -Xs -c fast -l \u0026#39;clone for slave\u0026#39; -P -v transaction log start point: 2/17000028 on timeline 1 pg_basebackup: directory \u0026#34;/Users/doug.hunley/pgdata/tablespaces\u0026#34; exists but is not empty There it is.\nWhen creating the backup, it tries to ensure the tablespace location is the same, but then it won\u0026rsquo;t write to a non-empty directory. My example is two different $PGDATA locations on the same box, but the issue is the same when using different machines because pg_basebackup backs up everything in $PGDATA which means your tablespace directory gets cloned before it gets to the actual cloning of the data in the tablespace so you end up with \u0026ldquo;stuff\u0026rdquo; in the dir, making it non-empty. Which gives you the same error and output.\nOK, so it breaks backups. I can work around that by using another backup method. What else?\nHow about using pg_upgrade to do an upgrade? No matter if you run in link mode or not, pg_upgrade will not move your tablespace location. So you may have ~/pgdata95 and ~/pgdata96 after the upgrade, but your tablespaces are still in ~/pgdata95/tablespaces. So, as per the docs:\nOnce you are satisfied with the upgrade, you can delete the old cluster\u0026rsquo;s data directories by running the script mentioned when pg_upgrade completes.\nAnd boom you\u0026rsquo;ve just deleted your tablespaces off disk. Congratulations!\nSo there you have it. Two very good reasons to not create tablespaces inside $PGDATA. Please, don\u0026rsquo;t do this. Everyone who admins that cluster going forward will thank you.\n","permalink":"https://hunleyd.github.io/posts/where-not-to-put-your-tablespaces/","tags":["postgresql"],"title":"Where Not To Put Your Tablespaces"},{"categories":["general","blog","github"],"contents":" After stumbling upon an example of how to do so, I\u0026rsquo;ve converted all my existing blog posts to use GitHub Gists instead of inline code blocks. It gives me added functionality, and reduces the size of the blog pages as well. Might as well use every GitHub feature I can since I\u0026rsquo;ve chosen that platform for hosting my blog, right? :)\n","permalink":"https://hunleyd.github.io/posts/making-use-of-gists/","tags":["general","blog","github"],"title":"Making use of gists"},{"categories":["postgresql"],"contents":" Just a quick shout out to PostgreSQL, the world\u0026rsquo;s most advanced open source database, as it celebrates it\u0026rsquo;s 20th year in existence. Happy birthday, y\u0026rsquo;all!\n","permalink":"https://hunleyd.github.io/posts/happy-20th-postgresql/","tags":["postgresql"],"title":"Happy 20th Birthday, PostgreSQL"},{"categories":["android","nexus5x","protip"],"contents":" As mentioned in my previous post I\u0026rsquo;ve stumbled across two issues with the 2nd Developer Preview of Android N. The latter of these issues is discussed here. For reasons I won\u0026rsquo;t get into, I had need to wipe and reset my Huawei Watch, and after doing so I attempted to setup Smart Lock such that when the watch was connected, the phone would stay unlocked.\nI was immediately aware of some kind of issue since the phone didn\u0026rsquo;t prompt me to setup Smaet Lock after pairing the watch. So I went into the Android Settings app, then Security, and went to tap on Smart Lock only to find it gone. Thinking maybe Google had moved it somewherre else, I used the new \u0026lsquo;search Settings\u0026rsquo; feature and searched for Smart Lock, Lock, and Smart. None of them tuened up what I was looking for! After some Googling, I had enough clues to figure out what had happened to the option and how to restore it.\nTo restore Smart Lock to the Security tab of the Settings app, do this:\nopen the Android Settings app tap Security scroll down to the Advanced section tap Trust Agents toggle Smart Lock (Google) Now, when you back button to the Security settings, the Smart Lock option will be restored and function like before. I dont know wtf \u0026lsquo;unset\u0026rsquo; it as a Trust Agent provider in DP2 or why, but there you go.\n","permalink":"https://hunleyd.github.io/posts/smart-lock-on-android-n-dp2/","tags":["android","nexus5x","protip"],"title":"Smart Lock on Android N DP2"},{"categories":["android","nexus5x","protip"],"contents":"Like any good geek, I jumped at the opportunity to enroll my Nexus 5x into the Android N Beta Program. And for the most part, it\u0026rsquo;s worked really well for me. I really like Android N so far and can\u0026rsquo;t wait for the final release. However, after updating from Developer Preview 1 (DP1) to Developer Preview 2 (DP2) via OTA, I noticed two small issues. The first issue is that I could not, for the life of me, get Now On Tap to enable even though it was previously enabled on DP1.\nI\u0026rsquo;m happy to report that I figured out, and I thought I\u0026rsquo;d share it in case anyone else has run this issue and was stumped. Just to be clear, if you open the Google app, then go into Settings, and then into Now on Tap, there\u0026rsquo;s a toggle. Simply flipping it is supposed to be enough to turn it on. But in my case, the toggle would not flip on. It was permanently flipped to off.\nTo fix this:\nhead into the Android Settings app touch Apps touch the gear icon on the top right corner touch Assist \u0026amp; voice input touch Assist App chose Google Once you\u0026rsquo;ve done this, you\u0026rsquo;ll find that you can enable Now on Tap through the normal toggle.\n","permalink":"https://hunleyd.github.io/posts/now-on-tap-android-n-dp2/","tags":["android","nexus5x","protip"],"title":"Now on Tap on Android N DP2"},{"categories":["postgresql","howto"],"contents":" Partitioning in PostgreSQL can be a little daunting at times. In fact, you should probably just use pg_partman and be done with it. However, if you\u0026rsquo;re trying to learn, can\u0026rsquo;t use pg_partman, or are a masochist you\u0026rsquo;ll probably be following the docs and thinking \u0026lsquo;seriously? i have to create indexes on each child? why don\u0026rsquo;t they copy the indexes of the parent? why isn\u0026rsquo;t this easier?\u0026rsquo;. Here\u0026rsquo;s a little tip to make things slightly easier:\nInstead of creating your child tables like the docs say:\n\u0026gt; CREATE TABLE child1 ( CHECK (blah blah) ) INHERITS (parent); Create your child tables thusly:\n\u0026gt; CREATE TABLE child1 ( LIKE parent INCLUDING ALL, CHECK (blah blah) ) INHERITS (parent); and PostgeSQL will copy all your indexes, primary keys, etc from the parent to the child. Which is what you wanted, right?\nEnjoy.\n","permalink":"https://hunleyd.github.io/posts/postgresql-partitioning-quick-tip/","tags":["postgresql","howto"],"title":"PostgreSQL Partitioning Quick Tip"},{"categories":["postgresql","replication","howto"],"contents":" UPDATE: My coworker Richard liked this write up, and Skytools, so much he threw together a demo script. You can get it here.\nI recently had to do a near-zero downtime upgrade from PostgreSQL 8.4.x to PostgreSQL 9.4.x for a custmer. I couldn\u0026rsquo;t use streaming replication because of the change in major version (and because it\u0026rsquo;s simply not present in 8.x), so that left me looking at logical replication options. Usually, everyone else would be thinking Slony right here. I\u0026rsquo;ve only messed with Slony a few times, but each time was a pita, and the whole thing just seemed overly complicated to me. So I decided to give Londiste a look.\nLondiste is part of the Skytools suite, originally developed by Skype back when they were a \u0026rsquo;no central node\u0026rsquo; setup. As such, the thing was literally born to be \u0026ldquo;master-master\u0026rdquo; and assumes nodes come and go at will, so it\u0026rsquo;s got all the tools to handle bringing nodes up/down, marking them active/inactive, catching them up, etc. It\u0026rsquo;s written in Python, and uses plain text ini files for configuration.\nThere\u0026rsquo;s really only two hurdles that I found with using Londiste. First is that if you can\u0026rsquo;t get the rpms from the PGDG Yum Repo you\u0026rsquo;re looking at compiling from Git. And second, the online documentation for it is hard to find, hard to follow, and practically no one has used it so you can\u0026rsquo;t ask RandomPostgresPerson for help.\nWhich is exactly why I\u0026rsquo;m writing this blog post. Here\u0026rsquo;s what I needed to get me through the migration in question. I hope it helps you, should you consider using Londiste for your own replication needs. To whit:\nAs with other logical replication tools, you must ensure that all the tables to be replicated have a valid primary key. So before you even get started, determine which tables are missing them and then pass that list to your junior DBA and have them create pkeys while you continue on: \u0026gt; SELECT n.nspname as schema, c.relname as table FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026#39;r\u0026#39; AND NOT EXISTS ( SELECT 1 FROM pg_constraint con WHERE con.conrelid = c.oid AND con.contype = \u0026#39;p\u0026#39; ) AND n.nspname \u0026lt;\u0026gt; ALL ( ARRAY [ \u0026#39;pg_catalog\u0026#39;, \u0026#39;sys\u0026#39;, \u0026#39;dbo\u0026#39;, \u0026#39;information_schema\u0026#39; ] ); On the PostgreSQL 9.4.x server that will be receiving the replicated data, we need to ensure that all roles are pre-created. We want all ownerships and grants to be identical when we\u0026rsquo;re done, right? You can use pg_dumpall -g on the PostgreSQL 8.4.x to get a listing of roles.\nAgain, like Slony, we should pre-build the schema on the PostgreSQL 9.4.x server. I think you can actually get Londiste to do this for you as part of the replication, but I couldn\u0026rsquo;t find anything online for sure, and I didn\u0026rsquo;t have time to add more experimentation here (we\u0026rsquo;re on the customer\u0026rsquo;s dime here, remember). So, use pg_dump over the network and pipe it to pg_restore to transfer the schema thusly:\n$ pg_dump \\ -Fc \\ -C \\ -S \\ -h IPofOldServer \\ -U postgres \\ myapp | \\ pg_restore \\ -d template1 \\ -v \\ -C \\ -e \\ -s Install Skytools on the PostgreSQL 9.4.x server using the PGDG repo: $ yum -y install \\ skytools-94 \\ skytools-94-modules Install Skytools from source on the PostgreSQL 8.4.x server: $ yum -y install \\ python-devel \\ asciidoc \\ xmlto \\ postgresql-devel $ git clone git://github.com/markokr/skytools.git $ cd skytools $ git submodule init $ git submodule update $ ./autogen.sh $ ./configure $ make $ make install Restart the PostgreSQL 8.4.x cluster to load the new libs and modules\nNow we configure the Londiste ticker. Note, we have trust setup for the postgres user in pg_hba.conf so there is no password= in the connection strings. Adjust to meet your setup:\n$ mkdir -pv ~postgres/londiste-config/{log,pid} $ cd ~postgres/londiste-config $ cat \u0026lt;\u0026lt; EOF \u0026gt; ticker.ini [pgqd] base_connstr = user=postgres host=IPofOldServer database_list = myapp logfile = log/ticker.log pidfile = pid/ticker.pid EOF Start up the ticker, to provide the replication \u0026ldquo;heartbeat\u0026rdquo; by running pgqd -d ticker.ini\nCheck the ticker.log to ensure there are no warnings or errors! You can stop the ticker with pgqd -s ticker.ini while you fix things.\nNow, we tell Londiste about the master node (same note applies about the lack of password in the connection string):\n$ cd ~postgres/londiste-config $ cat \u0026lt;\u0026lt; EOF \u0026gt; master.ini [londiste3] db = user=postgres host=IPofOldServer dbname=myapp queue_name = myappq loop_delay = 0.5 logfile = log/master.log pidfile = pid/master.pid EOF We have to actually create the master node as the root node by doing: $ londiste3 \\ master.ini \\ create-root \\ master \u0026#39;user=postgres host=IPofOldServer dbname=myapp\u0026#39; Check the master.log to see if you have a line like INFO Node \u0026quot;master\u0026quot; initialized for queue \u0026quot;myappq\u0026quot; with type \u0026quot;root\u0026quot;\nNow, spin up the master\u0026rsquo;s replication worker process by running londiste3 -d master.ini worker\nNext, we configure our slave node (same note applies about the lack of password in the connection string):\n$ cd ~postgres/londiste-config $ cat \u0026lt;\u0026lt; EOF \u0026gt; slave.ini [londiste3] db = user=postgres host=127.0.0.1 dbname=myapp queue_name = myappq loop_delay = 0.5 logfile = log/slave.log pidfile = pid/slave.pid EOF Like the master, we have to create the slave node. I created it as a leaf but I could have created it as a branch if we we\u0026rsquo;re going to cascade replication: $ londiste3 \\ slave.ini \\ create-leaf slave \u0026#39;user=postgres host=127.0.0.1 dbname=myapp\u0026#39; \\ --provider=\u0026#39;user=postgres host=IPofOldServer dbname=myapp\u0026#39; Check the slave.log to see if you have the line INFO Node \u0026quot;slave\u0026quot; initialized for queue \u0026quot;myappq\u0026quot; with type \u0026quot;branch\u0026quot;\nSpin up the slave\u0026rsquo;s replication worker process by running londiste3 -d slave.ini worker\nTell the master node that we want to replicate all the tables in the db (londiste3 master.ini add-table --all) as well as all the sequences (londiste3 master.ini add-seq --all). Note that this only adds the tables that currently exist. If you add new tables to the master db, you need to londiste3 master.ini add-table tablename to add them to replication. Ditto for new sequences.\nFor the slave node, also replicate all the tables (londiste3 slave.ini add-table --all) and all the sequences (londiste3 slave.ini add-seq --all). Note that this only adds the tables that currently exist. If you add new tables to the master db, you need to londiste3 slave.ini add-table tablename to add them to replication. Ditto for new sequences.\nAt this point, replication is actually up and running. Any changes occurring on the master node are being replicated to the slave node. That\u0026rsquo;s all you need to do.\nBut what about the data that was already in the master db? You don\u0026rsquo;t need to do anything. It\u0026rsquo;s already replicating. You can forcibly tell Londiste to \u0026lsquo;catch things up\u0026rsquo; by doing londiste3 slave.ini resync --all if you like though.\nIf you want to check on the replication at any point, simply issue londiste3 slave.ini status or to be more pedantic londiste3 slave.ini compare which will examine row counts and md5sums between master and slave.\nEnjoy your new cross-version logical replication!\n","permalink":"https://hunleyd.github.io/posts/logical-replication-with-skytool3/","tags":["postgresql","replication","howto"],"title":"Logical Replication with Skytools3"},{"categories":["postgresql","replication","howto"],"contents":" While there\u0026rsquo;s absolutely nothing new in this blog\u0026quot;post\u0026quot;that isn\u0026rsquo;t covered by the wonderful docs I\u0026rsquo;ve been asked multiple times now by customers if we had some kind of \u0026lsquo;crib notes\u0026rsquo; format for how to get replication up and running. And since I just had to set this up and document it for a customer, I figured I might as well\u0026quot;post\u0026quot;it so that I can simply point people to it in the future. So here we are.\nNow, let\u0026rsquo;s get started. I assume you already have two PostgreSQL servers up with the binaries installed. For simplicity\u0026rsquo;s sake, we will call these machines \u0026lsquo;master\u0026rsquo; and \u0026lsquo;standby\u0026rsquo;. Note too that I\u0026rsquo;m using replication slots which needs PostgreSQL 9.4.0 or later; if you\u0026rsquo;re using something earlier, simply ignore the slot stuff.\nLet\u0026rsquo;s get started!\nOn the master, do the following:\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; postgresql.conf wal_level = hot_standby full_page_writes = on wal_log_hints = on max_wal_senders = 6 max_replication_slots = 6 hot_standby = on hot_standby_feedback = on EOF On the master, add the external IP addresses of the servers to pg_hba.conf:\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; pg_hba.conf host replication repl_user IP_of_master/32 md5 host replication repl_user IP_of_standby/32 md5 EOF Restart PostgreSQL on the master for the changes to take affect\nOn the master, create the replication user:\n$ psql \\ -d postgres \\ -U postgres \\ -c \u0026#34;CREATE ROLE repl_user LOGIN REPLICATION ENCRYPTED PASSWORD \u0026#39;secretpasswordhere\u0026#39;;\u0026#34; On the master, create the replication slot for the standby:\n$ psql \\ -d postgres \\ -U postgres \\ -c \u0026#34;SELECT * FROM pg_create_physical_replication_slot(\u0026#39;standby1\u0026#39;, true);\u0026#34; On the standby, wipe the existing cluster:\n$ cd /var/lib/pgsql/9.4/data $ pg_ctl -D $PWD -mf stop $ cd .. $ rm -rfv data On the standby, use the pg_basebackup command to clone the master (enter the repl_user\u0026rsquo;s password from above when prompted):\n$ pg_basebackup \\ -D data \\ -Fp \\ -R \\ -Xs \\ -c fast \\ -l \u0026#39;initial clone\u0026#39; \\ -P \\ -v \\ -h IP_of_master \\ -U repl_user On the standby, tweak the recovery.conf that was created for you and add the replication slot name:\n$ cd data cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; recovery.conf primary_slot_name = \u0026#39;standby1\u0026#39; EOF Start the standby up\nAnd that\u0026rsquo;s it. You should be all done. Easy, right?\n","permalink":"https://hunleyd.github.io/posts/postgresql-streaming-replication-in-10-minutes/","tags":["postgresql","replication","howto"],"title":"PostgreSQL Streaming Replication in 10 Minutes"},{"categories":["general","networking","crowdfunding"],"contents":"I don\u0026rsquo;t normally back things on Indiegogo (I\u0026rsquo;m a Kickstarter snob), but I jumped all over the eero. I have always had issues with the wifi coverage in my home. I\u0026rsquo;ve tried everything I can think of to alleviate the issues; differernt brands of routers, flashing different OSS firmwares, \u0026lsquo;upgrading\u0026rsquo; to SOHO equipment instead of consumer equipment, moving them all over the damn house, etc. Nothing solved all the issues. Until now.\nI\u0026rsquo;ve actully had the eero up and running for a while now (almost two months already) but I held off saying anything because I wanted to see if issues would crop back up or not and I wanted to get past the initial \u0026lsquo;cool, it works!\u0026rsquo; feeling. So here I am ~60 days later, and I\u0026rsquo;m still feeling the \u0026rsquo;this is cool\u0026rsquo; feeling towards the eero.\nI bought the triple set, put the \u0026lsquo;main\u0026rsquo; one in the basement by the cable modem, the next one on the other end of the house on the first floor, and the final one in the middle of the house on the second floor. Setup was stupid simple and hassle free. Like, Tina or Emily could have done this, simple. Once up and talking to each other, they then scanned all the wifi leakage from the neighbors and picked a channel with the least amount of interference and started handing out DHCP leases to my devices. Easy peasy.\nSpeaking of \u0026lsquo;my devices\u0026rsquo;.. here\u0026rsquo;s everything I\u0026rsquo;ve got hooked up to the eero-powered wifi:\n1 x Nest thermostat 2 x \u0026ldquo;smart\u0026rdquo; tv 1 x \u0026ldquo;smart\u0026rdquo; bluray player 1 x Nexus Player / Android TV 1 x PS3 2 x PS4 1 x Sonos speaker 2 x Macbook Pro 2 x Android phones 3 x Android tablets 2 x iPhone 1 x iPad mini 1 x Chromebook 1 x Lenovo laptop Now, I didn\u0026rsquo;t make that list to impress you, or to depress myself at how much money I\u0026rsquo;ve spent over the years. That list is to show what\u0026rsquo;s connected to the eero and to show that there is a big variety of devices. Some are 2.4GHz, some 5GHz, some both. Some do jumbo frames. Some support wifi media extensions. And they all work flawlessly while connected to the eero. No matter how many are connected simultaneoussly. Which is the first time ever. Seriously. It works so well, I went around and unplugged Ethernet from devices that had it and put them on the wifi!\nWe can have multiple Netflix streams going while playing online in an FPS while streaming music to another room while someone else is on the laptop working. And no one even notices the other users. It\u0026rsquo;s epic. I now actually have a valid reason for buying the most expensive upload speed my ISP offers :)\nAs for gripes? Well, of course no system is perfect. In its quest to make things stupid simple, there\u0026rsquo;s a lot the eero mgmt the app doesn\u0026rsquo;t let you do. You can\u0026rsquo;t, for example, blacklist a MAC. Or turn off uplink for certain MACs between certain hours of the day. You can\u0026rsquo;t throttle certain devices. Basically, all the things nerds love to mess with that just confuse the normal consumer. And I\u0026rsquo;m (mostly) OK with that. I\u0026rsquo;m sure some of the functionality is coming in future updates, but even if it doesn\u0026rsquo;t, I don\u0026rsquo;t care. It just works. And it works well and easy enough that I can talk the wife through an issue over the phone while I\u0026rsquo;m on the other side of the planet.\nI \u0026lt;3 my eero.\n‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\n","permalink":"https://hunleyd.github.io/posts/hello_eero/","tags":["general","networking","crowdfunding"],"title":"Hello, eero"},{"categories":["work","computers","osx","linux"],"contents":" I recently got a new Macbook Pro from work as my old one was having keyboard issues and since it was 3+ yrs old they deemed it not worth fixing. As you probably know, I spent a lot of time and effort on the previous mbp to get Gentoo up and running on it. I chose not to do so with this one.\nWhile I have looked into it and believe that I could get Gentoo up and running fairly easily, I\u0026rsquo;ve decided to skip it for now and try to live with OSX. I travel for work and never know what I\u0026rsquo;m going to be asked to do when onsite. Sometimes I have to present things on a projector while other times the customer only has 5Ghz wifi available. And thanks to the proprietary nature of these machines, these features are either non-functional or sub-optimal under Linux. I\u0026rsquo;m also getting tired of our IT staff telling me Linux isn\u0026rsquo;t supported whenever I have an issue, even if that issue clearly doesn\u0026rsquo;t have anything to do with the OS. And several other minor annoyances that have added up over my three yrs tenure here.\nSo, yeah. I\u0026rsquo;ve decided to sign up for the pain of staying in OSX. So far, I think I\u0026rsquo;ve managed to coerce the system into a \u0026lsquo;feel-alike\u0026rsquo; of my prior KDE 5 Plasma environment but it\u0026rsquo;s taken a lot of tweaking and Googling and setting of magic CLI commands to get OSX to do my bidding. And I\u0026rsquo;ve had to buy more than a few apps to get functionality that I got for free on Linux, which chaps my ass.\nSo we\u0026rsquo;ll see how this lasts. Or doesn\u0026rsquo;t.\n","permalink":"https://hunleyd.github.io/posts/glutton-for-punishment/","tags":["work","computers","osx","linux"],"title":"Glutton for Punishment"},{"categories":["general"],"contents":"\nBeing a native of Ohio, I can assure you that I\u0026rsquo;ve heard every form of \u0026lsquo;flyover state\u0026rsquo; insult you could care to level at me. And while I won\u0026rsquo;t debate the merits of Ohio in this post, I will take a minute to point out something that I\u0026rsquo;ve been saying for quite some time.\nOhio really is pretty centrally located to anywhere you might care to be other than the left coast.\nThe map I\u0026rsquo;ve linked to in this\u0026quot;post\u0026quot;shows what $50 worth of gas means to someone from Columbus, OH as far as their ability to drive anywhere. Of course, it makes an assumption about the price of gas and the car\u0026rsquo;s mpg (24mpg, iirc) but the point stands.\nAnd this is what I\u0026rsquo;ve been telling my coworker a lot lately. I travel periodically for business, and whenever I do, I drive (unless it\u0026rsquo;s just way too impractical). It always trips people out when I do. I\u0026rsquo;d much rather grab a rental car, and spend a day on my own schedule seeing the country then being crammed in like cattle and dealing with the TSA. And this image shows that I\u0026rsquo;m generally saving the company money as well.\nSo, you all can keep \u0026ldquo;flying over\u0026rdquo;. In the meantime, I\u0026rsquo;ll be driving around enjoying myself.\n","permalink":"https://hunleyd.github.io/posts/flyover-state/","tags":["general"],"title":"Flyover State"},{"categories":["general"],"contents":"So I\u0026rsquo;m sitting here seting up a new Internet connected device I bought recently (more on it later) and I can\u0026rsquo;t seem to get it to setup properly. I can create an account on their site, I can login to said account, I can power on said device and connect to it using the app and configure it to use my home wifi, but the device just won\u0026rsquo;t ever finish connecting and syncing w/ my account on the site.\nI try this about half a dozen times, with a few reboots and even a reset of the device in there. Still no luck. So I power it off, set it aside, and go do some things IRL thinking \u0026ldquo;I\u0026rsquo;ll come back to it\u0026rdquo;. Not five minutes later, I get an email on my phone from Tech Support at the device\u0026rsquo;s company saying thusly:\nWe noticed that you are having problems adding $device to your system. To assist you with the process, we would like to first push a firmware update to your $device. Please keep your $device plugged in and we will take care of the rest. We will send you another email when you have been updated.\nNow that\u0026rsquo;s efficient. And super creepy.\nI kinda like it :D\n","permalink":"https://hunleyd.github.io/posts/efficient-yet-creepy/","tags":["general"],"title":"Efficient, yet Creepy"},{"categories":["android","review","geek"],"contents":"\nA couple of my mates wanted me to let them know what I thought of the Huawei Watch (hereafter, \u0026lsquo;H\u0026rsquo;) after upgrading from the Moto 360 (hereafter, 360) and spending some time getting to know the \u0026lsquo;H\u0026rsquo;. So here\u0026rsquo;s my thoughts.\nTL;DR Better than the 360 in every way that counts\nI should probably preface this review by saying that I loved (and still love) the 360. I was excited to buy it, I was excited to see it ship to my house, I was pissed that I wasn\u0026rsquo;t at my house when it arrived (long-term gig in LA), I was proud to wear it, and I loved showing it to anyone who cared (and probably more than a few who didn\u0026rsquo;t). Having said that, the 360 I had was a Gen 1 in every sense. It was the first watch Moto had made. It was the first smartwatch Moto had made. It was the flagship Android Wear device. And the minute I got my hands on a 2nd gen device, those things were readily apparent.\nDon\u0026rsquo;t get me wrong. It\u0026rsquo;s not all sunshine and roses w/ the \u0026lsquo;H\u0026rsquo;. For example, charging is actually quite a bit of a sorrier state than with the 360. You see, the 360 has a standard Qi charging mechanism. So while there is a cute little official charging stand, you can make do with any Qi charger you have once you figure out where to place the watch on the coil. And the official charger uses a standard, I-have-tons-of-them-all-of-the-place USB-A to USB-micro cable. The \u0026lsquo;H\u0026rsquo;, on the other hand, uses pogo pins to connect to it\u0026rsquo;s charger. Which means you have to use a compatible charger (of which there is only one currently). Take a road trip and forget your charger? Buy another one or go without your watch. And to add to the insult, the charger cable, while having a standard USB-A plug on one end, is terminated inside the charging puck. So if the cord should go bad or isn\u0026rsquo;t long enough, too bad. You can\u0026rsquo;t use one of those USB cables from the pile in your desk drawer. Nope. Sorry. And then, of course, there\u0026rsquo;s the act of charging the watch. With the 360, you simply slide it into its dock, and you\u0026rsquo;re done. Heck, after a while, you can almost lightly toss it into the charger like a game of corn hole and it will charge. Half asleep? Drunk? No problem. Tis easy. The \u0026lsquo;H\u0026rsquo; on the other hand? You have to line up the pogos exactly or you aren\u0026rsquo;t getting any juice. And I mean exact. Doesn\u0026rsquo;t matter if the watch and the charger look perfectly aligned and centered with each other. If the pins aren\u0026rsquo;t perfect, no juice for you! I\u0026rsquo;ve already had my share of waking up to find out the watch didn\u0026rsquo;t charge. It SUCKS.\nPro tip: I\u0026rsquo;ve found that hovering the \u0026lsquo;H\u0026rsquo; right over the charging puck allows the magnets in them to pull them together in such a way that the puck \u0026ldquo;leaps\u0026rdquo; up and attaches to the watch with the pogo pins perfectly positioned\nSo yeah, charging. The one thorn in the rose. But worth it, imho.\nReview: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ\n","permalink":"https://hunleyd.github.io/posts/huawei-watch-review/","tags":["android","review","geek"],"title":"Huawei Watch Review"},{"categories":["geek","kickstarter","review"],"contents":"\nA while back, I ran across a little project on Kickstarter from a company called SnapPower. The project was for a product called Guidelights.\nNow, I\u0026rsquo;m a \u0026ldquo;big boy\u0026rdquo; and don\u0026rsquo;t need a night light, but I do wander around the house after dark quite a bit and hate turning on the lights when I\u0026rsquo;m in a room for like a whole minute. So the concept of having LED lights embedded into the outlet cover and having them turn on/off based on the ambient light in the room sounded pretty cool. And the fact that there\u0026rsquo;s no wiring or anything was slick. So I ponied up and ordered some.\nI gotta tell you, they rock! I now have one in the upstairs hallyway, another at the bottom of the stairs, and a third on the island in the kitchen. It\u0026rsquo;s freaking perfect. Seriously. And like I said, there\u0026rsquo;s no skill involved. If you can work a flat head screwdriver, you can install these!\nIn fact, when Stephanie moves into her apartment (soonish), I\u0026rsquo;ll be outfitting her place with a few of these as well. They\u0026rsquo;re really that good,\nReview: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\ns much as I love the Guidelights, I was stoked when SnapPower announced their Snap Power Chargers.\nI don\u0026rsquo;t know what it\u0026rsquo;s like in your household, but all four of us have smart phones, three of us have tablets, and two of us have smart watches. Someone always needs to charge something. And my family is horrible about removing their chargers from outlets when they\u0026rsquo;re done. And one person in particular has taken to unplugging whatever is \u0026ldquo;in the way\u0026rdquo; so she can charge her phone close to wherever she\u0026rsquo;s sitting! Clearly, a solution that adds USB ports and frees up electrical outlets is a solution I need to have.\nSo I eagerly bought five of these things. I installed all but one yesterday, and I gotta say it is as good of an investment as the light covers. I will be installing a few of these in Steph\u0026rsquo;s apartment also. Now, I\u0026rsquo;ll grant you that they are not \u0026ldquo;quick charge\u0026rdquo;. And there\u0026rsquo;s only one USB facing the right side of the cover instead of one on both the left and right. But I understand there\u0026rsquo;s only so much room in these things, and only so much power you can leech off the contacts. So I\u0026rsquo;m not terribly upset. It would be nice though if you could choose left or right when ordering.\nHonestly, I\u0026rsquo;m kinda stoked to see what this company comes up with next. Of all the random crap I\u0026rsquo;ve backed on Kickstarter, SnapPower is easily the most useful and easiest to recommend to others.\nReview: ‚òÖ‚òÖ‚òÖ‚òÖ‚ú©\n","permalink":"https://hunleyd.github.io/posts/i-have-the-power/","tags":["geek","kickstarter","review"],"title":"I Have The Power"},{"categories":["blog"],"contents":"It\u0026rsquo;s been a bit of a busy day for me and my blog. I\u0026rsquo;ve added:\nan RSS feed an archive tags Disqus comments All with as little effort on my part as I\u0026rsquo;ve ever put into a blog. I fucking love this blogging platform!\n","permalink":"https://hunleyd.github.io/posts/more-bloggy-goodness/","tags":["blog"],"title":"More Bloggy Goodness"},{"categories":["blog"],"contents":"Welcome to the new blog.\nThat\u0026rsquo;s right, I\u0026rsquo;m running my own blog once again. I know, I know. It\u0026rsquo;s been a while. And I\u0026rsquo;ve picked yet another platform for hosting my blog. But you know what? You\u0026rsquo;re just going to have to suck it up. I\u0026rsquo;ve been itching to make some new posts for a while now, and the platforms at my disposal, cough G+ cough, just haven\u0026rsquo;t really cut it. Sure, they sorta worked, but with the recent direction that platform is headed? Forget it.\nSo then a coworker mentioned standing up his new blog using GitHub and Jekyll and I was blown away at how it worked. It just so totally fit with my workflow and preferences. So here we are. Everything old is new again. Or something.\n","permalink":"https://hunleyd.github.io/posts/once-more-with-feeling/","tags":["blog"],"title":"Once More, With Feeling"},{"categories":null,"contents":"Born at the ass end of 1973, Douglas lived a (mostly) quiet life in and around the village of Weston, OH until moving to Columbus, OH to attend college in 1993. He acquired a taste for ‚Äòcity life‚Äô during this time and has remained in the Greater Columbus Metro Area ever since.\nSurviving both Guillain‚ÄìBarr√© syndrome (GBS) and two teenage daughters are arguably his greatest achievements.\nWhen not blogging, Douglas can be found curled up on the couch with the family Chihuahuas reading a book, on his phone/tablet, playing PLayStation, or watching a movie.\n","permalink":"https://hunleyd.github.io/author/hunleyd/","tags":null,"title":"Doug Hunley"}]